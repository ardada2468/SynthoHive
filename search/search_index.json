{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SynthoHive Documentation","text":"<p>SynthoHive is a synthetic data engine that preserves relational integrity while offering privacy safeguards and validation tooling. These docs cover core concepts, guided workflows, demos, and API reference.</p>"},{"location":"#what-youll-find","title":"What you'll find","text":"<ul> <li>Quick start for installing and running demos.</li> <li>Concept pages for architecture and data flow.</li> <li>Guides for fitting, sampling, privacy, and validation.</li> <li>Demo walk-throughs mirroring the <code>examples/demos</code> folder.</li> <li>API reference generated from in-code docstrings via <code>mkdocstrings</code>.</li> </ul>"},{"location":"#at-a-glance","title":"At a glance","text":"<ol> <li>Install docs deps: <code>pip install .[docs]</code></li> <li>Preview docs locally: <code>mkdocs serve</code></li> <li>Build/deploy (GitHub Pages): <code>mkdocs gh-deploy --force</code></li> </ol>"},{"location":"architecture/","title":"Architecture","text":"<p>SynthoHive is organized into clear packages:</p> <ul> <li>interface: <code>Synthesizer</code> fa\u00e7ade, <code>Metadata</code>, <code>TableConfig</code>, <code>PrivacyConfig</code> entry points.</li> <li>core: <code>DataTransformer</code> for normalization/encoding, and <code>CTGAN</code> (Conditional WGAN-GP) for deep generative modeling. See API.</li> <li>relational: <code>StagedOrchestrator</code> managing the generation DAG, and <code>LinkageModel</code> for parent-child cardinality learning. See API.</li> <li>privacy: <code>PIISanitizer</code> with regex-based detection, and <code>ContextualFaker</code> for locale-aware obfuscation. See API.</li> <li>validation: <code>ValidationReport</code> and <code>StatisticalValidator</code> measuring KS/TVD metrics. See API.</li> <li>connectors: <code>SparkIO</code> for scalable I/O.</li> </ul>"},{"location":"architecture/#key-flows","title":"Key flows","text":"<ol> <li>Fit: transformers profile tables, CTGAN trains (optionally conditioned on parent context), linkage models learn child counts.</li> <li>Sample: generators produce rows, linkage models drive child counts, referential integrity enforced via FK assignment.</li> <li>Privacy: sanitizer detects/masks/fakes PII; contextual faker injects locale-aware replacements.</li> <li>Validation: KS/TVD per column, correlation distance, preview tables, HTML/JSON report.</li> </ol> <p>See Data Flow for a stepwise diagram and Guides for hands-on steps. </p>"},{"location":"changelog/","title":"Changelog","text":"<ul> <li>Initial MkDocs site scaffold with guides, demos, and API reference.</li> </ul>"},{"location":"data-flow/","title":"Data Flow","text":"<pre><code>flowchart TD\n    A[Real tables] --&gt; B[DataTransformer fit/transform]\n    B --&gt; C[CTGAN training]\n    A --&gt; D[LinkageModel (child counts)]\n    C --&gt; E[Generator]\n    D --&gt; E\n    E --&gt; F[Inverse transform]\n    F --&gt; G[Privacy sanitizer (optional)]\n    G --&gt; H[Validation report]\n</code></pre>"},{"location":"data-flow/#steps","title":"Steps","text":"<ol> <li>Transform: <code>DataTransformer.fit/transform</code> profiles each column (continuous via VGM, categorical via OHE or embeddings) and excludes PK/FK where configured.</li> <li>Train: <code>CTGAN.fit</code> learns distributions; conditional context from parent tables can be merged before fitting.</li> <li>Linkage: <code>LinkageModel.fit</code> learns child-row cardinalities from FK counts.</li> <li>Sample: <code>CTGAN.sample</code> generates rows; linkage drives child counts; FKs are assigned to maintain integrity.</li> <li>Inverse: transformer rebuilds the original schema; constraints (clip/round) are applied.</li> <li>Privacy: <code>PIISanitizer</code> masks/hashes/fakes PII; <code>ContextualFaker</code> injects locale-aware values.</li> <li>Validate: <code>ValidationReport</code> compares distributions (KS/TVD), correlations, and provides previews.</li> </ol>"},{"location":"getting-started/","title":"Getting Started","text":"<p>SynthoHive is a production-grade synthetic data engine that generates high-fidelity relational data while ensuring privacy compliance.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>SynthoHive requires Python 3.9+ and PySpark.</p> <ul> <li>Installation:   <pre><code>pip install synthohive pyspark pandas pyarrow\n</code></pre></li> </ul>"},{"location":"getting-started/#quick-start-single-script","title":"Quick Start: Single Script","text":"<p>You can run this entire workflow in a single file. Save the code below as <code>quickstart.py</code> and run it with <code>python quickstart.py</code>.</p> <pre><code>import os\nimport shutil\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nfrom syntho_hive.interface.config import Metadata, PrivacyConfig\nfrom syntho_hive.interface.synthesizer import Synthesizer\n\n# --- 1. SETUP SPARKSESSION ---\n# Initialize a local Spark session. In production, this would connect to your cluster.\nspark = SparkSession.builder \\\n    .appName(\"SynthoHive_QuickStart\") \\\n    .master(\"local[1]\") \\\n    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n    .getOrCreate()\n\n# --- 2. CREATE DUMMY DATA ---\n# For this demo, we create a small dataset and save it to disk. \n# SynthoHive normally reads from data lakes (Delta/Parquet) or Hive tables.\ndata_dir = \"./quickstart_data\"\nif os.path.exists(data_dir):\n    shutil.rmtree(data_dir)\nos.makedirs(data_dir)\n\nraw_data = pd.DataFrame({\n    \"user_id\": range(1, 101),\n    \"age\": [20, 30, 40, 50] * 25,\n    \"city\": [\"New York\", \"London\", \"Tokyo\", \"Paris\"] * 25,\n    \"income\": [50000.0, 60000.0, 75000.0, 90000.0] * 25\n})\n\ninput_path = f\"{data_dir}/users_input.parquet\"\nraw_data.to_parquet(input_path)\nprint(f\"\u2705 Dummy data created at {input_path}\")\n\n# --- 3. DEFINE METADATA ---\n# Tell SynthoHive about the schema.\n# We explicitly mark the Primary Key (pk).\nmetadata = Metadata()\nmetadata.add_table(\"users\", pk=\"user_id\")\n\n# --- 4. CONFIGURE &amp; TRAIN ---\n# PrivacyConfig controls sanitization (e.g. masking PII), default is safe.\nprivacy = PrivacyConfig()\n\n# Initialize the Synthesizer\n# It acts as the coordinator for reading data, privacy enforcement, and training.\nsynth = Synthesizer(\n    metadata=metadata,\n    privacy_config=privacy,\n    spark_session=spark\n)\n\n# Fit the model\n# We point 'users' to the parquet file we just made.\nprint(\"\ud83d\ude80 Training model...\")\nsynth.fit(\n    data={\"users\": input_path}, \n    epochs=10,        # Use 300+ for production quality\n    batch_size=50\n)\n\n# --- 5. GENERATE DATA ---\n# Sample new synthetic records from the learned distribution.\nprint(\"\u2728 Generating data...\")\noutput_base_path = f\"{data_dir}/output\"\noutput_paths = synth.sample(\n    num_rows={\"users\": 50},\n    output_path=output_base_path,\n    output_format=\"parquet\"\n)\n\n# --- 6. INSPECT RESULTS ---\n# output_paths is a dict mapping table name to the output directory\nsynth_df = pd.read_parquet(output_paths[\"users\"])\nprint(f\"\\n\ud83d\udcca Generated {len(synth_df)} synthetic records:\")\nprint(synth_df.head())\n\n# Clean up\ntry:\n    spark.stop()\nexcept:\n    pass\n</code></pre>"},{"location":"getting-started/#explanation","title":"Explanation","text":"<p>Here is what is happening in the script above:</p>"},{"location":"getting-started/#1-setup-spark","title":"1. Setup Spark","text":"<p>SynthoHive relies on PySpark for scalable data processing. In this example, we create a local session (<code>local[1]</code>) so you can run it on your laptop without a cluster.</p>"},{"location":"getting-started/#2-define-metadata","title":"2. Define Metadata","text":"<p>Use the <code>Metadata</code> object to define your schema. - <code>add_table</code>: Registers a table. - <code>pk</code>: Specifies the Primary Key. SynthoHive ensures this is unique in generated data.</p>"},{"location":"getting-started/#3-initialize-synthesizer","title":"3. Initialize Synthesizer","text":"<p>The <code>Synthesizer</code> class is the main entry point. It takes your metadata and privacy config and orchestrates the entire pipeline. - <code>privacy_config</code>: Used to define PII columns and anonymization strategies (e.g., masking emails).</p>"},{"location":"getting-started/#3-missing-data","title":"3. Missing Data","text":"<p>SynthoHive automatically handles missing values (<code>NaN</code>, <code>None</code>) in your dataset.  - Continuous columns: Missing values are modeled using a null indicator. - Categorical columns: Missing values are treated as a distinct category. No manual imputation is required before training.</p>"},{"location":"getting-started/#4-training-ctgan","title":"4. Training (CTGAN)","text":"<p>The <code>fit</code> method learns the statistical distribution of your real data. - <code>data</code>: A dictionary checking table names to their file paths (or Hive table names). - <code>epochs</code>: Low (10) for this demo, but should be higher (300-500) for high-fidelity results. - <code>checkpoint_dir</code>: (Optional) Directory to save the best model and training metrics.</p>"},{"location":"getting-started/#5-generate-sample","title":"5. Generate (<code>sample</code>)","text":"<p>The <code>sample</code> method creates new data based on the trained model. - <code>num_rows</code>: How many records you want. - <code>output_path</code>: Where to save the data. If omitted (set to <code>None</code>), it returns the generated DataFrames directly in memory!</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you've generated your first table, explore more advanced features:</p> <ul> <li>Relational Data: Learn how to model complex schemas with Foreign Keys.</li> <li>Privacy Guardrails: Configure PII detection and sanitization.</li> <li>Validation Reports: Generate HTML reports proving the quality of your data.</li> </ul>"},{"location":"api/connectors/","title":"Connectors","text":""},{"location":"api/connectors/#syntho_hive.connectors.spark_io.SparkIO","title":"syntho_hive.connectors.spark_io.SparkIO","text":"<p>Utility for reading and writing datasets via Spark and Delta Lake.</p> Source code in <code>syntho_hive/connectors/spark_io.py</code> <pre><code>class SparkIO:\n    \"\"\"Utility for reading and writing datasets via Spark and Delta Lake.\"\"\"\n    def __init__(self, spark: SparkSession):\n        \"\"\"Initialize the IO helper.\n\n        Args:\n            spark: Active SparkSession used for all IO.\n        \"\"\"\n        self.spark = spark\n\n    def read_dataset(self, path_or_table: str, format: str = None, **kwargs: Union[str, int, bool, float]) -&gt; DataFrame:\n        \"\"\"Read a dataset from a table name or filesystem path.\n\n        Args:\n            path_or_table: Hive table name or filesystem/URI path.\n            format: Optional explicit format override (e.g., ``\"csv\"``).\n            **kwargs: Additional Spark read options.\n\n        Returns:\n            Spark DataFrame loaded from the specified source.\n        \"\"\"\n        # Simple heuristic\n        if \"/\" in path_or_table or \"\\\\\" in path_or_table or path_or_table.startswith(\"file://\"):\n            if format:\n                return self.spark.read.format(format).load(path_or_table, **kwargs)\n\n            if path_or_table.endswith(\".csv\"):\n                return self.spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\", \"true\").load(path_or_table, **kwargs)\n            elif path_or_table.endswith(\".parquet\"):\n                return self.spark.read.format(\"parquet\").load(path_or_table, **kwargs)\n            else:\n                # Default to parquet for directories/tables (matching write default)\n                return self.spark.read.format(\"parquet\").load(path_or_table, **kwargs)\n        return self.spark.table(path_or_table)\n\n    def write_dataset(self, df: DataFrame, target_path: str, mode: str = \"overwrite\", partition_by: Optional[str] = None, format: str = \"parquet\"):\n        \"\"\"Write a Spark DataFrame to storage.\n\n        Args:\n            df: Spark DataFrame to persist.\n            target_path: Output path (directory or table location).\n            mode: Save mode, e.g., ``\"overwrite\"`` or ``\"append\"``.\n            partition_by: Optional column name to partition by.\n            format: Output format, defaults to ``\"parquet\"``.\n        \"\"\"\n        writer = df.write.format(format).mode(mode)\n        if partition_by:\n            writer = writer.partitionBy(partition_by)\n        writer.save(target_path)\n\n    def write_pandas(self, pdf: pd.DataFrame, target_path: str, mode: str = \"append\", format: str = \"parquet\"):\n        \"\"\"Write a Pandas DataFrame using Spark-backed persistence.\n\n        Args:\n            pdf: Pandas DataFrame to persist.\n            target_path: Output path for the written dataset.\n            mode: Save mode for Spark writer (default ``\"append\"``).\n            format: Storage format, defaults to ``\"parquet\"``.\n        \"\"\"\n        sdf = self.spark.createDataFrame(pdf)\n        self.write_dataset(sdf, target_path, mode=mode, format=format)\n</code></pre>"},{"location":"api/connectors/#syntho_hive.connectors.spark_io.SparkIO.read_dataset","title":"read_dataset","text":"<pre><code>read_dataset(path_or_table: str, format: str = None, **kwargs: Union[str, int, bool, float]) -&gt; DataFrame\n</code></pre> <p>Read a dataset from a table name or filesystem path.</p> <p>Parameters:</p> Name Type Description Default <code>path_or_table</code> <code>str</code> <p>Hive table name or filesystem/URI path.</p> required <code>format</code> <code>str</code> <p>Optional explicit format override (e.g., <code>\"csv\"</code>).</p> <code>None</code> <code>**kwargs</code> <code>Union[str, int, bool, float]</code> <p>Additional Spark read options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Spark DataFrame loaded from the specified source.</p> Source code in <code>syntho_hive/connectors/spark_io.py</code> <pre><code>def read_dataset(self, path_or_table: str, format: str = None, **kwargs: Union[str, int, bool, float]) -&gt; DataFrame:\n    \"\"\"Read a dataset from a table name or filesystem path.\n\n    Args:\n        path_or_table: Hive table name or filesystem/URI path.\n        format: Optional explicit format override (e.g., ``\"csv\"``).\n        **kwargs: Additional Spark read options.\n\n    Returns:\n        Spark DataFrame loaded from the specified source.\n    \"\"\"\n    # Simple heuristic\n    if \"/\" in path_or_table or \"\\\\\" in path_or_table or path_or_table.startswith(\"file://\"):\n        if format:\n            return self.spark.read.format(format).load(path_or_table, **kwargs)\n\n        if path_or_table.endswith(\".csv\"):\n            return self.spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\", \"true\").load(path_or_table, **kwargs)\n        elif path_or_table.endswith(\".parquet\"):\n            return self.spark.read.format(\"parquet\").load(path_or_table, **kwargs)\n        else:\n            # Default to parquet for directories/tables (matching write default)\n            return self.spark.read.format(\"parquet\").load(path_or_table, **kwargs)\n    return self.spark.table(path_or_table)\n</code></pre>"},{"location":"api/connectors/#syntho_hive.connectors.spark_io.SparkIO.write_dataset","title":"write_dataset","text":"<pre><code>write_dataset(df: DataFrame, target_path: str, mode: str = 'overwrite', partition_by: Optional[str] = None, format: str = 'parquet')\n</code></pre> <p>Write a Spark DataFrame to storage.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame to persist.</p> required <code>target_path</code> <code>str</code> <p>Output path (directory or table location).</p> required <code>mode</code> <code>str</code> <p>Save mode, e.g., <code>\"overwrite\"</code> or <code>\"append\"</code>.</p> <code>'overwrite'</code> <code>partition_by</code> <code>Optional[str]</code> <p>Optional column name to partition by.</p> <code>None</code> <code>format</code> <code>str</code> <p>Output format, defaults to <code>\"parquet\"</code>.</p> <code>'parquet'</code> Source code in <code>syntho_hive/connectors/spark_io.py</code> <pre><code>def write_dataset(self, df: DataFrame, target_path: str, mode: str = \"overwrite\", partition_by: Optional[str] = None, format: str = \"parquet\"):\n    \"\"\"Write a Spark DataFrame to storage.\n\n    Args:\n        df: Spark DataFrame to persist.\n        target_path: Output path (directory or table location).\n        mode: Save mode, e.g., ``\"overwrite\"`` or ``\"append\"``.\n        partition_by: Optional column name to partition by.\n        format: Output format, defaults to ``\"parquet\"``.\n    \"\"\"\n    writer = df.write.format(format).mode(mode)\n    if partition_by:\n        writer = writer.partitionBy(partition_by)\n    writer.save(target_path)\n</code></pre>"},{"location":"api/connectors/#syntho_hive.connectors.spark_io.SparkIO.write_pandas","title":"write_pandas","text":"<pre><code>write_pandas(pdf: DataFrame, target_path: str, mode: str = 'append', format: str = 'parquet')\n</code></pre> <p>Write a Pandas DataFrame using Spark-backed persistence.</p> <p>Parameters:</p> Name Type Description Default <code>pdf</code> <code>DataFrame</code> <p>Pandas DataFrame to persist.</p> required <code>target_path</code> <code>str</code> <p>Output path for the written dataset.</p> required <code>mode</code> <code>str</code> <p>Save mode for Spark writer (default <code>\"append\"</code>).</p> <code>'append'</code> <code>format</code> <code>str</code> <p>Storage format, defaults to <code>\"parquet\"</code>.</p> <code>'parquet'</code> Source code in <code>syntho_hive/connectors/spark_io.py</code> <pre><code>def write_pandas(self, pdf: pd.DataFrame, target_path: str, mode: str = \"append\", format: str = \"parquet\"):\n    \"\"\"Write a Pandas DataFrame using Spark-backed persistence.\n\n    Args:\n        pdf: Pandas DataFrame to persist.\n        target_path: Output path for the written dataset.\n        mode: Save mode for Spark writer (default ``\"append\"``).\n        format: Storage format, defaults to ``\"parquet\"``.\n    \"\"\"\n    sdf = self.spark.createDataFrame(pdf)\n    self.write_dataset(sdf, target_path, mode=mode, format=format)\n</code></pre>"},{"location":"api/connectors/#syntho_hive.connectors.sampling.RelationalSampler","title":"syntho_hive.connectors.sampling.RelationalSampler","text":"<p>Relational stratified sampler for parent-child table hierarchies.</p> Source code in <code>syntho_hive/connectors/sampling.py</code> <pre><code>class RelationalSampler:\n    \"\"\"Relational stratified sampler for parent-child table hierarchies.\"\"\"\n\n    def __init__(self, metadata: Metadata, spark: SparkSession):\n        \"\"\"Initialize the sampler.\n\n        Args:\n            metadata: Metadata describing tables and their keys.\n            spark: Active SparkSession for table access.\n        \"\"\"\n        self.metadata = metadata\n        self.spark = spark\n\n    def sample_relational(\n        self, \n        root_table: str, \n        sample_size: int, \n        stratify_by: Optional[str] = None\n    ) -&gt; Dict[str, DataFrame]:\n        \"\"\"Sample a root table and cascade the sample to child tables.\n\n        Args:\n            root_table: Name of the parent/root table to sample.\n            sample_size: Approximate number of rows to retain from the root.\n            stratify_by: Optional column for stratified sampling.\n\n        Returns:\n            Dictionary mapping table name to sampled Spark DataFrame.\n        \"\"\"\n        sampled_data = {}\n\n        # 1. Sample Root\n        print(f\"Sampling root table: {root_table}\")\n        # Placeholder for real table loading\n        root_df = self.spark.table(root_table)\n\n        if stratify_by:\n            # Approximate stratified sampling\n            fractions = root_df.select(stratify_by).distinct().withColumn(\"fraction\", F.lit(0.1)).rdd.collectAsMap() \n            # Note: fractions logic needs to be calculated based on sample_size / total_count\n            sampled_root = root_df.sampleBy(stratify_by, fractions, seed=42)\n        else:\n            fraction = min(1.0, sample_size / root_df.count())\n            sampled_root = root_df.sample(withReplacement=False, fraction=fraction, seed=42)\n\n        sampled_data[root_table] = sampled_root\n\n        # 2. Cascade to Children\n        # Simple BFS or using Graph\n        parent_pk = self.metadata.get_table(root_table).pk\n\n        # Find children\n        for child_name, config in self.metadata.tables.items():\n            for child_col, parent_ref in config.fk.items():\n                if parent_ref.startswith(f\"{root_table}.\"):\n                    print(f\"Cascading sample to child: {child_name}\")\n                    child_df = self.spark.table(child_name)\n\n                    # Semijoin\n                    # Join on PK-FK to keep only rows matching sampled parents\n                    sampled_child = child_df.join(\n                        sampled_root.select(parent_pk),\n                        child_df[child_col] == sampled_root[parent_pk],\n                        \"inner\"\n                    ).select(child_df.columns) # Keep only child cols\n\n                    sampled_data[child_name] = sampled_child\n\n        return sampled_data\n</code></pre>"},{"location":"api/connectors/#syntho_hive.connectors.sampling.RelationalSampler.sample_relational","title":"sample_relational","text":"<pre><code>sample_relational(root_table: str, sample_size: int, stratify_by: Optional[str] = None) -&gt; Dict[str, DataFrame]\n</code></pre> <p>Sample a root table and cascade the sample to child tables.</p> <p>Parameters:</p> Name Type Description Default <code>root_table</code> <code>str</code> <p>Name of the parent/root table to sample.</p> required <code>sample_size</code> <code>int</code> <p>Approximate number of rows to retain from the root.</p> required <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column for stratified sampling.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, DataFrame]</code> <p>Dictionary mapping table name to sampled Spark DataFrame.</p> Source code in <code>syntho_hive/connectors/sampling.py</code> <pre><code>def sample_relational(\n    self, \n    root_table: str, \n    sample_size: int, \n    stratify_by: Optional[str] = None\n) -&gt; Dict[str, DataFrame]:\n    \"\"\"Sample a root table and cascade the sample to child tables.\n\n    Args:\n        root_table: Name of the parent/root table to sample.\n        sample_size: Approximate number of rows to retain from the root.\n        stratify_by: Optional column for stratified sampling.\n\n    Returns:\n        Dictionary mapping table name to sampled Spark DataFrame.\n    \"\"\"\n    sampled_data = {}\n\n    # 1. Sample Root\n    print(f\"Sampling root table: {root_table}\")\n    # Placeholder for real table loading\n    root_df = self.spark.table(root_table)\n\n    if stratify_by:\n        # Approximate stratified sampling\n        fractions = root_df.select(stratify_by).distinct().withColumn(\"fraction\", F.lit(0.1)).rdd.collectAsMap() \n        # Note: fractions logic needs to be calculated based on sample_size / total_count\n        sampled_root = root_df.sampleBy(stratify_by, fractions, seed=42)\n    else:\n        fraction = min(1.0, sample_size / root_df.count())\n        sampled_root = root_df.sample(withReplacement=False, fraction=fraction, seed=42)\n\n    sampled_data[root_table] = sampled_root\n\n    # 2. Cascade to Children\n    # Simple BFS or using Graph\n    parent_pk = self.metadata.get_table(root_table).pk\n\n    # Find children\n    for child_name, config in self.metadata.tables.items():\n        for child_col, parent_ref in config.fk.items():\n            if parent_ref.startswith(f\"{root_table}.\"):\n                print(f\"Cascading sample to child: {child_name}\")\n                child_df = self.spark.table(child_name)\n\n                # Semijoin\n                # Join on PK-FK to keep only rows matching sampled parents\n                sampled_child = child_df.join(\n                    sampled_root.select(parent_pk),\n                    child_df[child_col] == sampled_root[parent_pk],\n                    \"inner\"\n                ).select(child_df.columns) # Keep only child cols\n\n                sampled_data[child_name] = sampled_child\n\n    return sampled_data\n</code></pre>"},{"location":"api/core/","title":"Core Models &amp; Data","text":"<p>The core module contains the deep learning implementations and data transformation logic.</p>"},{"location":"api/core/#models","title":"Models","text":""},{"location":"api/core/#syntho_hive.core.models.ctgan.CTGAN","title":"syntho_hive.core.models.ctgan.CTGAN","text":"<p>               Bases: <code>ConditionalGenerativeModel</code></p> <p>Conditional Tabular GAN with entity embeddings and parent context.</p> Source code in <code>syntho_hive/core/models/ctgan.py</code> <pre><code>class CTGAN(ConditionalGenerativeModel):\n    \"\"\"Conditional Tabular GAN with entity embeddings and parent context.\"\"\"\n    def __init__(\n        self,\n        metadata: Any,\n        embedding_dim: int = 128,\n        generator_dim: Tuple[int, int] = (256, 256),\n        discriminator_dim: Tuple[int, int] = (256, 256),\n        batch_size: int = 500,\n        epochs: int = 300,\n        device: str = \"cpu\",\n        embedding_threshold: int = 50,\n        discriminator_steps: int = 5\n    ):\n        \"\"\"Create a CTGAN instance configured for tabular synthesis.\n\n        Args:\n            metadata: Table metadata describing columns and constraints.\n            embedding_dim: Dimension of input noise vector.\n            generator_dim: Hidden layer widths for the generator.\n            discriminator_dim: Hidden layer widths for the discriminator.\n            batch_size: Training batch size.\n            epochs: Number of training epochs.\n            device: Torch device string, e.g. ``\"cpu\"`` or ``\"cuda\"``.\n            embedding_threshold: Cardinality threshold for switching to embeddings.\n            discriminator_steps: Number of discriminator steps per generator step.\n        \"\"\"\n        self.metadata = metadata\n        self.embedding_dim = embedding_dim\n        self.generator_dim = generator_dim\n        self.discriminator_dim = discriminator_dim\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.epochs = epochs\n        self.device = torch.device(device)\n        self.discriminator_steps = discriminator_steps\n        # Prioritize init arg, fallback to metadata if available, else default (already 50)\n        self.embedding_threshold = embedding_threshold\n\n        self.generator = None\n        self.discriminator = None\n        self.transformer = DataTransformer(metadata, embedding_threshold=self.embedding_threshold)\n        self.context_transformer = DataTransformer(metadata, embedding_threshold=self.embedding_threshold)\n\n        # Embedding Layers\n        self.embedding_layers = nn.ModuleDict()\n        self.data_column_info = [] # List of tuples: (dim, type, related_info)\n\n\n    def _compile_layout(self, transformer):\n        \"\"\"Analyze transformer output to map column indices and types.\n\n        Args:\n            transformer: Fitted ``DataTransformer`` for the child table.\n        \"\"\"\n        self.data_column_info = []\n        self.embedding_layers = nn.ModuleDict()\n\n        current_idx = 0\n        for col, info in transformer._column_info.items():\n            if info['type'] == 'categorical_embedding':\n                # Create Embedding Layer\n                num_categories = info['num_categories']\n                # Heuristic for embedding dimension: min(50, num_categories/2)\n                emb_dim = min(50, (num_categories + 1) // 2)\n\n                self.embedding_layers[col] = EntityEmbeddingLayer(num_categories, emb_dim).to(self.device)\n\n                self.data_column_info.append({\n                    'name': col,\n                    'type': 'embedding', \n                    'input_idx': current_idx, \n                    'input_dim': 1,\n                    'output_dim': emb_dim,\n                    'num_categories': num_categories \n                })\n                current_idx += 1\n            else:\n                self.data_column_info.append({\n                    'name': col,\n                    'type': 'normal',\n                    'input_idx': current_idx,\n                    'input_dim': info['dim'],\n                    'output_dim': info['dim']\n                })\n                current_idx += info['dim']\n\n    def _apply_embeddings(self, data, is_fake=False):\n        \"\"\"Convert a mixed categorical/continuous tensor into embedding space.\n\n        Args:\n            data: Input tensor with mixed column representations.\n            is_fake: Whether the tensor came from the generator (logits) or real data (indices).\n\n        Returns:\n            Tensor with embeddings applied to categorical columns.\n        \"\"\"\n        parts = []\n        for info in self.data_column_info:\n            idx = info['input_idx']\n            dim = info['input_dim']\n            col_data = data[:, idx:idx+dim]\n\n            if info['type'] == 'embedding':\n                layer = self.embedding_layers[info['name']]\n                if is_fake:\n                    # col_data contains Softmax logits from Generator\n                    # Needs hard Gumbel-Softmax or Softmax? Generator outputs unnormalized logits usually.\n                    # Ideally Generator outputs (N, num_cats). \n                    # Wait, 'data' passed here is strictly what Generator produced.\n                    # Discriminator expects (N, EmbDim).\n\n                    # Logic: Generator outputs Logits. We apply Softmax -&gt; Dense.\n                    # But wait, logic above says Generator outputs: \n                    # Embedding: Logits (dim=num_cats)\n                    # Normal: Values (dim=original_dim)\n\n                    # So 'dim' in loop here must match GENERATOR output structure, not Transformer output.\n                    # Compile Layout logic is slightly tricky because Generator output shape != Transformer output shape for Embeddings.\n\n                    # RE-THINK:\n                    # Transformer Output (Real): [Index] (1 dim)\n                    # Generator Output (Fake): [Logits] (num_cats dim)\n\n                    # This function strictly transforms Real Data (Index) -&gt; Embedding.\n                    # Or Fake Data (Logits) -&gt; Soft Embedding.\n\n                    # Problem: input 'data' has different shapes for Real vs Fake.\n                    # We need to handle them separately or have this function assume inputs are already sliced?\n                    # Let's pass sliced inputs or rely on info having both dims.\n                    pass\n                else:\n                    # Real Data: Indices -&gt; Embedding\n                    # input is (N, 1) indices\n                    embeddings = layer(col_data.long().squeeze(1))\n                    parts.append(embeddings)\n            else:\n                parts.append(col_data)\n\n        # Re-implementing clearer separated logic in Build Model / Forward\n        return torch.cat(parts, dim=1)\n\n    def _build_model(self, transformer_output_dim: int, context_dim: int = 0):\n        \"\"\"Instantiate generator and discriminator modules.\n\n        Args:\n            transformer_output_dim: Flattened dimension of transformed child data.\n            context_dim: Flattened dimension of transformed context (if any).\n        \"\"\"\n        # 1. Compile Layout first\n        self._compile_layout(self.transformer)\n\n        # 2. Calculate Generator Output Dim &amp; Discriminator Input Dim\n        gen_output_dim = 0\n        disc_input_dim_base = 0\n\n        for info in self.data_column_info:\n            if info['type'] == 'embedding':\n                gen_output_dim += info['num_categories'] # Generator outputs logits\n                disc_input_dim_base += info['output_dim'] # D sees embeddings\n            else:\n                gen_output_dim += info['output_dim']\n                disc_input_dim_base += info['output_dim']\n\n        # Generator: Noise + Context -&gt; Data (Logits/Values)\n        gen_input_dim = self.embedding_dim + context_dim\n\n        self.generator = nn.Sequential(\n            ResidualLayer(gen_input_dim, self.generator_dim[0]),\n            ResidualLayer(self.generator_dim[0], self.generator_dim[1]),\n            nn.Linear(self.generator_dim[1], gen_output_dim)\n        ).to(self.device)\n\n        # Discriminator: Data(Embeddings) + Context -&gt; Score\n        disc_input_dim = disc_input_dim_base + context_dim\n\n        self.discriminator = Discriminator(disc_input_dim, self.discriminator_dim[0]).to(self.device)\n\n    def fit(self, data: pd.DataFrame, context: Optional[pd.DataFrame] = None, table_name: Optional[str] = None, checkpoint_dir: Optional[str] = None, log_metrics: bool = True, **kwargs: Any) -&gt; None:\n        \"\"\"Train the CTGAN model on tabular data.\n\n        Args:\n            data: Child table data (target) to model.\n            context: Parent attributes to condition on (aligned row-wise).\n            table_name: Table name for metadata lookup and constraint handling.\n            checkpoint_dir: Directory to save checkpoints (best model, metrics). Defaults to None.\n            log_metrics: Whether to save training metrics to a CSV file. Defaults to True.\n            **kwargs: Extra training options (unused placeholder for compatibility).\n        \"\"\"\n        # 0. Setup Checkpointing\n        if checkpoint_dir:\n            os.makedirs(checkpoint_dir, exist_ok=True)\n\n        best_loss = float('inf')\n        history = []\n        # 1. Fit and Transform Data\n        self.transformer.fit(data, table_name=table_name)\n        train_data = self.transformer.transform(data)\n        train_data = torch.from_numpy(train_data).float().to(self.device)\n\n        # 2. Handle Context\n        if context is not None:\n            assert len(data) == len(context), \"Data and context must have same number of rows\"\n\n            # Use dedicated transformer for context\n            # NOTE: We abuse metdata here slightly. Ideally context comes from a known table (Parent).\n            # But context might be a mix of parent columns. \n            # For fit, we pass table_name=None to fit on just the columns present in context df.\n            self.context_transformer.fit(context)\n            context_transformed = self.context_transformer.transform(context)\n            context_data = torch.from_numpy(context_transformed).float().to(self.device)\n            context_dim = context_data.shape[1]\n        else:\n            context_data = None\n            context_dim = 0\n\n        data_dim = train_data.shape[1]\n\n        # 3. Build Model\n        if self.generator is None:\n            self._build_model(data_dim, context_dim)\n\n        optimizer_G = optim.Adam(self.generator.parameters(), lr=2e-4, betas=(0.5, 0.9))\n        optimizer_D = optim.Adam(self.discriminator.parameters(), lr=2e-4, betas=(0.5, 0.9))\n\n        # 4. Training Loop (WGAN-GP)\n        steps_per_epoch = max(len(train_data) // self.batch_size, 1)\n\n        for epoch in range(self.epochs):\n            for i in range(steps_per_epoch):\n                # --- Train Discriminator ---\n                for _ in range(self.discriminator_steps):\n                    # Sample real data\n                    idx = np.random.randint(0, len(train_data), self.batch_size)\n                    real_data_batch = train_data[idx]\n                    if context_data is not None:\n                        real_context_batch = context_data[idx]\n                        real_input = torch.cat([real_data_batch, real_context_batch], dim=1)\n                    else:\n                        real_context_batch = None\n                        real_input = real_data_batch\n\n                    # Generate fake data\n                    noise = torch.randn(self.batch_size, self.embedding_dim, device=self.device)\n                    if real_context_batch is not None:\n                        gen_input = torch.cat([noise, real_context_batch], dim=1)\n                    else:\n                        gen_input = noise\n\n                    fake_raw = self.generator(gen_input)\n\n                    # Apply Embeddings / Softmax to Fake Data\n                    fake_parts = []\n                    fake_ptr = 0\n                    for info in self.data_column_info:\n                        if info['type'] == 'embedding':\n                            dim = info['num_categories']\n                            logits = fake_raw[:, fake_ptr:fake_ptr+dim]\n                            fake_ptr += dim\n\n                            # Gumbel Softmax or Softmax? WGAN prefers generic softmax for differentiability\n                            # Note: Gumbel Softmax allows hard sampling with gradients.\n                            probs = F.softmax(logits, dim=1)\n                            emb_vect = self.embedding_layers[info['name']].forward_soft(probs)\n                            fake_parts.append(emb_vect)\n                        else:\n                            dim = info['output_dim']\n                            val = fake_raw[:, fake_ptr:fake_ptr+dim]\n                            fake_ptr += dim\n                            fake_parts.append(val)\n\n                    fake_data_batch = torch.cat(fake_parts, dim=1)\n\n                    # Apply Embeddings to Real Data\n                    real_parts = []\n                    real_ptr = 0\n                    # Need to iterate column info again to slice real data correctly\n                    # Real data from transformer is concatenated (Indices, Values...)\n                    for info in self.data_column_info:\n                        dim = info['input_dim'] # 1 for embedding (index)\n                        col_data = real_data_batch[:, real_ptr:real_ptr+dim]\n                        real_ptr += dim\n\n                        if info['type'] == 'embedding':\n                            emb_vect = self.embedding_layers[info['name']](col_data.long().squeeze(1))\n                            real_parts.append(emb_vect)\n                        else:\n                            real_parts.append(col_data)\n\n                    real_data_processed = torch.cat(real_parts, dim=1)\n\n                    if real_context_batch is not None:\n                        fake_input = torch.cat([fake_data_batch, real_context_batch], dim=1)\n                        real_input_processed = torch.cat([real_data_processed, real_context_batch], dim=1)\n                    else:\n                        fake_input = fake_data_batch\n                        real_input_processed = real_data_processed\n\n                    # Compute WGAN loss\n                    d_real = self.discriminator(real_input_processed)\n                    d_fake = self.discriminator(fake_input)\n\n                    # Gradient Penalty\n                    gp = compute_gradient_penalty(self.discriminator, real_input_processed, fake_input, self.device)\n\n                    loss_D = -torch.mean(d_real) + torch.mean(d_fake) + 10.0 * gp\n\n                    optimizer_D.zero_grad()\n                    loss_D.backward()\n                    optimizer_D.step()\n\n                # --- Train Generator ---\n                # Train generator once after n_critic discriminator steps\n                noise = torch.randn(self.batch_size, self.embedding_dim, device=self.device)\n                if real_context_batch is not None:\n                    # Re-sample context for generator training?? \n                    # Ideally yes, but reusing batch is fine for conditional stability\n                    # We'll stick to reusing the last seen batch for simplicity/stability\n                    gen_input = torch.cat([noise, real_context_batch], dim=1)\n                else:\n                    gen_input = noise\n\n                fake_raw = self.generator(gen_input)\n\n                # Apply Embeddings / Softmax (Same logic as above)\n                fake_parts = []\n                fake_ptr = 0\n                for info in self.data_column_info:\n                    if info['type'] == 'embedding':\n                        dim = info['num_categories']\n                        logits = fake_raw[:, fake_ptr:fake_ptr+dim]\n                        fake_ptr += dim\n                        probs = F.softmax(logits, dim=1)\n                        emb_vect = self.embedding_layers[info['name']].forward_soft(probs)\n                        fake_parts.append(emb_vect)\n                    else:\n                        dim = info['output_dim']\n                        val = fake_raw[:, fake_ptr:fake_ptr+dim]\n                        fake_ptr += dim\n                        fake_parts.append(val)\n\n                fake_data_batch = torch.cat(fake_parts, dim=1)\n\n                if real_context_batch is not None:\n                    fake_input = torch.cat([fake_data_batch, real_context_batch], dim=1)\n                else:\n                    fake_input = fake_data_batch\n\n                d_fake = self.discriminator(fake_input)\n                loss_G = -torch.mean(d_fake)\n\n                optimizer_G.zero_grad()\n                loss_G.backward()\n                optimizer_G.step()\n\n            if epoch % 10 == 0:\n                print(f\"Epoch {epoch}: Loss D={loss_D.item():.4f}, Loss G={loss_G.item():.4f}\")\n\n            # --- Checkpointing &amp; Logging ---\n            current_loss_g = loss_G.item()\n            current_loss_d = loss_D.item()\n\n            if log_metrics:\n                history.append({\n                    'epoch': epoch,\n                    'loss_g': current_loss_g,\n                    'loss_d': current_loss_d\n                })\n\n            if checkpoint_dir and current_loss_g &lt; best_loss:\n                best_loss = current_loss_g\n                self.save(os.path.join(checkpoint_dir, \"best_model.pt\"))\n\n        # End of training: Save metrics and last model\n        if checkpoint_dir:\n            self.save(os.path.join(checkpoint_dir, \"last_model.pt\"))\n\n            if log_metrics and history:\n                metrics_path = os.path.join(checkpoint_dir, \"training_metrics.csv\")\n                keys = history[0].keys()\n                with open(metrics_path, 'w', newline='') as f:\n                    dict_writer = csv.DictWriter(f, fieldnames=keys)\n                    dict_writer.writeheader()\n                    dict_writer.writerows(history)\n                print(f\"Training metrics saved to {metrics_path}\")\n\n    def sample(self, num_rows: int, context: Optional[pd.DataFrame] = None, **kwargs: Any) -&gt; pd.DataFrame:\n        \"\"\"Generate synthetic samples, optionally conditioned on parent context.\n\n        Args:\n            num_rows: Number of rows to generate.\n            context: Optional parent attributes aligned to the requested rows.\n            **kwargs: Additional sampling controls (unused placeholder).\n\n        Returns:\n            DataFrame of synthetic rows mapped back to original schema.\n        \"\"\"\n        self.generator.eval()\n        with torch.no_grad():\n            noise = torch.randn(num_rows, self.embedding_dim, device=self.device)\n\n            if context is not None:\n                # Assuming context is provided for exactly num_rows\n                assert len(context) == num_rows\n\n                # Transform context using the fitted context transformer\n                context_transformed = self.context_transformer.transform(context)\n                context_data = torch.from_numpy(context_transformed).float().to(self.device)\n\n                gen_input = torch.cat([noise, context_data], dim=1)\n            else:\n                gen_input = noise\n\n            fake_raw = self.generator(gen_input)\n\n            # Post-process logits to indices for output\n            output_parts = []\n            fake_ptr = 0\n            for info in self.data_column_info:\n                if info['type'] == 'embedding':\n                    dim = info['num_categories']\n                    logits = fake_raw[:, fake_ptr:fake_ptr+dim]\n                    fake_ptr += dim\n\n                    # Argmax to get index\n                    indices = torch.argmax(logits, dim=1, keepdim=True)\n                    output_parts.append(indices.cpu().numpy())\n                else:\n                    dim = info['output_dim']\n                    val = fake_raw[:, fake_ptr:fake_ptr+dim]\n                    fake_ptr += dim\n                    output_parts.append(val.cpu().numpy())\n\n            fake_data_np = np.concatenate(output_parts, axis=1)\n\n        return self.transformer.inverse_transform(fake_data_np)\n\n    def save(self, path: str) -&gt; None:\n        \"\"\"Persist generator and discriminator state dicts to disk.\n\n        Args:\n            path: Filesystem path to write the checkpoint to.\n        \"\"\"\n        torch.save({\n            \"generator\": self.generator.state_dict(),\n            \"discriminator\": self.discriminator.state_dict(),\n            # Ideally we pickle the transformer, but for now we assume it's reconstructible or part of the object state\n            # A distinct save mechanism for the full object is better (e.g. pickle or joblib)\n        }, path)\n\n    def load(self, path: str) -&gt; None:\n        \"\"\"Load generator and discriminator weights from disk.\n\n        Args:\n            path: Filesystem path containing a saved checkpoint.\n        \"\"\"\n        checkpoint = torch.load(path)\n        self.generator.load_state_dict(checkpoint[\"generator\"])\n        self.discriminator.load_state_dict(checkpoint[\"discriminator\"])\n</code></pre>"},{"location":"api/core/#syntho_hive.core.models.ctgan.CTGAN.fit","title":"fit","text":"<pre><code>fit(data: DataFrame, context: Optional[DataFrame] = None, table_name: Optional[str] = None, checkpoint_dir: Optional[str] = None, log_metrics: bool = True, **kwargs: Any) -&gt; None\n</code></pre> <p>Train the CTGAN model on tabular data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Child table data (target) to model.</p> required <code>context</code> <code>Optional[DataFrame]</code> <p>Parent attributes to condition on (aligned row-wise).</p> <code>None</code> <code>table_name</code> <code>Optional[str]</code> <p>Table name for metadata lookup and constraint handling.</p> <code>None</code> <code>checkpoint_dir</code> <code>Optional[str]</code> <p>Directory to save checkpoints (best model, metrics). Defaults to None.</p> <code>None</code> <code>log_metrics</code> <code>bool</code> <p>Whether to save training metrics to a CSV file. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Extra training options (unused placeholder for compatibility).</p> <code>{}</code> Source code in <code>syntho_hive/core/models/ctgan.py</code> <pre><code>def fit(self, data: pd.DataFrame, context: Optional[pd.DataFrame] = None, table_name: Optional[str] = None, checkpoint_dir: Optional[str] = None, log_metrics: bool = True, **kwargs: Any) -&gt; None:\n    \"\"\"Train the CTGAN model on tabular data.\n\n    Args:\n        data: Child table data (target) to model.\n        context: Parent attributes to condition on (aligned row-wise).\n        table_name: Table name for metadata lookup and constraint handling.\n        checkpoint_dir: Directory to save checkpoints (best model, metrics). Defaults to None.\n        log_metrics: Whether to save training metrics to a CSV file. Defaults to True.\n        **kwargs: Extra training options (unused placeholder for compatibility).\n    \"\"\"\n    # 0. Setup Checkpointing\n    if checkpoint_dir:\n        os.makedirs(checkpoint_dir, exist_ok=True)\n\n    best_loss = float('inf')\n    history = []\n    # 1. Fit and Transform Data\n    self.transformer.fit(data, table_name=table_name)\n    train_data = self.transformer.transform(data)\n    train_data = torch.from_numpy(train_data).float().to(self.device)\n\n    # 2. Handle Context\n    if context is not None:\n        assert len(data) == len(context), \"Data and context must have same number of rows\"\n\n        # Use dedicated transformer for context\n        # NOTE: We abuse metdata here slightly. Ideally context comes from a known table (Parent).\n        # But context might be a mix of parent columns. \n        # For fit, we pass table_name=None to fit on just the columns present in context df.\n        self.context_transformer.fit(context)\n        context_transformed = self.context_transformer.transform(context)\n        context_data = torch.from_numpy(context_transformed).float().to(self.device)\n        context_dim = context_data.shape[1]\n    else:\n        context_data = None\n        context_dim = 0\n\n    data_dim = train_data.shape[1]\n\n    # 3. Build Model\n    if self.generator is None:\n        self._build_model(data_dim, context_dim)\n\n    optimizer_G = optim.Adam(self.generator.parameters(), lr=2e-4, betas=(0.5, 0.9))\n    optimizer_D = optim.Adam(self.discriminator.parameters(), lr=2e-4, betas=(0.5, 0.9))\n\n    # 4. Training Loop (WGAN-GP)\n    steps_per_epoch = max(len(train_data) // self.batch_size, 1)\n\n    for epoch in range(self.epochs):\n        for i in range(steps_per_epoch):\n            # --- Train Discriminator ---\n            for _ in range(self.discriminator_steps):\n                # Sample real data\n                idx = np.random.randint(0, len(train_data), self.batch_size)\n                real_data_batch = train_data[idx]\n                if context_data is not None:\n                    real_context_batch = context_data[idx]\n                    real_input = torch.cat([real_data_batch, real_context_batch], dim=1)\n                else:\n                    real_context_batch = None\n                    real_input = real_data_batch\n\n                # Generate fake data\n                noise = torch.randn(self.batch_size, self.embedding_dim, device=self.device)\n                if real_context_batch is not None:\n                    gen_input = torch.cat([noise, real_context_batch], dim=1)\n                else:\n                    gen_input = noise\n\n                fake_raw = self.generator(gen_input)\n\n                # Apply Embeddings / Softmax to Fake Data\n                fake_parts = []\n                fake_ptr = 0\n                for info in self.data_column_info:\n                    if info['type'] == 'embedding':\n                        dim = info['num_categories']\n                        logits = fake_raw[:, fake_ptr:fake_ptr+dim]\n                        fake_ptr += dim\n\n                        # Gumbel Softmax or Softmax? WGAN prefers generic softmax for differentiability\n                        # Note: Gumbel Softmax allows hard sampling with gradients.\n                        probs = F.softmax(logits, dim=1)\n                        emb_vect = self.embedding_layers[info['name']].forward_soft(probs)\n                        fake_parts.append(emb_vect)\n                    else:\n                        dim = info['output_dim']\n                        val = fake_raw[:, fake_ptr:fake_ptr+dim]\n                        fake_ptr += dim\n                        fake_parts.append(val)\n\n                fake_data_batch = torch.cat(fake_parts, dim=1)\n\n                # Apply Embeddings to Real Data\n                real_parts = []\n                real_ptr = 0\n                # Need to iterate column info again to slice real data correctly\n                # Real data from transformer is concatenated (Indices, Values...)\n                for info in self.data_column_info:\n                    dim = info['input_dim'] # 1 for embedding (index)\n                    col_data = real_data_batch[:, real_ptr:real_ptr+dim]\n                    real_ptr += dim\n\n                    if info['type'] == 'embedding':\n                        emb_vect = self.embedding_layers[info['name']](col_data.long().squeeze(1))\n                        real_parts.append(emb_vect)\n                    else:\n                        real_parts.append(col_data)\n\n                real_data_processed = torch.cat(real_parts, dim=1)\n\n                if real_context_batch is not None:\n                    fake_input = torch.cat([fake_data_batch, real_context_batch], dim=1)\n                    real_input_processed = torch.cat([real_data_processed, real_context_batch], dim=1)\n                else:\n                    fake_input = fake_data_batch\n                    real_input_processed = real_data_processed\n\n                # Compute WGAN loss\n                d_real = self.discriminator(real_input_processed)\n                d_fake = self.discriminator(fake_input)\n\n                # Gradient Penalty\n                gp = compute_gradient_penalty(self.discriminator, real_input_processed, fake_input, self.device)\n\n                loss_D = -torch.mean(d_real) + torch.mean(d_fake) + 10.0 * gp\n\n                optimizer_D.zero_grad()\n                loss_D.backward()\n                optimizer_D.step()\n\n            # --- Train Generator ---\n            # Train generator once after n_critic discriminator steps\n            noise = torch.randn(self.batch_size, self.embedding_dim, device=self.device)\n            if real_context_batch is not None:\n                # Re-sample context for generator training?? \n                # Ideally yes, but reusing batch is fine for conditional stability\n                # We'll stick to reusing the last seen batch for simplicity/stability\n                gen_input = torch.cat([noise, real_context_batch], dim=1)\n            else:\n                gen_input = noise\n\n            fake_raw = self.generator(gen_input)\n\n            # Apply Embeddings / Softmax (Same logic as above)\n            fake_parts = []\n            fake_ptr = 0\n            for info in self.data_column_info:\n                if info['type'] == 'embedding':\n                    dim = info['num_categories']\n                    logits = fake_raw[:, fake_ptr:fake_ptr+dim]\n                    fake_ptr += dim\n                    probs = F.softmax(logits, dim=1)\n                    emb_vect = self.embedding_layers[info['name']].forward_soft(probs)\n                    fake_parts.append(emb_vect)\n                else:\n                    dim = info['output_dim']\n                    val = fake_raw[:, fake_ptr:fake_ptr+dim]\n                    fake_ptr += dim\n                    fake_parts.append(val)\n\n            fake_data_batch = torch.cat(fake_parts, dim=1)\n\n            if real_context_batch is not None:\n                fake_input = torch.cat([fake_data_batch, real_context_batch], dim=1)\n            else:\n                fake_input = fake_data_batch\n\n            d_fake = self.discriminator(fake_input)\n            loss_G = -torch.mean(d_fake)\n\n            optimizer_G.zero_grad()\n            loss_G.backward()\n            optimizer_G.step()\n\n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch}: Loss D={loss_D.item():.4f}, Loss G={loss_G.item():.4f}\")\n\n        # --- Checkpointing &amp; Logging ---\n        current_loss_g = loss_G.item()\n        current_loss_d = loss_D.item()\n\n        if log_metrics:\n            history.append({\n                'epoch': epoch,\n                'loss_g': current_loss_g,\n                'loss_d': current_loss_d\n            })\n\n        if checkpoint_dir and current_loss_g &lt; best_loss:\n            best_loss = current_loss_g\n            self.save(os.path.join(checkpoint_dir, \"best_model.pt\"))\n\n    # End of training: Save metrics and last model\n    if checkpoint_dir:\n        self.save(os.path.join(checkpoint_dir, \"last_model.pt\"))\n\n        if log_metrics and history:\n            metrics_path = os.path.join(checkpoint_dir, \"training_metrics.csv\")\n            keys = history[0].keys()\n            with open(metrics_path, 'w', newline='') as f:\n                dict_writer = csv.DictWriter(f, fieldnames=keys)\n                dict_writer.writeheader()\n                dict_writer.writerows(history)\n            print(f\"Training metrics saved to {metrics_path}\")\n</code></pre>"},{"location":"api/core/#syntho_hive.core.models.ctgan.CTGAN.sample","title":"sample","text":"<pre><code>sample(num_rows: int, context: Optional[DataFrame] = None, **kwargs: Any) -&gt; pd.DataFrame\n</code></pre> <p>Generate synthetic samples, optionally conditioned on parent context.</p> <p>Parameters:</p> Name Type Description Default <code>num_rows</code> <code>int</code> <p>Number of rows to generate.</p> required <code>context</code> <code>Optional[DataFrame]</code> <p>Optional parent attributes aligned to the requested rows.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional sampling controls (unused placeholder).</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame of synthetic rows mapped back to original schema.</p> Source code in <code>syntho_hive/core/models/ctgan.py</code> <pre><code>def sample(self, num_rows: int, context: Optional[pd.DataFrame] = None, **kwargs: Any) -&gt; pd.DataFrame:\n    \"\"\"Generate synthetic samples, optionally conditioned on parent context.\n\n    Args:\n        num_rows: Number of rows to generate.\n        context: Optional parent attributes aligned to the requested rows.\n        **kwargs: Additional sampling controls (unused placeholder).\n\n    Returns:\n        DataFrame of synthetic rows mapped back to original schema.\n    \"\"\"\n    self.generator.eval()\n    with torch.no_grad():\n        noise = torch.randn(num_rows, self.embedding_dim, device=self.device)\n\n        if context is not None:\n            # Assuming context is provided for exactly num_rows\n            assert len(context) == num_rows\n\n            # Transform context using the fitted context transformer\n            context_transformed = self.context_transformer.transform(context)\n            context_data = torch.from_numpy(context_transformed).float().to(self.device)\n\n            gen_input = torch.cat([noise, context_data], dim=1)\n        else:\n            gen_input = noise\n\n        fake_raw = self.generator(gen_input)\n\n        # Post-process logits to indices for output\n        output_parts = []\n        fake_ptr = 0\n        for info in self.data_column_info:\n            if info['type'] == 'embedding':\n                dim = info['num_categories']\n                logits = fake_raw[:, fake_ptr:fake_ptr+dim]\n                fake_ptr += dim\n\n                # Argmax to get index\n                indices = torch.argmax(logits, dim=1, keepdim=True)\n                output_parts.append(indices.cpu().numpy())\n            else:\n                dim = info['output_dim']\n                val = fake_raw[:, fake_ptr:fake_ptr+dim]\n                fake_ptr += dim\n                output_parts.append(val.cpu().numpy())\n\n        fake_data_np = np.concatenate(output_parts, axis=1)\n\n    return self.transformer.inverse_transform(fake_data_np)\n</code></pre>"},{"location":"api/core/#syntho_hive.core.models.ctgan.CTGAN.save","title":"save","text":"<pre><code>save(path: str) -&gt; None\n</code></pre> <p>Persist generator and discriminator state dicts to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Filesystem path to write the checkpoint to.</p> required Source code in <code>syntho_hive/core/models/ctgan.py</code> <pre><code>def save(self, path: str) -&gt; None:\n    \"\"\"Persist generator and discriminator state dicts to disk.\n\n    Args:\n        path: Filesystem path to write the checkpoint to.\n    \"\"\"\n    torch.save({\n        \"generator\": self.generator.state_dict(),\n        \"discriminator\": self.discriminator.state_dict(),\n        # Ideally we pickle the transformer, but for now we assume it's reconstructible or part of the object state\n        # A distinct save mechanism for the full object is better (e.g. pickle or joblib)\n    }, path)\n</code></pre>"},{"location":"api/core/#syntho_hive.core.models.ctgan.CTGAN.load","title":"load","text":"<pre><code>load(path: str) -&gt; None\n</code></pre> <p>Load generator and discriminator weights from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Filesystem path containing a saved checkpoint.</p> required Source code in <code>syntho_hive/core/models/ctgan.py</code> <pre><code>def load(self, path: str) -&gt; None:\n    \"\"\"Load generator and discriminator weights from disk.\n\n    Args:\n        path: Filesystem path containing a saved checkpoint.\n    \"\"\"\n    checkpoint = torch.load(path)\n    self.generator.load_state_dict(checkpoint[\"generator\"])\n    self.discriminator.load_state_dict(checkpoint[\"discriminator\"])\n</code></pre>"},{"location":"api/core/#syntho_hive.core.models.base.ConditionalGenerativeModel","title":"syntho_hive.core.models.base.ConditionalGenerativeModel","text":"<p>               Bases: <code>GenerativeModel</code></p> <p>Contract for models that condition on parent context during training/sampling.</p> Source code in <code>syntho_hive/core/models/base.py</code> <pre><code>class ConditionalGenerativeModel(GenerativeModel):\n    \"\"\"Contract for models that condition on parent context during training/sampling.\"\"\"\n\n    @abstractmethod\n    def fit(self, data: pd.DataFrame, context: Optional[pd.DataFrame] = None, **kwargs: Any) -&gt; None:\n        \"\"\"Train the model with optional parent context.\n\n        Args:\n            data: Child table data to learn from.\n            context: Optional parent attributes used for conditioning.\n            **kwargs: Model-specific training options.\n        \"\"\"\n        pass  # pragma: no cover\n\n    @abstractmethod\n    def sample(self, num_rows: int, context: Optional[pd.DataFrame] = None, **kwargs: Any) -&gt; pd.DataFrame:\n        \"\"\"Generate synthetic rows with optional conditioning context.\n\n        Args:\n            num_rows: Number of rows to generate.\n            context: Optional parent attributes aligned to the requested rows.\n            **kwargs: Additional sampling controls.\n\n        Returns:\n            DataFrame of synthetic samples aligned to the provided context (if any).\n        \"\"\"\n        pass  # pragma: no cover\n</code></pre>"},{"location":"api/core/#syntho_hive.core.models.base.ConditionalGenerativeModel.fit","title":"fit  <code>abstractmethod</code>","text":"<pre><code>fit(data: DataFrame, context: Optional[DataFrame] = None, **kwargs: Any) -&gt; None\n</code></pre> <p>Train the model with optional parent context.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Child table data to learn from.</p> required <code>context</code> <code>Optional[DataFrame]</code> <p>Optional parent attributes used for conditioning.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Model-specific training options.</p> <code>{}</code> Source code in <code>syntho_hive/core/models/base.py</code> <pre><code>@abstractmethod\ndef fit(self, data: pd.DataFrame, context: Optional[pd.DataFrame] = None, **kwargs: Any) -&gt; None:\n    \"\"\"Train the model with optional parent context.\n\n    Args:\n        data: Child table data to learn from.\n        context: Optional parent attributes used for conditioning.\n        **kwargs: Model-specific training options.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"api/core/#syntho_hive.core.models.base.ConditionalGenerativeModel.sample","title":"sample  <code>abstractmethod</code>","text":"<pre><code>sample(num_rows: int, context: Optional[DataFrame] = None, **kwargs: Any) -&gt; pd.DataFrame\n</code></pre> <p>Generate synthetic rows with optional conditioning context.</p> <p>Parameters:</p> Name Type Description Default <code>num_rows</code> <code>int</code> <p>Number of rows to generate.</p> required <code>context</code> <code>Optional[DataFrame]</code> <p>Optional parent attributes aligned to the requested rows.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional sampling controls.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame of synthetic samples aligned to the provided context (if any).</p> Source code in <code>syntho_hive/core/models/base.py</code> <pre><code>@abstractmethod\ndef sample(self, num_rows: int, context: Optional[pd.DataFrame] = None, **kwargs: Any) -&gt; pd.DataFrame:\n    \"\"\"Generate synthetic rows with optional conditioning context.\n\n    Args:\n        num_rows: Number of rows to generate.\n        context: Optional parent attributes aligned to the requested rows.\n        **kwargs: Additional sampling controls.\n\n    Returns:\n        DataFrame of synthetic samples aligned to the provided context (if any).\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"api/core/#data-transformation","title":"Data Transformation","text":""},{"location":"api/core/#syntho_hive.core.data.transformer.DataTransformer","title":"syntho_hive.core.data.transformer.DataTransformer","text":"<p>Reversible transformer for tabular data.</p> <p>Continuous columns use a Bayesian GMM-based normalizer, while categorical columns are either one-hot encoded or mapped to indices for embeddings.</p> Source code in <code>syntho_hive/core/data/transformer.py</code> <pre><code>class DataTransformer:\n    \"\"\"Reversible transformer for tabular data.\n\n    Continuous columns use a Bayesian GMM-based normalizer, while categorical\n    columns are either one-hot encoded or mapped to indices for embeddings.\n    \"\"\"\n\n    def __init__(self, metadata: Any, embedding_threshold: int = 50):\n        \"\"\"Create a transformer configured by table metadata.\n\n        Args:\n            metadata: Metadata object describing tables, keys, and constraints.\n            embedding_threshold: Switch to embedding mode when cardinality exceeds this value.\n        \"\"\"\n        self.metadata = metadata\n        self.embedding_threshold = embedding_threshold\n        self._transformers = {}\n        self._column_info = {} # Maps col_name -&gt; {'type': str, 'dim': int, 'transformer': obj}\n        self.output_dim = 0\n        self._excluded_columns = []\n\n    def _prepare_categorical(self, series: pd.Series) -&gt; pd.Series:\n        \"\"\"Fill nulls with a sentinel value and ensure string type.\"\"\"\n        # Convert to object to handle mixed types (e.g. numbers and NaNs)\n        series = series.astype(object)\n        return series.fillna('&lt;NAN&gt;').astype(str)\n\n\n    def fit(self, data: pd.DataFrame, table_name: Optional[str] = None):\n        \"\"\"Fit per-column transformers and collect column layout metadata.\n\n        Args:\n            data: DataFrame to profile and transform.\n            table_name: Optional table name for applying PK/FK exclusions and constraints.\n\n        Raises:\n            ValueError: If metadata is missing table configurations.\n        \"\"\"\n        self.table_name = table_name # Store for constraint application later\n        if not self.metadata.tables:\n            raise ValueError(\"Metadata must be populated with table configs\")\n\n        columns_to_transform = data.columns.tolist()\n\n        # Handle relational constraints if table_name is provided\n        if table_name:\n            table_config = self.metadata.get_table(table_name)\n            if table_config:\n                # Exclude PK and FKs from transformation\n                pk = table_config.pk\n                fks = list(table_config.fk.keys())\n                self._excluded_columns = [pk] + fks\n                columns_to_transform = [c for c in columns_to_transform if c not in self._excluded_columns]\n\n        self.output_dim = 0\n\n        for col in columns_to_transform:\n            col_data = data[col]\n\n            if pd.api.types.is_numeric_dtype(col_data):\n                # Continuous column\n                transformer = ClusterBasedNormalizer(n_components=10)\n                transformer.fit(col_data)\n\n                # Dim is managed by the transformer now (dynamic based on nulls)\n                dim = transformer.output_dim\n                self._transformers[col] = transformer\n                self._column_info[col] = {\n                    'type': 'continuous',\n                    'dim': dim,\n                    'transformer': transformer\n                }\n                self.output_dim += dim\n\n            else:\n                # Categorical column\n                # Use OneHotEncoder for now. \n                # Categorical column\n                # Check cardinality for embedding suggestion\n                n_unique = col_data.nunique()\n                if n_unique &gt; self.embedding_threshold:\n                    # Use LabelEncoder for Entity Embeddings\n                    from sklearn.preprocessing import LabelEncoder\n                    transformer = LabelEncoder()\n\n                    # Fill Nulls\n                    col_data_filled = self._prepare_categorical(col_data)\n\n                    # LabelEncoder expects 1D array\n                    transformer.fit(col_data_filled)\n\n                    dim = 1 # Just the index\n                    num_categories = len(transformer.classes_)  # include sentinel for nulls\n                    self._transformers[col] = transformer\n                    self._column_info[col] = {\n                        'type': 'categorical_embedding',\n                        'dim': dim,\n                        'num_categories': num_categories,\n                        'transformer': transformer\n                    }\n                    self.output_dim += dim\n                else:\n                    # Use OneHotEncoder\n                    transformer = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n\n                    # Fill Nulls\n                    col_data_filled = self._prepare_categorical(col_data)\n\n                    values = col_data_filled.values.reshape(-1, 1)\n                    transformer.fit(values)\n\n                    dim = len(transformer.categories_[0])\n                    self._transformers[col] = transformer\n                    self._column_info[col] = {\n                        'type': 'categorical',\n                        'dim': dim,\n                        'transformer': transformer\n                    }\n                    self.output_dim += dim\n\n    def transform(self, data: pd.DataFrame) -&gt; np.ndarray:\n        \"\"\"Transform a dataframe into model-ready numpy arrays.\n\n        Args:\n            data: DataFrame with the same columns used during ``fit``.\n\n        Raises:\n            ValueError: If the transformer has not been fitted or a column is missing.\n\n        Returns:\n            Concatenated numpy array representing all transformed columns.\n        \"\"\"\n        if not self._transformers:\n            raise ValueError(\"Transformer has not been fitted.\")\n\n        output_arrays = []\n\n        # Iterate in the same order as fit/stored in _column_info\n        for col, info in self._column_info.items():\n            if col not in data.columns:\n                raise ValueError(f\"Column {col} missing from input data\")\n\n            transformer = self._transformers[col]\n            col_data = data[col]\n\n            if info['type'] == 'continuous':\n                # Returns (N, n_components + 1 [+ 1 if nulls])\n                transformed = transformer.transform(col_data)\n            elif info['type'] == 'categorical_embedding':\n                # Returns (N, 1)\n                col_data_filled = self._prepare_categorical(col_data)\n                values = transformer.transform(col_data_filled)\n                transformed = values.reshape(-1, 1)\n            else:\n                # Returns (N, n_categories)\n                col_data_filled = self._prepare_categorical(col_data)\n                values = col_data_filled.values.reshape(-1, 1)\n                transformed = transformer.transform(values)\n\n            output_arrays.append(transformed)\n\n        return np.concatenate(output_arrays, axis=1)\n\n    def inverse_transform(self, data: np.ndarray) -&gt; pd.DataFrame:\n        \"\"\"Convert model outputs back to the original dataframe schema.\n\n        Args:\n            data: Numpy array produced by a model, aligned to transform layout.\n\n        Raises:\n            ValueError: If called before ``fit``.\n\n        Returns:\n            DataFrame with original column names and value types (constraints applied).\n        \"\"\"\n        if not self._transformers:\n            raise ValueError(\"Transformer has not been fitted.\")\n\n        output_df = pd.DataFrame()\n        start_idx = 0\n\n        for col, info in self._column_info.items():\n            dim = info['dim']\n            end_idx = start_idx + dim\n            col_data = data[:, start_idx:end_idx]\n\n            transformer = self._transformers[col]\n\n            if info['type'] == 'continuous':\n                original_values = transformer.inverse_transform(col_data)\n            elif info['type'] == 'categorical_embedding':\n                 # col_data is (N, 1) floats/ints. \n                 # We need ints for LabelEncoder.\n                 indices = np.clip(col_data.flatten().astype(int), 0, info['num_categories'] - 1)\n                 original_values = transformer.inverse_transform(indices)\n                 # Restore NaNs\n                 original_values = pd.Series(original_values).replace('&lt;NAN&gt;', np.nan).values\n            else:\n                original_values = transformer.inverse_transform(col_data).flatten()\n                # Restore NaNs\n                original_values = pd.Series(original_values).replace('&lt;NAN&gt;', np.nan).values\n\n            # Apply Constraints\n            if self.metadata and hasattr(self, 'table_name') and self.table_name:\n                table_config = self.metadata.get_table(self.table_name)\n                if table_config and col in table_config.constraints:\n                    constraint = table_config.constraints[col]\n\n                    # 1. Rounding/Type\n                    if constraint.dtype == \"int\":\n                        original_values = np.round(original_values).astype(int)\n\n                    # 2. Clipping\n                    if constraint.min is not None or constraint.max is not None:\n                        # Handle potential pandas Series or numpy array\n                        if isinstance(original_values, pd.Series):\n                            original_values = original_values.clip(lower=constraint.min, upper=constraint.max)\n                        else:\n                            original_values = np.clip(original_values, constraint.min, constraint.max)\n\n            output_df[col] = original_values\n            start_idx = end_idx\n\n        return output_df\n</code></pre>"},{"location":"api/core/#syntho_hive.core.data.transformer.DataTransformer.fit","title":"fit","text":"<pre><code>fit(data: DataFrame, table_name: Optional[str] = None)\n</code></pre> <p>Fit per-column transformers and collect column layout metadata.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame to profile and transform.</p> required <code>table_name</code> <code>Optional[str]</code> <p>Optional table name for applying PK/FK exclusions and constraints.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If metadata is missing table configurations.</p> Source code in <code>syntho_hive/core/data/transformer.py</code> <pre><code>def fit(self, data: pd.DataFrame, table_name: Optional[str] = None):\n    \"\"\"Fit per-column transformers and collect column layout metadata.\n\n    Args:\n        data: DataFrame to profile and transform.\n        table_name: Optional table name for applying PK/FK exclusions and constraints.\n\n    Raises:\n        ValueError: If metadata is missing table configurations.\n    \"\"\"\n    self.table_name = table_name # Store for constraint application later\n    if not self.metadata.tables:\n        raise ValueError(\"Metadata must be populated with table configs\")\n\n    columns_to_transform = data.columns.tolist()\n\n    # Handle relational constraints if table_name is provided\n    if table_name:\n        table_config = self.metadata.get_table(table_name)\n        if table_config:\n            # Exclude PK and FKs from transformation\n            pk = table_config.pk\n            fks = list(table_config.fk.keys())\n            self._excluded_columns = [pk] + fks\n            columns_to_transform = [c for c in columns_to_transform if c not in self._excluded_columns]\n\n    self.output_dim = 0\n\n    for col in columns_to_transform:\n        col_data = data[col]\n\n        if pd.api.types.is_numeric_dtype(col_data):\n            # Continuous column\n            transformer = ClusterBasedNormalizer(n_components=10)\n            transformer.fit(col_data)\n\n            # Dim is managed by the transformer now (dynamic based on nulls)\n            dim = transformer.output_dim\n            self._transformers[col] = transformer\n            self._column_info[col] = {\n                'type': 'continuous',\n                'dim': dim,\n                'transformer': transformer\n            }\n            self.output_dim += dim\n\n        else:\n            # Categorical column\n            # Use OneHotEncoder for now. \n            # Categorical column\n            # Check cardinality for embedding suggestion\n            n_unique = col_data.nunique()\n            if n_unique &gt; self.embedding_threshold:\n                # Use LabelEncoder for Entity Embeddings\n                from sklearn.preprocessing import LabelEncoder\n                transformer = LabelEncoder()\n\n                # Fill Nulls\n                col_data_filled = self._prepare_categorical(col_data)\n\n                # LabelEncoder expects 1D array\n                transformer.fit(col_data_filled)\n\n                dim = 1 # Just the index\n                num_categories = len(transformer.classes_)  # include sentinel for nulls\n                self._transformers[col] = transformer\n                self._column_info[col] = {\n                    'type': 'categorical_embedding',\n                    'dim': dim,\n                    'num_categories': num_categories,\n                    'transformer': transformer\n                }\n                self.output_dim += dim\n            else:\n                # Use OneHotEncoder\n                transformer = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n\n                # Fill Nulls\n                col_data_filled = self._prepare_categorical(col_data)\n\n                values = col_data_filled.values.reshape(-1, 1)\n                transformer.fit(values)\n\n                dim = len(transformer.categories_[0])\n                self._transformers[col] = transformer\n                self._column_info[col] = {\n                    'type': 'categorical',\n                    'dim': dim,\n                    'transformer': transformer\n                }\n                self.output_dim += dim\n</code></pre>"},{"location":"api/core/#syntho_hive.core.data.transformer.DataTransformer.transform","title":"transform","text":"<pre><code>transform(data: DataFrame) -&gt; np.ndarray\n</code></pre> <p>Transform a dataframe into model-ready numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame with the same columns used during <code>fit</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the transformer has not been fitted or a column is missing.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Concatenated numpy array representing all transformed columns.</p> Source code in <code>syntho_hive/core/data/transformer.py</code> <pre><code>def transform(self, data: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Transform a dataframe into model-ready numpy arrays.\n\n    Args:\n        data: DataFrame with the same columns used during ``fit``.\n\n    Raises:\n        ValueError: If the transformer has not been fitted or a column is missing.\n\n    Returns:\n        Concatenated numpy array representing all transformed columns.\n    \"\"\"\n    if not self._transformers:\n        raise ValueError(\"Transformer has not been fitted.\")\n\n    output_arrays = []\n\n    # Iterate in the same order as fit/stored in _column_info\n    for col, info in self._column_info.items():\n        if col not in data.columns:\n            raise ValueError(f\"Column {col} missing from input data\")\n\n        transformer = self._transformers[col]\n        col_data = data[col]\n\n        if info['type'] == 'continuous':\n            # Returns (N, n_components + 1 [+ 1 if nulls])\n            transformed = transformer.transform(col_data)\n        elif info['type'] == 'categorical_embedding':\n            # Returns (N, 1)\n            col_data_filled = self._prepare_categorical(col_data)\n            values = transformer.transform(col_data_filled)\n            transformed = values.reshape(-1, 1)\n        else:\n            # Returns (N, n_categories)\n            col_data_filled = self._prepare_categorical(col_data)\n            values = col_data_filled.values.reshape(-1, 1)\n            transformed = transformer.transform(values)\n\n        output_arrays.append(transformed)\n\n    return np.concatenate(output_arrays, axis=1)\n</code></pre>"},{"location":"api/core/#syntho_hive.core.data.transformer.DataTransformer.inverse_transform","title":"inverse_transform","text":"<pre><code>inverse_transform(data: ndarray) -&gt; pd.DataFrame\n</code></pre> <p>Convert model outputs back to the original dataframe schema.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Numpy array produced by a model, aligned to transform layout.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If called before <code>fit</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with original column names and value types (constraints applied).</p> Source code in <code>syntho_hive/core/data/transformer.py</code> <pre><code>def inverse_transform(self, data: np.ndarray) -&gt; pd.DataFrame:\n    \"\"\"Convert model outputs back to the original dataframe schema.\n\n    Args:\n        data: Numpy array produced by a model, aligned to transform layout.\n\n    Raises:\n        ValueError: If called before ``fit``.\n\n    Returns:\n        DataFrame with original column names and value types (constraints applied).\n    \"\"\"\n    if not self._transformers:\n        raise ValueError(\"Transformer has not been fitted.\")\n\n    output_df = pd.DataFrame()\n    start_idx = 0\n\n    for col, info in self._column_info.items():\n        dim = info['dim']\n        end_idx = start_idx + dim\n        col_data = data[:, start_idx:end_idx]\n\n        transformer = self._transformers[col]\n\n        if info['type'] == 'continuous':\n            original_values = transformer.inverse_transform(col_data)\n        elif info['type'] == 'categorical_embedding':\n             # col_data is (N, 1) floats/ints. \n             # We need ints for LabelEncoder.\n             indices = np.clip(col_data.flatten().astype(int), 0, info['num_categories'] - 1)\n             original_values = transformer.inverse_transform(indices)\n             # Restore NaNs\n             original_values = pd.Series(original_values).replace('&lt;NAN&gt;', np.nan).values\n        else:\n            original_values = transformer.inverse_transform(col_data).flatten()\n            # Restore NaNs\n            original_values = pd.Series(original_values).replace('&lt;NAN&gt;', np.nan).values\n\n        # Apply Constraints\n        if self.metadata and hasattr(self, 'table_name') and self.table_name:\n            table_config = self.metadata.get_table(self.table_name)\n            if table_config and col in table_config.constraints:\n                constraint = table_config.constraints[col]\n\n                # 1. Rounding/Type\n                if constraint.dtype == \"int\":\n                    original_values = np.round(original_values).astype(int)\n\n                # 2. Clipping\n                if constraint.min is not None or constraint.max is not None:\n                    # Handle potential pandas Series or numpy array\n                    if isinstance(original_values, pd.Series):\n                        original_values = original_values.clip(lower=constraint.min, upper=constraint.max)\n                    else:\n                        original_values = np.clip(original_values, constraint.min, constraint.max)\n\n        output_df[col] = original_values\n        start_idx = end_idx\n\n    return output_df\n</code></pre>"},{"location":"api/core/#syntho_hive.core.data.transformer.ClusterBasedNormalizer","title":"syntho_hive.core.data.transformer.ClusterBasedNormalizer","text":"<p>VGM-based normalizer for continuous columns.</p> <p>Projects a value to a cluster assignment and a normalized scalar relative to the chosen component.</p> Source code in <code>syntho_hive/core/data/transformer.py</code> <pre><code>class ClusterBasedNormalizer:\n    \"\"\"VGM-based normalizer for continuous columns.\n\n    Projects a value to a cluster assignment and a normalized scalar relative\n    to the chosen component.\n    \"\"\"\n\n    def __init__(self, n_components: int = 10):\n        \"\"\"Configure the number of mixture components.\"\"\"\n        self.n_components = n_components\n        self.model = BayesianGaussianMixture(\n            n_components=n_components,\n            weight_concentration_prior_type='dirichlet_process',\n            n_init=1,\n            random_state=42\n        )\n        self.means = None\n        self.stds = None\n        self.has_nulls = False\n        self.fill_value = 0.0\n        self.output_dim = 0\n\n    def fit(self, data: pd.Series):\n        \"\"\"Fit the Bayesian GMM on a continuous series.\n\n        Args:\n            data: Continuous pandas Series to normalize.\n        \"\"\"\n        # 1. Handle Nulls\n        self.has_nulls = data.isnull().any()\n        if self.has_nulls:\n            self.fill_value = data.mean()\n            # Impute for training GMM\n            values = data.fillna(self.fill_value).values.reshape(-1, 1)\n            self.output_dim = self.n_components + 1 + 1 # +1 for null indicator\n        else:\n            values = data.values.reshape(-1, 1)\n            self.output_dim = self.n_components + 1\n\n        self.model.fit(values)\n        self.means = self.model.means_.flatten() # (n_components,)\n        self.stds = np.sqrt(self.model.covariances_).flatten() # (n_components,)\n\n    def transform(self, data: pd.Series) -&gt; np.ndarray:\n        \"\"\"Project values to one-hot cluster assignment and normalized scalar.\n\n        Args:\n            data: Continuous pandas Series to transform.\n\n        Returns:\n            Numpy array of shape ``(N, n_components + 1 [+1])`` with one-hot cluster, scaled value, [null_ind].\n        \"\"\"\n        values_raw = data.values.reshape(-1, 1)\n        n_samples = len(values_raw)\n\n        if self.has_nulls:\n            # 0. Create Null Indicator\n            null_indicator = pd.isnull(data).values.astype(float).reshape(-1, 1)\n\n            # 1. Impute for projection\n            values_clean = data.fillna(self.fill_value).values.reshape(-1, 1)\n        else:\n            values_clean = values_raw\n\n        # 2. Get cluster probabilities: P(c|x)\n        probs = self.model.predict_proba(values_clean) # (N, n_components)\n\n        # 2. Sample component c ~ P(c|x) (Argmax for simplicity/determinism in this impl)\n        # CTGAN uses argmax during interaction but sampling during training prep sometimes. \n        # Using argmax is stable.\n        # 3. Sample component c ~ P(c|x) (Argmax for simplicity/determinism in this impl)\n        # CTGAN uses argmax during interaction but sampling during training prep sometimes. \n        # Using argmax is stable.\n        cluster_assignments = np.argmax(probs, axis=1)\n\n        # 4. Calculate normalized scalar: v = (x - mu_c) / (4 * sigma_c)\n        # Clip to [-1, 1] usually, or roughly there.\n        means = self.means[cluster_assignments]\n        stds = self.stds[cluster_assignments]\n\n        normalized_values = (values_clean.flatten() - means) / (4 * stds)\n        normalized_values = normalized_values.reshape(-1, 1)\n\n        # 5. Create One-Hot encoding of cluster assignment\n        cluster_one_hot = np.zeros((n_samples, self.n_components))\n        cluster_one_hot[np.arange(n_samples), cluster_assignments] = 1\n\n        # Output: [one_hot_cluster, scalar, (null_indicator)]\n        if self.has_nulls:\n            return np.concatenate([cluster_one_hot, normalized_values, null_indicator], axis=1)\n        else:\n            return np.concatenate([cluster_one_hot, normalized_values], axis=1)\n\n    def inverse_transform(self, data: np.ndarray) -&gt; pd.Series:\n        \"\"\"Reconstruct approximate original values from normalized representation.\n\n        Args:\n            data: Array shaped ``(N, n_components + 1 [+1])`` produced by ``transform``.\n\n        Returns:\n            Pandas Series of reconstructed continuous values.\n        \"\"\"\n        # data shape: (N, n_components + 1 [+1])\n\n        current_idx = 0\n\n        # 1. Cluster One-Hot\n        cluster_one_hot = data[:, current_idx : current_idx + self.n_components]\n        current_idx += self.n_components\n\n        # 2. Scalar\n        scalars = data[:, current_idx]\n        current_idx += 1\n\n        # 3. Null Indicator\n        if self.has_nulls:\n            null_indicators = data[:, current_idx]\n            current_idx += 1\n        else:\n            null_indicators = None\n\n        # Identify cluster\n        cluster_assignments = np.argmax(cluster_one_hot, axis=1)\n\n        means = self.means[cluster_assignments]\n        stds = self.stds[cluster_assignments]\n\n        # Reconstruct: x = v * 4 * sigma_c + mu_c\n        reconstructed_values = scalars * 4 * stds + means\n\n        # Apply Null Masking\n        if null_indicators is not None:\n            # If null indicator &gt; 0.5 (generated as sigmoid usually, but here just boolean/float)\n            # Generator should output something close to 0 or 1.\n            # We assume threshold 0.5\n            is_null = null_indicators &gt; 0.5\n            reconstructed_values[is_null] = np.nan\n\n        return pd.Series(reconstructed_values)\n</code></pre>"},{"location":"api/core/#syntho_hive.core.data.transformer.ClusterBasedNormalizer.fit","title":"fit","text":"<pre><code>fit(data: Series)\n</code></pre> <p>Fit the Bayesian GMM on a continuous series.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series</code> <p>Continuous pandas Series to normalize.</p> required Source code in <code>syntho_hive/core/data/transformer.py</code> <pre><code>def fit(self, data: pd.Series):\n    \"\"\"Fit the Bayesian GMM on a continuous series.\n\n    Args:\n        data: Continuous pandas Series to normalize.\n    \"\"\"\n    # 1. Handle Nulls\n    self.has_nulls = data.isnull().any()\n    if self.has_nulls:\n        self.fill_value = data.mean()\n        # Impute for training GMM\n        values = data.fillna(self.fill_value).values.reshape(-1, 1)\n        self.output_dim = self.n_components + 1 + 1 # +1 for null indicator\n    else:\n        values = data.values.reshape(-1, 1)\n        self.output_dim = self.n_components + 1\n\n    self.model.fit(values)\n    self.means = self.model.means_.flatten() # (n_components,)\n    self.stds = np.sqrt(self.model.covariances_).flatten() # (n_components,)\n</code></pre>"},{"location":"api/core/#syntho_hive.core.data.transformer.ClusterBasedNormalizer.inverse_transform","title":"inverse_transform","text":"<pre><code>inverse_transform(data: ndarray) -&gt; pd.Series\n</code></pre> <p>Reconstruct approximate original values from normalized representation.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Array shaped <code>(N, n_components + 1 [+1])</code> produced by <code>transform</code>.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Pandas Series of reconstructed continuous values.</p> Source code in <code>syntho_hive/core/data/transformer.py</code> <pre><code>def inverse_transform(self, data: np.ndarray) -&gt; pd.Series:\n    \"\"\"Reconstruct approximate original values from normalized representation.\n\n    Args:\n        data: Array shaped ``(N, n_components + 1 [+1])`` produced by ``transform``.\n\n    Returns:\n        Pandas Series of reconstructed continuous values.\n    \"\"\"\n    # data shape: (N, n_components + 1 [+1])\n\n    current_idx = 0\n\n    # 1. Cluster One-Hot\n    cluster_one_hot = data[:, current_idx : current_idx + self.n_components]\n    current_idx += self.n_components\n\n    # 2. Scalar\n    scalars = data[:, current_idx]\n    current_idx += 1\n\n    # 3. Null Indicator\n    if self.has_nulls:\n        null_indicators = data[:, current_idx]\n        current_idx += 1\n    else:\n        null_indicators = None\n\n    # Identify cluster\n    cluster_assignments = np.argmax(cluster_one_hot, axis=1)\n\n    means = self.means[cluster_assignments]\n    stds = self.stds[cluster_assignments]\n\n    # Reconstruct: x = v * 4 * sigma_c + mu_c\n    reconstructed_values = scalars * 4 * stds + means\n\n    # Apply Null Masking\n    if null_indicators is not None:\n        # If null indicator &gt; 0.5 (generated as sigmoid usually, but here just boolean/float)\n        # Generator should output something close to 0 or 1.\n        # We assume threshold 0.5\n        is_null = null_indicators &gt; 0.5\n        reconstructed_values[is_null] = np.nan\n\n    return pd.Series(reconstructed_values)\n</code></pre>"},{"location":"api/core/#syntho_hive.core.data.transformer.ClusterBasedNormalizer.transform","title":"transform","text":"<pre><code>transform(data: Series) -&gt; np.ndarray\n</code></pre> <p>Project values to one-hot cluster assignment and normalized scalar.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series</code> <p>Continuous pandas Series to transform.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Numpy array of shape <code>(N, n_components + 1 [+1])</code> with one-hot cluster, scaled value, [null_ind].</p> Source code in <code>syntho_hive/core/data/transformer.py</code> <pre><code>def transform(self, data: pd.Series) -&gt; np.ndarray:\n    \"\"\"Project values to one-hot cluster assignment and normalized scalar.\n\n    Args:\n        data: Continuous pandas Series to transform.\n\n    Returns:\n        Numpy array of shape ``(N, n_components + 1 [+1])`` with one-hot cluster, scaled value, [null_ind].\n    \"\"\"\n    values_raw = data.values.reshape(-1, 1)\n    n_samples = len(values_raw)\n\n    if self.has_nulls:\n        # 0. Create Null Indicator\n        null_indicator = pd.isnull(data).values.astype(float).reshape(-1, 1)\n\n        # 1. Impute for projection\n        values_clean = data.fillna(self.fill_value).values.reshape(-1, 1)\n    else:\n        values_clean = values_raw\n\n    # 2. Get cluster probabilities: P(c|x)\n    probs = self.model.predict_proba(values_clean) # (N, n_components)\n\n    # 2. Sample component c ~ P(c|x) (Argmax for simplicity/determinism in this impl)\n    # CTGAN uses argmax during interaction but sampling during training prep sometimes. \n    # Using argmax is stable.\n    # 3. Sample component c ~ P(c|x) (Argmax for simplicity/determinism in this impl)\n    # CTGAN uses argmax during interaction but sampling during training prep sometimes. \n    # Using argmax is stable.\n    cluster_assignments = np.argmax(probs, axis=1)\n\n    # 4. Calculate normalized scalar: v = (x - mu_c) / (4 * sigma_c)\n    # Clip to [-1, 1] usually, or roughly there.\n    means = self.means[cluster_assignments]\n    stds = self.stds[cluster_assignments]\n\n    normalized_values = (values_clean.flatten() - means) / (4 * stds)\n    normalized_values = normalized_values.reshape(-1, 1)\n\n    # 5. Create One-Hot encoding of cluster assignment\n    cluster_one_hot = np.zeros((n_samples, self.n_components))\n    cluster_one_hot[np.arange(n_samples), cluster_assignments] = 1\n\n    # Output: [one_hot_cluster, scalar, (null_indicator)]\n    if self.has_nulls:\n        return np.concatenate([cluster_one_hot, normalized_values, null_indicator], axis=1)\n    else:\n        return np.concatenate([cluster_one_hot, normalized_values], axis=1)\n</code></pre>"},{"location":"api/interface/","title":"Interface &amp; Config","text":""},{"location":"api/interface/#syntho_hive.interface.synthesizer.Synthesizer","title":"syntho_hive.interface.synthesizer.Synthesizer","text":"<p>Main entry point that wires metadata, privacy, and orchestration.</p> Source code in <code>syntho_hive/interface/synthesizer.py</code> <pre><code>class Synthesizer:\n    \"\"\"Main entry point that wires metadata, privacy, and orchestration.\"\"\"\n    def __init__(\n        self,\n        metadata: Metadata,\n        privacy_config: PrivacyConfig,\n        spark_session: Optional[SparkSession] = None,\n        backend: str = \"CTGAN\",\n        embedding_threshold: int = 50\n    ):\n        \"\"\"Instantiate the synthesizer fa\u00e7ade.\n\n        Args:\n            metadata: Dataset schema and relational configuration.\n            privacy_config: Privacy guardrail configuration.\n            spark_session: Optional SparkSession required for orchestration.\n            backend: Synthesis backend identifier (currently CTGAN).\n            embedding_threshold: Cardinality threshold for switching to embeddings.\n        \"\"\"\n        self.metadata = metadata\n        self.privacy = privacy_config\n        self.spark = spark_session\n        self.backend = backend\n        self.embedding_threshold = embedding_threshold\n\n        # Initialize internal components\n        if self.spark:\n            self.orchestrator = StagedOrchestrator(metadata, self.spark)\n        else:\n            self.orchestrator = None # Mode without Spark (maybe local pandas only in future)\n\n    def fit(\n        self, \n        data: Any, # Str (database name) or Dict[str, str] (table paths)\n        sampling_strategy: str = \"relational_stratified\", \n        sample_size: int = 5_000_000, \n        validate: bool = False,\n        epochs: int = 300,\n        batch_size: int = 500,\n        **model_kwargs: Union[int, str, Tuple[int, int]]\n    ):\n        \"\"\"Fit the generative models on the real database.\n\n        Args:\n            data: Database name (str) or mapping of {table: path} (dict).\n            sampling_strategy: Strategy for sampling real data.\n            sample_size: Number of rows to sample from real data (approx).\n            validate: Whether to run validation after fitting.\n            epochs: Number of training epochs for CTGAN.\n            batch_size: Batch size for training.\n            **model_kwargs: Additional args forwarded to the underlying model (e.g., embedding_dim).\n\n        Raises:\n            ValueError: If Spark is unavailable or sample_size is invalid.\n        \"\"\"\n        if not self.orchestrator:\n            raise ValueError(\"SparkSession required for fit()\")\n\n        if sample_size &lt;= 0:\n            raise ValueError(\"sample_size must be positive\")\n\n        print(f\"Fitting on data source with {sampling_strategy} (target: {sample_size} rows)...\")\n        print(f\"Training Config: epochs={epochs}, batch_size={batch_size}\")\n\n        # Determine paths\n        if isinstance(data, str):\n            real_paths = {t: f\"{data}.{t}\" for t in self.metadata.tables}\n        elif isinstance(data, dict):\n            real_paths = data\n        else:\n            raise ValueError(\"Argument 'data' must be a database name (str) or path mapping (dict).\")\n\n        self.orchestrator.fit_all(real_paths, epochs=epochs, batch_size=batch_size, **model_kwargs)\n\n    def sample(self, num_rows: Dict[str, int], output_format: str = \"delta\", output_path: Optional[str] = None) -&gt; Union[Dict[str, str], Dict[str, pd.DataFrame]]:\n        \"\"\"Generate synthetic data for each table.\n\n        Args:\n            num_rows: Mapping of table name to number of rows to generate.\n            output_format: Storage format for generated datasets (default ``\"delta\"``).\n            output_path: Optional path to write files. If None, returns DataFrames in memory.\n            output_path: Optional path to write files. If None, returns DataFrames in memory.\n\n        Raises:\n            ValueError: If Spark orchestration is unavailable.\n\n        Returns:\n            Mapping of table name to the output path (if wrote to disk) OR Dictionary of DataFrames (if in-memory).\n        \"\"\"\n        if not self.orchestrator:\n            raise ValueError(\"SparkSession required for sample()\")\n\n        print(f\"Generating data with {self.backend} backend...\")\n\n        # If output_path is explicitly None, we return DataFrames\n        if output_path is None:\n             return self.orchestrator.generate(num_rows, output_path_base=None)\n\n        # Otherwise, write to disk (legacy/default behavior could be forced key if needed, but current API allows None)\n        # Wait, previous code forced a default if output_path was None. \n        # To maintain exact backward compat we might want a flag, but user asked for \"option to save to df\".\n        # If I change default behavior (None -&gt; /tmp to None -&gt; Memory), it breaks scripts relying on /tmp default?\n        # The previous default was implicit in the logic.\n        # Let's assume explicit None means memory now, and if they want disk they provide path.\n        # OR: we could interpret empty string as None? No, None is Pythonic.\n        # The user's request \"add option to save to df object instead\" implies a switch.\n\n        output_base = output_path\n        self.orchestrator.generate(num_rows, output_base)\n\n        # Return paths mapping\n        return {t: f\"{output_base}/{t}\" for t in self.metadata.tables}\n\n    def generate_validation_report(self, real_data: Dict[str, str], synthetic_data: Dict[str, str], output_path: str):\n        \"\"\"Generate a validation report comparing real vs synthetic datasets.\n\n        Args:\n            real_data: Map of table name to real dataset path/table.\n            synthetic_data: Map of table name to generated dataset path.\n            output_path: Filesystem path for the rendered report.\n\n        Raises:\n            ValueError: If Spark is unavailable.\n            Exception: Propagates any failures encountered while reading or validating.\n        \"\"\"\n        if not self.spark:\n             raise ValueError(\"SparkSession required for validation report generation\")\n\n        print(\"Generating validation report...\")\n        report_gen = ValidationReport()\n\n        real_dfs = {}\n        synth_dfs = {}\n\n        try:\n            # 1. Load Real Data\n            for table, path in real_data.items():\n                print(f\"Loading real data for {table} from {path}...\")\n                # Try reading as table first, then path\n                try:\n                    df = self.spark.read.table(path)\n                except:\n                   # Fallback to loading as path (parquet/delta default)\n                   df = self.spark.read.format(\"delta\").load(path)\n\n                real_dfs[table] = df.toPandas()\n\n            # 2. Load Synthetic Data\n            for table, path in synthetic_data.items():\n                print(f\"Loading synthetic data for {table} from {path}...\")\n                df = self.spark.read.format(\"delta\").load(path)\n                synth_dfs[table] = df.toPandas()\n\n            # 3. Generate Report\n            report_gen.generate(real_dfs, synth_dfs, output_path)\n\n        except Exception as e:\n            print(f\"Error generating validation report: {str(e)}\")\n            raise e\n\n    def save_to_hive(self, synthetic_data: Dict[str, str], target_db: str, overwrite: bool = True):\n        \"\"\"Register generated datasets as Hive tables.\n\n        Args:\n            synthetic_data: Map of table name to generated dataset path.\n            target_db: Hive database where tables should be registered.\n            overwrite: Whether to drop and recreate existing tables.\n\n        Raises:\n            ValueError: If Spark is unavailable.\n        \"\"\"\n        if not self.spark:\n            raise ValueError(\"SparkSession required for Hive registration\")\n\n        print(f\"Save to Hive database: {target_db}\")\n\n        # Ensure DB exists\n        self.spark.sql(f\"CREATE DATABASE IF NOT EXISTS {target_db}\")\n\n        for table, path in synthetic_data.items():\n            full_table_name = f\"{target_db}.{table}\"\n            print(f\"Registering table {full_table_name} at {path}\")\n\n            if overwrite:\n                self.spark.sql(f\"DROP TABLE IF EXISTS {full_table_name}\")\n\n            # Register External Table\n            self.spark.sql(f\"CREATE TABLE {full_table_name} USING DELTA LOCATION '{path}'\")\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.synthesizer.Synthesizer.fit","title":"fit","text":"<pre><code>fit(data: Any, sampling_strategy: str = 'relational_stratified', sample_size: int = 5000000, validate: bool = False, epochs: int = 300, batch_size: int = 500, **model_kwargs: Union[int, str, Tuple[int, int]])\n</code></pre> <p>Fit the generative models on the real database.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Database name (str) or mapping of {table: path} (dict).</p> required <code>sampling_strategy</code> <code>str</code> <p>Strategy for sampling real data.</p> <code>'relational_stratified'</code> <code>sample_size</code> <code>int</code> <p>Number of rows to sample from real data (approx).</p> <code>5000000</code> <code>validate</code> <code>bool</code> <p>Whether to run validation after fitting.</p> <code>False</code> <code>epochs</code> <code>int</code> <p>Number of training epochs for CTGAN.</p> <code>300</code> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>500</code> <code>**model_kwargs</code> <code>Union[int, str, Tuple[int, int]]</code> <p>Additional args forwarded to the underlying model (e.g., embedding_dim).</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If Spark is unavailable or sample_size is invalid.</p> Source code in <code>syntho_hive/interface/synthesizer.py</code> <pre><code>def fit(\n    self, \n    data: Any, # Str (database name) or Dict[str, str] (table paths)\n    sampling_strategy: str = \"relational_stratified\", \n    sample_size: int = 5_000_000, \n    validate: bool = False,\n    epochs: int = 300,\n    batch_size: int = 500,\n    **model_kwargs: Union[int, str, Tuple[int, int]]\n):\n    \"\"\"Fit the generative models on the real database.\n\n    Args:\n        data: Database name (str) or mapping of {table: path} (dict).\n        sampling_strategy: Strategy for sampling real data.\n        sample_size: Number of rows to sample from real data (approx).\n        validate: Whether to run validation after fitting.\n        epochs: Number of training epochs for CTGAN.\n        batch_size: Batch size for training.\n        **model_kwargs: Additional args forwarded to the underlying model (e.g., embedding_dim).\n\n    Raises:\n        ValueError: If Spark is unavailable or sample_size is invalid.\n    \"\"\"\n    if not self.orchestrator:\n        raise ValueError(\"SparkSession required for fit()\")\n\n    if sample_size &lt;= 0:\n        raise ValueError(\"sample_size must be positive\")\n\n    print(f\"Fitting on data source with {sampling_strategy} (target: {sample_size} rows)...\")\n    print(f\"Training Config: epochs={epochs}, batch_size={batch_size}\")\n\n    # Determine paths\n    if isinstance(data, str):\n        real_paths = {t: f\"{data}.{t}\" for t in self.metadata.tables}\n    elif isinstance(data, dict):\n        real_paths = data\n    else:\n        raise ValueError(\"Argument 'data' must be a database name (str) or path mapping (dict).\")\n\n    self.orchestrator.fit_all(real_paths, epochs=epochs, batch_size=batch_size, **model_kwargs)\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.synthesizer.Synthesizer.generate_validation_report","title":"generate_validation_report","text":"<pre><code>generate_validation_report(real_data: Dict[str, str], synthetic_data: Dict[str, str], output_path: str)\n</code></pre> <p>Generate a validation report comparing real vs synthetic datasets.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>Dict[str, str]</code> <p>Map of table name to real dataset path/table.</p> required <code>synthetic_data</code> <code>Dict[str, str]</code> <p>Map of table name to generated dataset path.</p> required <code>output_path</code> <code>str</code> <p>Filesystem path for the rendered report.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If Spark is unavailable.</p> <code>Exception</code> <p>Propagates any failures encountered while reading or validating.</p> Source code in <code>syntho_hive/interface/synthesizer.py</code> <pre><code>def generate_validation_report(self, real_data: Dict[str, str], synthetic_data: Dict[str, str], output_path: str):\n    \"\"\"Generate a validation report comparing real vs synthetic datasets.\n\n    Args:\n        real_data: Map of table name to real dataset path/table.\n        synthetic_data: Map of table name to generated dataset path.\n        output_path: Filesystem path for the rendered report.\n\n    Raises:\n        ValueError: If Spark is unavailable.\n        Exception: Propagates any failures encountered while reading or validating.\n    \"\"\"\n    if not self.spark:\n         raise ValueError(\"SparkSession required for validation report generation\")\n\n    print(\"Generating validation report...\")\n    report_gen = ValidationReport()\n\n    real_dfs = {}\n    synth_dfs = {}\n\n    try:\n        # 1. Load Real Data\n        for table, path in real_data.items():\n            print(f\"Loading real data for {table} from {path}...\")\n            # Try reading as table first, then path\n            try:\n                df = self.spark.read.table(path)\n            except:\n               # Fallback to loading as path (parquet/delta default)\n               df = self.spark.read.format(\"delta\").load(path)\n\n            real_dfs[table] = df.toPandas()\n\n        # 2. Load Synthetic Data\n        for table, path in synthetic_data.items():\n            print(f\"Loading synthetic data for {table} from {path}...\")\n            df = self.spark.read.format(\"delta\").load(path)\n            synth_dfs[table] = df.toPandas()\n\n        # 3. Generate Report\n        report_gen.generate(real_dfs, synth_dfs, output_path)\n\n    except Exception as e:\n        print(f\"Error generating validation report: {str(e)}\")\n        raise e\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.synthesizer.Synthesizer.sample","title":"sample","text":"<pre><code>sample(num_rows: Dict[str, int], output_format: str = 'delta', output_path: Optional[str] = None) -&gt; Union[Dict[str, str], Dict[str, pd.DataFrame]]\n</code></pre> <p>Generate synthetic data for each table.</p> <p>Parameters:</p> Name Type Description Default <code>num_rows</code> <code>Dict[str, int]</code> <p>Mapping of table name to number of rows to generate.</p> required <code>output_format</code> <code>str</code> <p>Storage format for generated datasets (default <code>\"delta\"</code>).</p> <code>'delta'</code> <code>output_path</code> <code>Optional[str]</code> <p>Optional path to write files. If None, returns DataFrames in memory.</p> <code>None</code> <code>output_path</code> <code>Optional[str]</code> <p>Optional path to write files. If None, returns DataFrames in memory.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If Spark orchestration is unavailable.</p> <p>Returns:</p> Type Description <code>Union[Dict[str, str], Dict[str, DataFrame]]</code> <p>Mapping of table name to the output path (if wrote to disk) OR Dictionary of DataFrames (if in-memory).</p> Source code in <code>syntho_hive/interface/synthesizer.py</code> <pre><code>def sample(self, num_rows: Dict[str, int], output_format: str = \"delta\", output_path: Optional[str] = None) -&gt; Union[Dict[str, str], Dict[str, pd.DataFrame]]:\n    \"\"\"Generate synthetic data for each table.\n\n    Args:\n        num_rows: Mapping of table name to number of rows to generate.\n        output_format: Storage format for generated datasets (default ``\"delta\"``).\n        output_path: Optional path to write files. If None, returns DataFrames in memory.\n        output_path: Optional path to write files. If None, returns DataFrames in memory.\n\n    Raises:\n        ValueError: If Spark orchestration is unavailable.\n\n    Returns:\n        Mapping of table name to the output path (if wrote to disk) OR Dictionary of DataFrames (if in-memory).\n    \"\"\"\n    if not self.orchestrator:\n        raise ValueError(\"SparkSession required for sample()\")\n\n    print(f\"Generating data with {self.backend} backend...\")\n\n    # If output_path is explicitly None, we return DataFrames\n    if output_path is None:\n         return self.orchestrator.generate(num_rows, output_path_base=None)\n\n    # Otherwise, write to disk (legacy/default behavior could be forced key if needed, but current API allows None)\n    # Wait, previous code forced a default if output_path was None. \n    # To maintain exact backward compat we might want a flag, but user asked for \"option to save to df\".\n    # If I change default behavior (None -&gt; /tmp to None -&gt; Memory), it breaks scripts relying on /tmp default?\n    # The previous default was implicit in the logic.\n    # Let's assume explicit None means memory now, and if they want disk they provide path.\n    # OR: we could interpret empty string as None? No, None is Pythonic.\n    # The user's request \"add option to save to df object instead\" implies a switch.\n\n    output_base = output_path\n    self.orchestrator.generate(num_rows, output_base)\n\n    # Return paths mapping\n    return {t: f\"{output_base}/{t}\" for t in self.metadata.tables}\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.synthesizer.Synthesizer.save_to_hive","title":"save_to_hive","text":"<pre><code>save_to_hive(synthetic_data: Dict[str, str], target_db: str, overwrite: bool = True)\n</code></pre> <p>Register generated datasets as Hive tables.</p> <p>Parameters:</p> Name Type Description Default <code>synthetic_data</code> <code>Dict[str, str]</code> <p>Map of table name to generated dataset path.</p> required <code>target_db</code> <code>str</code> <p>Hive database where tables should be registered.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to drop and recreate existing tables.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If Spark is unavailable.</p> Source code in <code>syntho_hive/interface/synthesizer.py</code> <pre><code>def save_to_hive(self, synthetic_data: Dict[str, str], target_db: str, overwrite: bool = True):\n    \"\"\"Register generated datasets as Hive tables.\n\n    Args:\n        synthetic_data: Map of table name to generated dataset path.\n        target_db: Hive database where tables should be registered.\n        overwrite: Whether to drop and recreate existing tables.\n\n    Raises:\n        ValueError: If Spark is unavailable.\n    \"\"\"\n    if not self.spark:\n        raise ValueError(\"SparkSession required for Hive registration\")\n\n    print(f\"Save to Hive database: {target_db}\")\n\n    # Ensure DB exists\n    self.spark.sql(f\"CREATE DATABASE IF NOT EXISTS {target_db}\")\n\n    for table, path in synthetic_data.items():\n        full_table_name = f\"{target_db}.{table}\"\n        print(f\"Registering table {full_table_name} at {path}\")\n\n        if overwrite:\n            self.spark.sql(f\"DROP TABLE IF EXISTS {full_table_name}\")\n\n        # Register External Table\n        self.spark.sql(f\"CREATE TABLE {full_table_name} USING DELTA LOCATION '{path}'\")\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.config.Metadata","title":"syntho_hive.interface.config.Metadata","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema definition for the entire dataset.</p> Source code in <code>syntho_hive/interface/config.py</code> <pre><code>class Metadata(BaseModel):\n    \"\"\"Schema definition for the entire dataset.\"\"\"\n    tables: Dict[str, TableConfig] = Field(default_factory=dict)\n\n    def add_table(self, name: str, pk: str, **kwargs: Union[List[str], Dict[str, str], Dict[str, Constraint]]):\n        \"\"\"Register a table configuration.\n\n        Args:\n            name: Table name.\n            pk: Primary key column name.\n            **kwargs: Additional fields to populate ``TableConfig``.\n\n        Raises:\n            ValueError: If a table with the same name already exists.\n        \"\"\"\n        if name in self.tables:\n             raise ValueError(f\"Table '{name}' already exists in metadata.\")\n        self.tables[name] = TableConfig(name=name, pk=pk, **kwargs)\n\n    def get_table(self, name: str) -&gt; Optional[TableConfig]:\n        \"\"\"Fetch a table configuration by name.\n\n        Args:\n            name: Table name to retrieve.\n\n        Returns:\n            Corresponding ``TableConfig`` or ``None`` if missing.\n        \"\"\"\n        return self.tables.get(name)\n\n    def validate_schema(self):\n        \"\"\"Validate schema integrity, focusing on foreign key references.\n\n        Raises:\n            ValueError: When an FK reference is malformed or targets a missing table.\n        \"\"\"\n        for table_name, table_config in self.tables.items():\n            for local_col, parent_ref in table_config.fk.items():\n                if \".\" not in parent_ref:\n                    raise ValueError(f\"Invalid FK reference '{parent_ref}' in table '{table_name}'. Format should be 'parent_table.parent_col'.\")\n\n                parent_table, parent_col = parent_ref.split(\".\", 1)\n\n                if parent_table not in self.tables:\n                    raise ValueError(f\"Table '{table_name}' references non-existent parent table '{parent_table}'.\")\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.config.Metadata.add_table","title":"add_table","text":"<pre><code>add_table(name: str, pk: str, **kwargs: Union[List[str], Dict[str, str], Dict[str, Constraint]])\n</code></pre> <p>Register a table configuration.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Table name.</p> required <code>pk</code> <code>str</code> <p>Primary key column name.</p> required <code>**kwargs</code> <code>Union[List[str], Dict[str, str], Dict[str, Constraint]]</code> <p>Additional fields to populate <code>TableConfig</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a table with the same name already exists.</p> Source code in <code>syntho_hive/interface/config.py</code> <pre><code>def add_table(self, name: str, pk: str, **kwargs: Union[List[str], Dict[str, str], Dict[str, Constraint]]):\n    \"\"\"Register a table configuration.\n\n    Args:\n        name: Table name.\n        pk: Primary key column name.\n        **kwargs: Additional fields to populate ``TableConfig``.\n\n    Raises:\n        ValueError: If a table with the same name already exists.\n    \"\"\"\n    if name in self.tables:\n         raise ValueError(f\"Table '{name}' already exists in metadata.\")\n    self.tables[name] = TableConfig(name=name, pk=pk, **kwargs)\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.config.Metadata.get_table","title":"get_table","text":"<pre><code>get_table(name: str) -&gt; Optional[TableConfig]\n</code></pre> <p>Fetch a table configuration by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Table name to retrieve.</p> required <p>Returns:</p> Type Description <code>Optional[TableConfig]</code> <p>Corresponding <code>TableConfig</code> or <code>None</code> if missing.</p> Source code in <code>syntho_hive/interface/config.py</code> <pre><code>def get_table(self, name: str) -&gt; Optional[TableConfig]:\n    \"\"\"Fetch a table configuration by name.\n\n    Args:\n        name: Table name to retrieve.\n\n    Returns:\n        Corresponding ``TableConfig`` or ``None`` if missing.\n    \"\"\"\n    return self.tables.get(name)\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.config.Metadata.validate_schema","title":"validate_schema","text":"<pre><code>validate_schema()\n</code></pre> <p>Validate schema integrity, focusing on foreign key references.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When an FK reference is malformed or targets a missing table.</p> Source code in <code>syntho_hive/interface/config.py</code> <pre><code>def validate_schema(self):\n    \"\"\"Validate schema integrity, focusing on foreign key references.\n\n    Raises:\n        ValueError: When an FK reference is malformed or targets a missing table.\n    \"\"\"\n    for table_name, table_config in self.tables.items():\n        for local_col, parent_ref in table_config.fk.items():\n            if \".\" not in parent_ref:\n                raise ValueError(f\"Invalid FK reference '{parent_ref}' in table '{table_name}'. Format should be 'parent_table.parent_col'.\")\n\n            parent_table, parent_col = parent_ref.split(\".\", 1)\n\n            if parent_table not in self.tables:\n                raise ValueError(f\"Table '{table_name}' references non-existent parent table '{parent_table}'.\")\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.config.TableConfig","title":"syntho_hive.interface.config.TableConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a single table, including keys and constraints.</p> Source code in <code>syntho_hive/interface/config.py</code> <pre><code>class TableConfig(BaseModel):\n    \"\"\"Configuration for a single table, including keys and constraints.\"\"\"\n    name: str\n    pk: str\n    pii_cols: List[str] = Field(default_factory=list)\n    high_cardinality_cols: List[str] = Field(default_factory=list)\n    fk: Dict[str, str] = Field(default_factory=dict, description=\"Map of local_col -&gt; parent_table.parent_col\")\n    parent_context_cols: List[str] = Field(default_factory=list, description=\"List of parent attributes to condition on (e.g., 'users.region')\")\n    constraints: Dict[str, Constraint] = Field(default_factory=dict, description=\"Map of col_name -&gt; Constraint\")\n\n    @property\n    def has_dependencies(self) -&gt; bool:\n        \"\"\"Whether the table declares any foreign key dependencies.\"\"\"\n        return bool(self.fk)\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.config.TableConfig.has_dependencies","title":"has_dependencies  <code>property</code>","text":"<pre><code>has_dependencies: bool\n</code></pre> <p>Whether the table declares any foreign key dependencies.</p>"},{"location":"api/interface/#syntho_hive.interface.config.PrivacyConfig","title":"syntho_hive.interface.config.PrivacyConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for privacy guardrails applied during synthesis.</p> Source code in <code>syntho_hive/interface/config.py</code> <pre><code>class PrivacyConfig(BaseModel):\n    \"\"\"Configuration for privacy guardrails applied during synthesis.\"\"\"\n    enable_differential_privacy: bool = False\n    epsilon: float = 1.0\n    pii_strategy: Literal[\"mask\", \"faker\", \"context_aware_faker\"] = \"context_aware_faker\"\n    k_anonymity_threshold: int = 5\n    pii_columns: List[str] = Field(default_factory=list)\n</code></pre>"},{"location":"api/interface/#syntho_hive.validation.report_generator.ValidationReport","title":"syntho_hive.validation.report_generator.ValidationReport","text":"<p>Generate summary reports of validation metrics.</p> Source code in <code>syntho_hive/validation/report_generator.py</code> <pre><code>class ValidationReport:\n    \"\"\"Generate summary reports of validation metrics.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize statistical validator and metric store.\"\"\"\n        self.validator = StatisticalValidator()\n        self.metrics = {}\n\n    def _calculate_detailed_stats(self, real_df: pd.DataFrame, synth_df: pd.DataFrame) -&gt; Dict[str, Any]:\n        \"\"\"Calculate descriptive statistics for side-by-side comparison.\n\n        Args:\n            real_df: Real dataframe.\n            synth_df: Synthetic dataframe aligned to the real columns.\n\n        Returns:\n            Nested dict of summary stats for each column.\n        \"\"\"\n        stats = {}\n        for col in real_df.columns:\n            if col not in synth_df.columns:\n                continue\n\n            col_stats = {\"real\": {}, \"synth\": {}}\n\n            for name, df, res in [(\"real\", real_df, col_stats[\"real\"]), (\"synth\", synth_df, col_stats[\"synth\"])]:\n                series = df[col]\n                if pd.api.types.is_numeric_dtype(series):\n                    res[\"mean\"] = series.mean()\n                    res[\"std\"] = series.std()\n                    res[\"min\"] = series.min()\n                    res[\"max\"] = series.max()\n                else:\n                    res[\"unique_count\"] = series.nunique()\n                    res[\"top_value\"] = series.mode().iloc[0] if not series.mode().empty else \"N/A\"\n                    res[\"top_freq\"] = series.value_counts().iloc[0] if not series.empty else 0\n\n            stats[col] = col_stats\n        return stats\n\n    def generate(self, real_data: Dict[str, pd.DataFrame], synth_data: Dict[str, pd.DataFrame], output_path: str):\n        \"\"\"Run validation and save a report.\n\n        Args:\n            real_data: Mapping of table name to real dataframe.\n            synth_data: Mapping of table name to synthetic dataframe.\n            output_path: Destination path for HTML or JSON report.\n        \"\"\"\n        report = {\n            \"tables\": {},\n            \"summary\": \"Validation Report\"\n        }\n\n        for table_name, real_df in real_data.items():\n            if table_name not in synth_data:\n                continue\n\n            synth_df = synth_data[table_name]\n\n            # 1. Column comparisons\n            col_metrics = self.validator.compare_columns(real_df, synth_df)\n\n            # 2. Correlation\n            corr_diff = self.validator.check_correlations(real_df, synth_df)\n\n            # 3. Detailed Stats\n            stats = self._calculate_detailed_stats(real_df, synth_df)\n\n            # 4. Data Preview\n            # Use Pandas to_html for easy formatting, strict constraints\n            preview = {\n                \"real_html\": real_df.head(10).to_html(index=False, classes='scroll-table', border=0),\n                \"synth_html\": synth_df.head(10).to_html(index=False, classes='scroll-table', border=0)\n            }\n\n            report[\"tables\"][table_name] = {\n                \"column_metrics\": col_metrics,\n                \"correlation_distance\": corr_diff,\n                \"detailed_stats\": stats,\n                \"preview\": preview\n            }\n\n        if output_path.endswith(\".html\"):\n            self._save_html(report, output_path)\n        else:\n            # Save to JSON for now (PDF requires more deps)\n            with open(output_path, \"w\") as f:\n                json.dump(report, f, indent=2, default=str)\n\n        import os\n        print(f\"Report saved to {os.path.abspath(output_path)}\")\n\n    def _save_html(self, report: Dict[str, Any], output_path: str):\n        \"\"\"Render a rich HTML report with metric explanations, stats, and previews.\n\n        Args:\n            report: Structured report dictionary produced by ``generate``.\n            output_path: Filesystem path to write the HTML file.\n        \"\"\"\n        html_content = [\n            \"\"\"&lt;html&gt;\n            &lt;head&gt;\n                &lt;style&gt;\n                    body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 20px; background-color: #f9f9f9; color: #333; }\n                    h1, h2, h3 { color: #2c3e50; }\n                    .container { max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }\n\n                    /* Tables */\n                    table { border-collapse: collapse; width: 100%; margin-bottom: 20px; font-size: 14px; }\n                    th, td { border: 1px solid #e1e4e8; padding: 10px; text-align: left; }\n                    th { background-color: #f1f8ff; color: #0366d6; font-weight: 600; }\n                    tr:nth-child(even) { background-color: #f8f9fa; }\n\n                    /* Status Colors */\n                    .pass { color: #28a745; font-weight: bold; }\n                    .fail { color: #dc3545; font-weight: bold; }\n\n                    /* Layout */\n                    .section { margin-top: 40px; border-top: 1px solid #eee; padding-top: 20px; }\n                    .metric-box { background: #f0f4f8; padding: 15px; border-radius: 5px; margin-bottom: 20px; border-left: 5px solid #0366d6; }\n                    .row { display: flex; gap: 20px; }\n                    .col { flex: 1; overflow-x: auto; }\n\n                    /* Tabs/Previews */\n                    .preview-header { font-weight: bold; margin-bottom: 10px; color: #555; }\n                    .scroll-table { max-height: 400px; overflow-y: auto; display: block; }\n                &lt;/style&gt;\n            &lt;/head&gt;\n            &lt;body&gt;\n            &lt;div class=\"container\"&gt;\n                &lt;h1&gt;Validation Report&lt;/h1&gt;\n\n                &lt;div class=\"metric-box\"&gt;\n                    &lt;h3&gt;Metric Explanations&lt;/h3&gt;\n                    &lt;ul&gt;\n                        &lt;li&gt;&lt;strong&gt;KS Test (Kolmogorov-Smirnov):&lt;/strong&gt; Used for continuous numerical columns. Compares the cumulative distribution functions of the real and synthetic data. &lt;br&gt;\n                            &lt;em&gt;Result:&lt;/em&gt; Returns a p-value. If p &gt; 0.05, we fail to reject the null hypothesis (i.e., distributions are likely the same).&lt;/li&gt;\n                        &lt;li&gt;&lt;strong&gt;TVD (Total Variation Distance):&lt;/strong&gt; Used for categorical or discrete columns. Measures the maximum difference between probabilities assigned to the same event by two distributions. &lt;br&gt;\n                            &lt;em&gt;Result:&lt;/em&gt; Value between 0 and 1. Lower is better (0 means identical). We consider &lt; 0.1 as passing.&lt;/li&gt;\n                        &lt;li&gt;&lt;strong&gt;Correlation Distance:&lt;/strong&gt; Measures how well the pairwise correlations between numerical columns are preserved. Calculated as the Frobenius norm of the difference between correlation matrices. &lt;br&gt;\n                            &lt;em&gt;Result:&lt;/em&gt; Lower is better (0 means identical correlation structure).&lt;/li&gt;\n                    &lt;/ul&gt;\n                &lt;/div&gt;\n            \"\"\"]\n\n        for table_name, data in report[\"tables\"].items():\n            html_content.append(f\"&lt;div class='section'&gt;&lt;h2&gt;Table: {table_name}&lt;/h2&gt;\")\n\n            # --- 1. Correlation &amp; Overall ---\n            corr_dist = data.get('correlation_distance', 0.0)\n            html_content.append(f\"&lt;p&gt;&lt;strong&gt;Correlation Distance:&lt;/strong&gt; {corr_dist:.4f}&lt;/p&gt;\")\n\n            # --- 2. Column Metrics ---\n            html_content.append(\"&lt;h3&gt;Column Validation Metrics&lt;/h3&gt;\")\n            html_content.append(\"&lt;table&gt;&lt;tr&gt;&lt;th&gt;Column&lt;/th&gt;&lt;th&gt;Test Type&lt;/th&gt;&lt;th&gt;Statistic&lt;/th&gt;&lt;th&gt;P-Value / Score&lt;/th&gt;&lt;th&gt;Status&lt;/th&gt;&lt;/tr&gt;\")\n\n            for col, metrics in data[\"column_metrics\"].items():\n                if \"error\" in metrics:\n                    html_content.append(f\"&lt;tr&gt;&lt;td&gt;{col}&lt;/td&gt;&lt;td colspan='4' class='fail'&gt;Error: {metrics['error']}&lt;/td&gt;&lt;/tr&gt;\")\n                    continue\n\n                status = \"PASS\" if metrics.get(\"passed\", False) else \"FAIL\"\n                cls = \"pass\" if status == \"PASS\" else \"fail\"\n\n                stat = f\"{metrics.get('statistic', 0):.4f}\"\n                # TVD doesn't have a p-value, KS does.\n                pval = f\"{metrics.get('p_value', 0):.4f}\" if metrics.get('p_value') is not None else \"N/A\"\n                test_name = metrics.get('test', 'N/A')\n\n                html_content.append(f\"&lt;tr&gt;&lt;td&gt;{col}&lt;/td&gt;&lt;td&gt;{test_name}&lt;/td&gt;&lt;td&gt;{stat}&lt;/td&gt;&lt;td&gt;{pval}&lt;/td&gt;&lt;td class='{cls}'&gt;{status}&lt;/td&gt;&lt;/tr&gt;\")\n\n            html_content.append(\"&lt;/table&gt;\")\n\n            # --- 3. Detailed Statistics ---\n            if \"detailed_stats\" in data:\n                html_content.append(\"&lt;h3&gt;Detailed Statistics (Real vs Synthetic)&lt;/h3&gt;\")\n                html_content.append(\"&lt;table&gt;&lt;tr&gt;&lt;th&gt;Column&lt;/th&gt;&lt;th&gt;Metric&lt;/th&gt;&lt;th&gt;Real&lt;/th&gt;&lt;th&gt;Synthetic&lt;/th&gt;&lt;/tr&gt;\")\n\n                for col, stats in data[\"detailed_stats\"].items():\n                    # stats has \"real\": {...}, \"synth\": {...}\n                    real_s = stats.get(\"real\", {})\n                    synth_s = stats.get(\"synth\", {})\n\n                    # Merge keys to show\n                    all_keys = sorted(list(set(real_s.keys()) | set(synth_s.keys())))\n                    # Usually we want mean, std, min, max or unique, top\n\n                    first = True\n                    for k in all_keys:\n                        r_val = real_s.get(k, \"-\")\n                        s_val = synth_s.get(k, \"-\")\n\n                        # Format floats\n                        if isinstance(r_val, (float, np.floating)): r_val = f\"{r_val:.4f}\"\n                        if isinstance(s_val, (float, np.floating)): s_val = f\"{s_val:.4f}\"\n\n                        row_start = f\"&lt;tr&gt;&lt;td rowspan='{len(all_keys)}'&gt;{col}&lt;/td&gt;\" if first else \"&lt;tr&gt;\"\n                        row_end = f\"&lt;td&gt;{k}&lt;/td&gt;&lt;td&gt;{r_val}&lt;/td&gt;&lt;td&gt;{s_val}&lt;/td&gt;&lt;/tr&gt;\"\n                        html_content.append(row_start + row_end)\n                        first = False\n                html_content.append(\"&lt;/table&gt;\")\n\n            # --- 4. Data Preview ---\n            if \"preview\" in data:\n                html_content.append(\"&lt;h3&gt;Data Preview (First 10 Rows)&lt;/h3&gt;\")\n                html_content.append(\"&lt;div class='row'&gt;\")\n\n                # Real\n                html_content.append(\"&lt;div class='col'&gt;\")\n                html_content.append(\"&lt;div class='preview-header'&gt;Original Data (Real)&lt;/div&gt;\")\n                html_content.append(data[\"preview\"][\"real_html\"])\n                html_content.append(\"&lt;/div&gt;\")\n\n                # Synth\n                html_content.append(\"&lt;div class='col'&gt;\")\n                html_content.append(\"&lt;div class='preview-header'&gt;Synthetic Data (Generated)&lt;/div&gt;\")\n                html_content.append(data[\"preview\"][\"synth_html\"])\n                html_content.append(\"&lt;/div&gt;\")\n\n                html_content.append(\"&lt;/div&gt;\") # End row\n\n            html_content.append(\"&lt;/div&gt;\") # End section\n\n        html_content.append(\"&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;\")\n\n        with open(output_path, \"w\") as f:\n            f.write(\"\\n\".join(html_content))\n</code></pre>"},{"location":"api/interface/#syntho_hive.validation.report_generator.ValidationReport.generate","title":"generate","text":"<pre><code>generate(real_data: Dict[str, DataFrame], synth_data: Dict[str, DataFrame], output_path: str)\n</code></pre> <p>Run validation and save a report.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>Dict[str, DataFrame]</code> <p>Mapping of table name to real dataframe.</p> required <code>synth_data</code> <code>Dict[str, DataFrame]</code> <p>Mapping of table name to synthetic dataframe.</p> required <code>output_path</code> <code>str</code> <p>Destination path for HTML or JSON report.</p> required Source code in <code>syntho_hive/validation/report_generator.py</code> <pre><code>def generate(self, real_data: Dict[str, pd.DataFrame], synth_data: Dict[str, pd.DataFrame], output_path: str):\n    \"\"\"Run validation and save a report.\n\n    Args:\n        real_data: Mapping of table name to real dataframe.\n        synth_data: Mapping of table name to synthetic dataframe.\n        output_path: Destination path for HTML or JSON report.\n    \"\"\"\n    report = {\n        \"tables\": {},\n        \"summary\": \"Validation Report\"\n    }\n\n    for table_name, real_df in real_data.items():\n        if table_name not in synth_data:\n            continue\n\n        synth_df = synth_data[table_name]\n\n        # 1. Column comparisons\n        col_metrics = self.validator.compare_columns(real_df, synth_df)\n\n        # 2. Correlation\n        corr_diff = self.validator.check_correlations(real_df, synth_df)\n\n        # 3. Detailed Stats\n        stats = self._calculate_detailed_stats(real_df, synth_df)\n\n        # 4. Data Preview\n        # Use Pandas to_html for easy formatting, strict constraints\n        preview = {\n            \"real_html\": real_df.head(10).to_html(index=False, classes='scroll-table', border=0),\n            \"synth_html\": synth_df.head(10).to_html(index=False, classes='scroll-table', border=0)\n        }\n\n        report[\"tables\"][table_name] = {\n            \"column_metrics\": col_metrics,\n            \"correlation_distance\": corr_diff,\n            \"detailed_stats\": stats,\n            \"preview\": preview\n        }\n\n    if output_path.endswith(\".html\"):\n        self._save_html(report, output_path)\n    else:\n        # Save to JSON for now (PDF requires more deps)\n        with open(output_path, \"w\") as f:\n            json.dump(report, f, indent=2, default=str)\n\n    import os\n    print(f\"Report saved to {os.path.abspath(output_path)}\")\n</code></pre>"},{"location":"api/privacy/","title":"Privacy &amp; Sanitization","text":"<p>The privacy module ensures that sensitive information is detected and handled correctly before any modeling takes place.</p>"},{"location":"api/privacy/#sanitizer","title":"Sanitizer","text":""},{"location":"api/privacy/#syntho_hive.privacy.sanitizer.PIISanitizer","title":"syntho_hive.privacy.sanitizer.PIISanitizer","text":"<p>Detect and sanitize PII columns based on configurable rules.</p> Source code in <code>syntho_hive/privacy/sanitizer.py</code> <pre><code>class PIISanitizer:\n    \"\"\"Detect and sanitize PII columns based on configurable rules.\"\"\"\n\n    def __init__(self, config: Optional[PrivacyConfig] = None):\n        \"\"\"Create a sanitizer with contextual faker support.\n\n        Args:\n            config: Optional privacy configuration; defaults to ``PrivacyConfig.default``.\n        \"\"\"\n        self.config = config or PrivacyConfig.default()\n        self.faker = ContextualFaker()\n\n    def analyze(self, df: pd.DataFrame) -&gt; Dict[str, str]:\n        \"\"\"Detect potential PII columns using configured rules.\n\n        Args:\n            df: DataFrame to inspect for PII.\n\n        Returns:\n            Mapping of column name to matched PII rule name.\n        \"\"\"\n        detected = {}\n\n        # 1. Check column names (heuristics)\n        for col in df.columns:\n            col_lower = col.lower()\n            for rule in self.config.rules:\n                if rule.name in col_lower:\n                    detected[col] = rule.name\n                    break\n\n        # 2. Check content for remaining columns\n        # Sample first 100 rows (or all if small) to speed up\n        sample = df.head(100)\n\n        for col in df.columns:\n            if col in detected:\n                continue\n\n            # Skip non-string columns for regex matching\n            if not pd.api.types.is_string_dtype(sample[col]):\n                continue\n\n            valid_rows = sample[col].dropna().astype(str)\n            if len(valid_rows) == 0:\n                continue\n\n            # Check each rule\n            best_rule = None\n            max_matches = 0\n\n            for rule in self.config.rules:\n                match_count = 0\n                for val in valid_rows:\n                    # Check any pattern for this rule\n                    for pat in rule.patterns:\n                        if re.search(pat, val):\n                            match_count += 1\n                            break # Match found for this value\n\n                # If &gt; 50% match, consider it a candidate\n                if match_count &gt; len(valid_rows) * 0.5:\n                    if match_count &gt; max_matches:\n                        max_matches = match_count\n                        best_rule = rule.name\n\n            if best_rule:\n                detected[col] = best_rule\n\n        return detected\n\n    def sanitize(self, df: pd.DataFrame, pii_map: Optional[Dict[str, str]] = None) -&gt; pd.DataFrame:\n        \"\"\"Apply sanitization rules to a dataframe.\n\n        Args:\n            df: Input dataframe containing potential PII.\n            pii_map: Optional precomputed map of column name to PII rule name.\n\n        Returns:\n            Sanitized dataframe with PII handled according to configured actions.\n        \"\"\"\n        if pii_map is None:\n            pii_map = self.analyze(df)\n\n        output_df = df.copy()\n\n        for col, rule_name in pii_map.items():\n            rule = next((r for r in self.config.rules if r.name == rule_name), None)\n            if not rule:\n                continue\n\n            if rule.action == \"drop\":\n                output_df.drop(columns=[col], inplace=True)\n\n            elif rule.action == \"mask\":\n                output_df[col] = output_df[col].apply(lambda x: self._mask_value(x))\n\n            elif rule.action == \"hash\":\n                output_df[col] = output_df[col].apply(lambda x: self._hash_value(x))\n\n            elif rule.action == \"fake\":\n                output_df[col] = self._fake_column(output_df, col, rule)\n\n            elif rule.action == \"custom\":\n                if rule.custom_generator:\n                    # Use custom generator, passing row context\n                    # Note: This checks frame line by line, slower but powerful\n                    output_df[col] = output_df.apply(lambda row: rule.custom_generator(row.to_dict()), axis=1)\n                else:\n                    # Fallback if no generator provided\n                    output_df[col] = self._mask_value(output_df[col])\n\n        return output_df\n\n    def _mask_value(self, val: Any) -&gt; str:\n        \"\"\"Mask a value, preserving only the last four characters.\"\"\"\n        s = str(val)\n        if len(s) &lt;= 4:\n            return \"*\" * len(s)\n        return \"*\" * (len(s) - 4) + s[-4:]\n\n    def _hash_value(self, val: Any) -&gt; str:\n        \"\"\"Return a SHA256 hash representation of a value.\"\"\"\n        return hashlib.sha256(str(val).encode()).hexdigest()\n\n    def _fake_column(self, df: pd.DataFrame, col: str, rule: PiiRule) -&gt; pd.Series:\n        \"\"\"Generate fake data for a column using contextual faker.\n\n        Args:\n            df: DataFrame containing the column to fake.\n            col: Column name.\n            rule: PII rule describing the type being faked.\n\n        Returns:\n            Series of fake values aligned to ``df``.\n        \"\"\"\n        # Context strategy: \n        # If the rule has a context_key (not yet fully implemented in config, but good for future), use it.\n        # Fallback to simple random generation.\n\n        # We can pass the dataframe to the faker to handle this column\n        # But our FakerContextual currently handles whole DF. \n        # Let's call generate_pii for the length of DF.\n\n        # Optimization: fast path if no context needed\n        return df.apply(lambda row: self.faker.generate_pii(rule.name, context=row.to_dict())[0], axis=1)\n</code></pre>"},{"location":"api/privacy/#syntho_hive.privacy.sanitizer.PIISanitizer.analyze","title":"analyze","text":"<pre><code>analyze(df: DataFrame) -&gt; Dict[str, str]\n</code></pre> <p>Detect potential PII columns using configured rules.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to inspect for PII.</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Mapping of column name to matched PII rule name.</p> Source code in <code>syntho_hive/privacy/sanitizer.py</code> <pre><code>def analyze(self, df: pd.DataFrame) -&gt; Dict[str, str]:\n    \"\"\"Detect potential PII columns using configured rules.\n\n    Args:\n        df: DataFrame to inspect for PII.\n\n    Returns:\n        Mapping of column name to matched PII rule name.\n    \"\"\"\n    detected = {}\n\n    # 1. Check column names (heuristics)\n    for col in df.columns:\n        col_lower = col.lower()\n        for rule in self.config.rules:\n            if rule.name in col_lower:\n                detected[col] = rule.name\n                break\n\n    # 2. Check content for remaining columns\n    # Sample first 100 rows (or all if small) to speed up\n    sample = df.head(100)\n\n    for col in df.columns:\n        if col in detected:\n            continue\n\n        # Skip non-string columns for regex matching\n        if not pd.api.types.is_string_dtype(sample[col]):\n            continue\n\n        valid_rows = sample[col].dropna().astype(str)\n        if len(valid_rows) == 0:\n            continue\n\n        # Check each rule\n        best_rule = None\n        max_matches = 0\n\n        for rule in self.config.rules:\n            match_count = 0\n            for val in valid_rows:\n                # Check any pattern for this rule\n                for pat in rule.patterns:\n                    if re.search(pat, val):\n                        match_count += 1\n                        break # Match found for this value\n\n            # If &gt; 50% match, consider it a candidate\n            if match_count &gt; len(valid_rows) * 0.5:\n                if match_count &gt; max_matches:\n                    max_matches = match_count\n                    best_rule = rule.name\n\n        if best_rule:\n            detected[col] = best_rule\n\n    return detected\n</code></pre>"},{"location":"api/privacy/#syntho_hive.privacy.sanitizer.PIISanitizer.sanitize","title":"sanitize","text":"<pre><code>sanitize(df: DataFrame, pii_map: Optional[Dict[str, str]] = None) -&gt; pd.DataFrame\n</code></pre> <p>Apply sanitization rules to a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input dataframe containing potential PII.</p> required <code>pii_map</code> <code>Optional[Dict[str, str]]</code> <p>Optional precomputed map of column name to PII rule name.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Sanitized dataframe with PII handled according to configured actions.</p> Source code in <code>syntho_hive/privacy/sanitizer.py</code> <pre><code>def sanitize(self, df: pd.DataFrame, pii_map: Optional[Dict[str, str]] = None) -&gt; pd.DataFrame:\n    \"\"\"Apply sanitization rules to a dataframe.\n\n    Args:\n        df: Input dataframe containing potential PII.\n        pii_map: Optional precomputed map of column name to PII rule name.\n\n    Returns:\n        Sanitized dataframe with PII handled according to configured actions.\n    \"\"\"\n    if pii_map is None:\n        pii_map = self.analyze(df)\n\n    output_df = df.copy()\n\n    for col, rule_name in pii_map.items():\n        rule = next((r for r in self.config.rules if r.name == rule_name), None)\n        if not rule:\n            continue\n\n        if rule.action == \"drop\":\n            output_df.drop(columns=[col], inplace=True)\n\n        elif rule.action == \"mask\":\n            output_df[col] = output_df[col].apply(lambda x: self._mask_value(x))\n\n        elif rule.action == \"hash\":\n            output_df[col] = output_df[col].apply(lambda x: self._hash_value(x))\n\n        elif rule.action == \"fake\":\n            output_df[col] = self._fake_column(output_df, col, rule)\n\n        elif rule.action == \"custom\":\n            if rule.custom_generator:\n                # Use custom generator, passing row context\n                # Note: This checks frame line by line, slower but powerful\n                output_df[col] = output_df.apply(lambda row: rule.custom_generator(row.to_dict()), axis=1)\n            else:\n                # Fallback if no generator provided\n                output_df[col] = self._mask_value(output_df[col])\n\n    return output_df\n</code></pre>"},{"location":"api/privacy/#configuration","title":"Configuration","text":""},{"location":"api/privacy/#syntho_hive.privacy.sanitizer.PiiRule","title":"syntho_hive.privacy.sanitizer.PiiRule  <code>dataclass</code>","text":"<p>Configuration for a single PII type and handling strategy.</p> Source code in <code>syntho_hive/privacy/sanitizer.py</code> <pre><code>@dataclass\nclass PiiRule:\n    \"\"\"Configuration for a single PII type and handling strategy.\"\"\"\n    name: str\n    patterns: List[str]  # List of regex patterns to match\n    action: str = \"drop\"  # Options: \"drop\", \"mask\", \"hash\", \"fake\", \"custom\", \"keep\"\n    context_key: Optional[str] = None  # Key to look for in context (e.g. 'country' for locale)\n    custom_generator: Optional[Callable[[Dict[str, Any]], Any]] = None  # Custom lambda for generation\n</code></pre>"},{"location":"api/privacy/#syntho_hive.privacy.sanitizer.PrivacyConfig","title":"syntho_hive.privacy.sanitizer.PrivacyConfig  <code>dataclass</code>","text":"<p>Collection of rules for PII detection and handling.</p> Source code in <code>syntho_hive/privacy/sanitizer.py</code> <pre><code>@dataclass\nclass PrivacyConfig:\n    \"\"\"Collection of rules for PII detection and handling.\"\"\"\n    rules: List[PiiRule] = field(default_factory=list)\n\n    @classmethod\n    def default(cls) -&gt; 'PrivacyConfig':\n        \"\"\"Create a default privacy configuration with common PII rules.\"\"\"\n        return cls(rules=[\n            PiiRule(name=\"email\", patterns=[r\"[^@]+@[^@]+\\.[^@]+\"], action=\"fake\"),\n            PiiRule(name=\"ssn\", patterns=[r\"\\d{3}-\\d{2}-\\d{4}\"], action=\"mask\"),\n            PiiRule(name=\"phone\", patterns=[r\"\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\"], action=\"fake\"),\n            PiiRule(name=\"credit_card\", patterns=[r\"\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}\"], action=\"mask\"),\n            PiiRule(name=\"ipv4\", patterns=[r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\"], action=\"fake\"),\n        ])\n</code></pre>"},{"location":"api/privacy/#syntho_hive.privacy.sanitizer.PrivacyConfig.default","title":"default  <code>classmethod</code>","text":"<pre><code>default() -&gt; PrivacyConfig\n</code></pre> <p>Create a default privacy configuration with common PII rules.</p> Source code in <code>syntho_hive/privacy/sanitizer.py</code> <pre><code>@classmethod\ndef default(cls) -&gt; 'PrivacyConfig':\n    \"\"\"Create a default privacy configuration with common PII rules.\"\"\"\n    return cls(rules=[\n        PiiRule(name=\"email\", patterns=[r\"[^@]+@[^@]+\\.[^@]+\"], action=\"fake\"),\n        PiiRule(name=\"ssn\", patterns=[r\"\\d{3}-\\d{2}-\\d{4}\"], action=\"mask\"),\n        PiiRule(name=\"phone\", patterns=[r\"\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\"], action=\"fake\"),\n        PiiRule(name=\"credit_card\", patterns=[r\"\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}\"], action=\"mask\"),\n        PiiRule(name=\"ipv4\", patterns=[r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\"], action=\"fake\"),\n    ])\n</code></pre>"},{"location":"api/privacy/#faking-strategy","title":"Faking Strategy","text":""},{"location":"api/privacy/#syntho_hive.privacy.faker_contextual.ContextualFaker","title":"syntho_hive.privacy.faker_contextual.ContextualFaker","text":"<p>Context-aware PII generator leveraging Faker locales.</p> Source code in <code>syntho_hive/privacy/faker_contextual.py</code> <pre><code>class ContextualFaker:\n    \"\"\"Context-aware PII generator leveraging Faker locales.\"\"\"\n\n    LOCALE_MAP = {\n        \"JP\": \"ja_JP\",\n        \"US\": \"en_US\",\n        \"UK\": \"en_GB\",\n        \"GB\": \"en_GB\",\n        \"DE\": \"de_DE\",\n        \"FR\": \"fr_FR\",\n        \"CN\": \"zh_CN\",\n        \"IN\": \"en_IN\",\n        # Add more as needed\n    }\n\n    def __init__(self):\n        \"\"\"Initialize faker cache and logger.\"\"\"\n        self._fakers: Dict[str, Faker] = {}\n        # Initialize default\n        self._fakers[\"default\"] = Faker()\n        self.logger = logging.getLogger(__name__)\n\n    def _get_faker(self, locale: Optional[str]) -&gt; Faker:\n        \"\"\"Get or create a Faker instance for a locale.\n\n        Args:\n            locale: Optional locale string (e.g., ``\"JP\"`` or ``\"en_US\"``).\n\n        Returns:\n            Faker instance configured for the requested locale.\n        \"\"\"\n        if not locale:\n            return self._fakers[\"default\"]\n\n        mapped_locale = self.LOCALE_MAP.get(locale.upper(), \"en_US\")\n\n        if mapped_locale not in self._fakers:\n            try:\n                self._fakers[mapped_locale] = Faker(mapped_locale)\n            except Exception as e:\n                self.logger.warning(f\"Could not load locale {mapped_locale}, falling back to default. Error: {e}\")\n                self._fakers[mapped_locale] = self._fakers[\"default\"]\n\n        return self._fakers[mapped_locale]\n\n    def generate_pii(self, pii_type: str, context: Optional[Dict[str, Any]] = None, count: int = 1) -&gt; List[str]:\n        \"\"\"Generate PII values with optional contextual locale.\n\n        Args:\n            pii_type: Faker provider name (e.g., ``\"email\"`` or ``\"phone\"``).\n            context: Optional row context used to infer locale (country/locale/region keys).\n            count: Number of values to generate.\n\n        Returns:\n            List of generated PII strings.\n        \"\"\"\n        if context is None:\n            context = {}\n\n        # Attempt to infer locale from context\n        # Heuristic: Look for 'country', 'region', 'locale' keys\n        locale = context.get('country') or context.get('locale') or context.get('region')\n\n        fake = self._get_faker(locale if isinstance(locale, str) else None)\n\n        results = []\n        for _ in range(count):\n            val = self._generate_single_value(fake, pii_type)\n            results.append(val)\n\n        return results\n\n    def _generate_single_value(self, fake: Faker, pii_type: str) -&gt; str:\n        \"\"\"Generate a single PII value using a Faker instance.\"\"\"\n        try:\n            if hasattr(fake, pii_type):\n                 # Dynamic method call on Faker instance\n                return str(getattr(fake, pii_type)())\n\n            # Custom mappings for common PII types if name mismatch or special logic\n            if pii_type == 'phone':\n                 return fake.phone_number()\n            elif pii_type == 'ip' or pii_type == 'ipv4':\n                return fake.ipv4()\n            elif pii_type == 'credit_card':\n                return fake.credit_card_number()\n\n            # Fallback\n            return str(fake.text(max_nb_chars=20))\n        except Exception as e:\n            self.logger.error(f\"Error generating {pii_type}: {e}\")\n            return \"REDACTED\"\n\n    def process_dataframe(self, df: pd.DataFrame, pii_cols: Dict[str, str]) -&gt; pd.DataFrame:\n        \"\"\"Replace placeholders with generated PII in a dataframe.\n\n        Args:\n            df: Input dataframe containing placeholder columns.\n            pii_cols: Mapping of column name to PII type (e.g., ``{\"user_email\": \"email\"}``).\n\n        Returns:\n            DataFrame with specified columns replaced by generated PII.\n        \"\"\"\n        output_df = df.copy()\n\n        # Check if we have context columns\n        has_country_context = 'country' in df.columns or 'locale' in df.columns\n\n        if not has_country_context:\n            # Fast path: Vectorized apply (Fake doesn't vectorize well but we avoid row iteration overhead if possible)\n            # Actually simpler: Just generate N fake values using default locale\n            for col, pii_type in pii_cols.items():\n                fake = self._get_faker(None)\n                # Generate list\n                values = [self._generate_single_value(fake, pii_type) for _ in range(len(df))]\n                output_df[col] = values\n        else:\n            # Slow path: Row-by-row for context awareness\n            for idx, row in output_df.iterrows():\n                context = row.to_dict()\n                for col, pii_type in pii_cols.items():\n                    val = self.generate_pii(pii_type, context=context, count=1)[0]\n                    output_df.at[idx, col] = val\n\n        return output_df\n</code></pre>"},{"location":"api/privacy/#syntho_hive.privacy.faker_contextual.ContextualFaker.generate_pii","title":"generate_pii","text":"<pre><code>generate_pii(pii_type: str, context: Optional[Dict[str, Any]] = None, count: int = 1) -&gt; List[str]\n</code></pre> <p>Generate PII values with optional contextual locale.</p> <p>Parameters:</p> Name Type Description Default <code>pii_type</code> <code>str</code> <p>Faker provider name (e.g., <code>\"email\"</code> or <code>\"phone\"</code>).</p> required <code>context</code> <code>Optional[Dict[str, Any]]</code> <p>Optional row context used to infer locale (country/locale/region keys).</p> <code>None</code> <code>count</code> <code>int</code> <p>Number of values to generate.</p> <code>1</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of generated PII strings.</p> Source code in <code>syntho_hive/privacy/faker_contextual.py</code> <pre><code>def generate_pii(self, pii_type: str, context: Optional[Dict[str, Any]] = None, count: int = 1) -&gt; List[str]:\n    \"\"\"Generate PII values with optional contextual locale.\n\n    Args:\n        pii_type: Faker provider name (e.g., ``\"email\"`` or ``\"phone\"``).\n        context: Optional row context used to infer locale (country/locale/region keys).\n        count: Number of values to generate.\n\n    Returns:\n        List of generated PII strings.\n    \"\"\"\n    if context is None:\n        context = {}\n\n    # Attempt to infer locale from context\n    # Heuristic: Look for 'country', 'region', 'locale' keys\n    locale = context.get('country') or context.get('locale') or context.get('region')\n\n    fake = self._get_faker(locale if isinstance(locale, str) else None)\n\n    results = []\n    for _ in range(count):\n        val = self._generate_single_value(fake, pii_type)\n        results.append(val)\n\n    return results\n</code></pre>"},{"location":"api/privacy/#syntho_hive.privacy.faker_contextual.ContextualFaker.process_dataframe","title":"process_dataframe","text":"<pre><code>process_dataframe(df: DataFrame, pii_cols: Dict[str, str]) -&gt; pd.DataFrame\n</code></pre> <p>Replace placeholders with generated PII in a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input dataframe containing placeholder columns.</p> required <code>pii_cols</code> <code>Dict[str, str]</code> <p>Mapping of column name to PII type (e.g., <code>{\"user_email\": \"email\"}</code>).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with specified columns replaced by generated PII.</p> Source code in <code>syntho_hive/privacy/faker_contextual.py</code> <pre><code>def process_dataframe(self, df: pd.DataFrame, pii_cols: Dict[str, str]) -&gt; pd.DataFrame:\n    \"\"\"Replace placeholders with generated PII in a dataframe.\n\n    Args:\n        df: Input dataframe containing placeholder columns.\n        pii_cols: Mapping of column name to PII type (e.g., ``{\"user_email\": \"email\"}``).\n\n    Returns:\n        DataFrame with specified columns replaced by generated PII.\n    \"\"\"\n    output_df = df.copy()\n\n    # Check if we have context columns\n    has_country_context = 'country' in df.columns or 'locale' in df.columns\n\n    if not has_country_context:\n        # Fast path: Vectorized apply (Fake doesn't vectorize well but we avoid row iteration overhead if possible)\n        # Actually simpler: Just generate N fake values using default locale\n        for col, pii_type in pii_cols.items():\n            fake = self._get_faker(None)\n            # Generate list\n            values = [self._generate_single_value(fake, pii_type) for _ in range(len(df))]\n            output_df[col] = values\n    else:\n        # Slow path: Row-by-row for context awareness\n        for idx, row in output_df.iterrows():\n            context = row.to_dict()\n            for col, pii_type in pii_cols.items():\n                val = self.generate_pii(pii_type, context=context, count=1)[0]\n                output_df.at[idx, col] = val\n\n    return output_df\n</code></pre>"},{"location":"api/relational/","title":"Relational Orchestration","text":"<p>This module handles the complexity of multi-table generation, ensuring referential integrity and statistical correlation between tables.</p>"},{"location":"api/relational/#orchestrator","title":"Orchestrator","text":""},{"location":"api/relational/#syntho_hive.relational.orchestrator.StagedOrchestrator","title":"syntho_hive.relational.orchestrator.StagedOrchestrator","text":"<p>Manage staged relational synthesis across parent/child tables.</p> Source code in <code>syntho_hive/relational/orchestrator.py</code> <pre><code>class StagedOrchestrator:\n    \"\"\"Manage staged relational synthesis across parent/child tables.\"\"\"\n\n    def __init__(self, metadata: Metadata, spark: SparkSession):\n        \"\"\"Initialize orchestrator dependencies.\n\n        Args:\n            metadata: Dataset metadata with relational details.\n            spark: SparkSession used for IO and potential UDFs.\n        \"\"\"\n        self.metadata = metadata\n        self.spark = spark\n        self.graph = SchemaGraph(metadata)\n        self.io = SparkIO(spark)\n        self.models: Dict[str, CTGAN] = {}\n        self.linkage_models: Dict[str, LinkageModel] = {}\n\n    def fit_all(self, real_data_paths: Dict[str, str], epochs: int = 300, batch_size: int = 500, **model_kwargs: Union[int, str, Tuple[int, int]]):\n        \"\"\"Fit CTGAN and linkage models for every table.\n\n        Args:\n            real_data_paths: Mapping ``{table_name: 'db.table' or '/path'}``.\n            epochs: Number of training epochs for CTGAN.\n            batch_size: Training batch size.\n            **model_kwargs: Extra parameters forwarded to CTGAN constructor.\n        \"\"\"\n        # Topo sort to train parents first? Or independent?\n        # Linkage model needs both parent and child data.\n        # CTGAN needs Child data + Parent attributes (joined).\n\n        # Training order doesn't strictly matter as long as we have data, \n        # but generation order matters.\n\n        for table_name in self.metadata.tables:\n            print(f\"Fitting model for table: {table_name}\")\n            data_path = real_data_paths.get(table_name)\n            if not data_path:\n                print(f\"Warning: No data path provided for {table_name}, skipping.\")\n                continue\n\n            # Read data\n            target_df = self.io.read_dataset(data_path)\n            # Convert to Pandas for CTGAN (prototype limitation)\n            target_pdf = target_df.toPandas()\n\n            config = self.metadata.get_table(table_name)\n            if not config.has_dependencies:\n                # Root Table\n                model = CTGAN(\n                    self.metadata, \n                    batch_size=batch_size, \n                    epochs=epochs,\n                    **model_kwargs\n                )\n                model.fit(target_pdf, table_name=table_name)\n                self.models[table_name] = model\n            else:\n                # Child Table\n                # 1. Identify \"Driver\" Parent (First FK)\n                pk_map = config.fk\n                # pk_map is {local_col: \"parent_table.parent_col\"}\n\n                # Sort keys to ensure deterministic driver selection\n                sorted_fks = sorted(pk_map.keys())\n                driver_fk = sorted_fks[0]\n                driver_ref = pk_map[driver_fk]\n\n                driver_parent_table, driver_parent_pk = driver_ref.split(\".\")\n\n                parent_path = real_data_paths.get(driver_parent_table)\n                parent_df = self.io.read_dataset(parent_path).toPandas()\n\n                # 2. Train Linkage Model on Driver Parent\n                print(f\"Training Linkage for {table_name} driven by {driver_parent_table}\")\n                linkage = LinkageModel()\n                linkage.fit(parent_df, target_pdf, fk_col=driver_fk, pk_col=driver_parent_pk)\n                self.linkage_models[table_name] = linkage\n\n                # 3. Train Conditional CTGAN (Conditioning on Driver Parent Context)\n                context_cols = config.parent_context_cols\n                if context_cols:\n                     # Prepare parent data for merge\n                     right_side = parent_df[[driver_parent_pk] + context_cols].copy()\n\n                     rename_map = {c: f\"__ctx__{c}\" for c in context_cols}\n                     right_side = right_side.rename(columns=rename_map)\n\n                     joined = target_pdf.merge(\n                         right_side,\n                         left_on=driver_fk,\n                         right_on=driver_parent_pk,\n                         how=\"left\"\n                     )\n\n                     context_df = joined[list(rename_map.values())].copy()\n                     context_df.columns = context_cols\n                else:\n                    context_df = None\n\n                model = CTGAN(\n                    self.metadata, \n                    batch_size=batch_size, \n                    epochs=epochs,\n                    **model_kwargs\n                )\n                # Note: We exclude ALL FK columns from CTGAN modeling to avoid them being treated as continuous/categorical features\n                # The DataTransformer handles excluding PK/FK if they are marked in metadata.\n                # But we must ensure metadata knows about ALL FKs. (It does via config.fk)\n                model.fit(target_pdf, context=context_df, table_name=table_name)\n                self.models[table_name] = model\n\n    def generate(self, num_rows_root: Dict[str, int], output_path_base: Optional[str] = None) -&gt; Dict[str, pd.DataFrame]:\n        \"\"\"Execute the multi-stage generation pipeline.\n\n        Args:\n            num_rows_root: Mapping of root table name to number of rows to generate.\n            output_path_base: Base path where generated tables will be stored. If None, returns DataFrames in memory.\n\n        Returns:\n            Dictionary of generated DataFrames {table_name: dataframe}.\n        \"\"\"\n        generation_order = self.graph.get_generation_order()\n\n        generated_tables = {}\n\n        for table_name in generation_order:\n            config = self.metadata.get_table(table_name)\n            is_root = not config.fk\n\n            model = self.models[table_name]\n\n            generated_pdf = None\n\n            if is_root:\n                print(f\"Generating root table: {table_name}\")\n                n_rows = num_rows_root.get(table_name, 1000)\n                generated_pdf = model.sample(n_rows)\n                # Assign PKs\n                generated_pdf[config.pk] = range(1, n_rows + 1)\n            else:\n                print(f\"Generating child table: {table_name}\")\n\n                # 1. Handle Driver Parent (Cardinality &amp; Context)\n                pk_map = config.fk\n                sorted_fks = sorted(pk_map.keys())\n                driver_fk = sorted_fks[0]\n                driver_ref = pk_map[driver_fk]\n                driver_parent_table, driver_parent_pk = driver_ref.split(\".\")\n\n                # Read Driver Parent Data (From Output or Memory)\n                if output_path_base:\n                     parent_path = f\"{output_path_base}/{driver_parent_table}\"\n                     parent_df = self.io.read_dataset(parent_path).toPandas()\n                else:\n                     parent_df = generated_tables[driver_parent_table]\n\n                linkage = self.linkage_models[table_name]\n\n                # Sample Counts\n                counts = linkage.sample_counts(parent_df)\n\n                # Construct Context from Driver\n                parent_ids_repeated = np.repeat(parent_df[driver_parent_pk].values, counts)\n\n                context_cols = config.parent_context_cols\n                if context_cols:\n                    context_repeated_vals = {}\n                    for col in context_cols:\n                        context_repeated_vals[col] = np.repeat(parent_df[col].values, counts)\n                    context_df = pd.DataFrame(context_repeated_vals)\n                else:\n                    context_df = None\n\n                total_child_rows = len(parent_ids_repeated)\n\n                # 2. Generate Data\n                if total_child_rows &gt; 0:\n                     generated_pdf = model.sample(total_child_rows, context=context_df)\n\n                     # Assign Driver FK\n                     generated_pdf[driver_fk] = parent_ids_repeated\n\n                     # Assign Secondary FKs (Random Sampling from respective Parents)\n                     for fk_col in sorted_fks[1:]:\n                         ref = pk_map[fk_col]\n                         p_table, p_pk = ref.split(\".\")\n\n                         # Read Secondary Parent\n                         if output_path_base:\n                             p_path = f\"{output_path_base}/{p_table}\"\n                             p_df = self.io.read_dataset(p_path).toPandas()\n                         else:\n                             p_df = generated_tables[p_table]\n\n                         valid_pks = p_df[p_pk].values\n\n                         # Randomly sample valid PKs for this column\n                         generated_pdf[fk_col] = np.random.choice(valid_pks, size=total_child_rows)\n\n                     # Assign PKs\n                     generated_pdf[config.pk] = range(1, total_child_rows + 1)\n\n            if generated_pdf is not None:\n                if output_path_base:\n                    output_path = f\"{output_path_base}/{table_name}\"\n                    self.io.write_pandas(generated_pdf, output_path)\n\n                # Always store in memory for downstream children if needed (and return if requested)\n                # For massive datasets this might be risky, but consistent with user request: \"save to a df object\"\n                generated_tables[table_name] = generated_pdf\n\n        return generated_tables\n</code></pre>"},{"location":"api/relational/#syntho_hive.relational.orchestrator.StagedOrchestrator.fit_all","title":"fit_all","text":"<pre><code>fit_all(real_data_paths: Dict[str, str], epochs: int = 300, batch_size: int = 500, **model_kwargs: Union[int, str, Tuple[int, int]])\n</code></pre> <p>Fit CTGAN and linkage models for every table.</p> <p>Parameters:</p> Name Type Description Default <code>real_data_paths</code> <code>Dict[str, str]</code> <p>Mapping <code>{table_name: 'db.table' or '/path'}</code>.</p> required <code>epochs</code> <code>int</code> <p>Number of training epochs for CTGAN.</p> <code>300</code> <code>batch_size</code> <code>int</code> <p>Training batch size.</p> <code>500</code> <code>**model_kwargs</code> <code>Union[int, str, Tuple[int, int]]</code> <p>Extra parameters forwarded to CTGAN constructor.</p> <code>{}</code> Source code in <code>syntho_hive/relational/orchestrator.py</code> <pre><code>def fit_all(self, real_data_paths: Dict[str, str], epochs: int = 300, batch_size: int = 500, **model_kwargs: Union[int, str, Tuple[int, int]]):\n    \"\"\"Fit CTGAN and linkage models for every table.\n\n    Args:\n        real_data_paths: Mapping ``{table_name: 'db.table' or '/path'}``.\n        epochs: Number of training epochs for CTGAN.\n        batch_size: Training batch size.\n        **model_kwargs: Extra parameters forwarded to CTGAN constructor.\n    \"\"\"\n    # Topo sort to train parents first? Or independent?\n    # Linkage model needs both parent and child data.\n    # CTGAN needs Child data + Parent attributes (joined).\n\n    # Training order doesn't strictly matter as long as we have data, \n    # but generation order matters.\n\n    for table_name in self.metadata.tables:\n        print(f\"Fitting model for table: {table_name}\")\n        data_path = real_data_paths.get(table_name)\n        if not data_path:\n            print(f\"Warning: No data path provided for {table_name}, skipping.\")\n            continue\n\n        # Read data\n        target_df = self.io.read_dataset(data_path)\n        # Convert to Pandas for CTGAN (prototype limitation)\n        target_pdf = target_df.toPandas()\n\n        config = self.metadata.get_table(table_name)\n        if not config.has_dependencies:\n            # Root Table\n            model = CTGAN(\n                self.metadata, \n                batch_size=batch_size, \n                epochs=epochs,\n                **model_kwargs\n            )\n            model.fit(target_pdf, table_name=table_name)\n            self.models[table_name] = model\n        else:\n            # Child Table\n            # 1. Identify \"Driver\" Parent (First FK)\n            pk_map = config.fk\n            # pk_map is {local_col: \"parent_table.parent_col\"}\n\n            # Sort keys to ensure deterministic driver selection\n            sorted_fks = sorted(pk_map.keys())\n            driver_fk = sorted_fks[0]\n            driver_ref = pk_map[driver_fk]\n\n            driver_parent_table, driver_parent_pk = driver_ref.split(\".\")\n\n            parent_path = real_data_paths.get(driver_parent_table)\n            parent_df = self.io.read_dataset(parent_path).toPandas()\n\n            # 2. Train Linkage Model on Driver Parent\n            print(f\"Training Linkage for {table_name} driven by {driver_parent_table}\")\n            linkage = LinkageModel()\n            linkage.fit(parent_df, target_pdf, fk_col=driver_fk, pk_col=driver_parent_pk)\n            self.linkage_models[table_name] = linkage\n\n            # 3. Train Conditional CTGAN (Conditioning on Driver Parent Context)\n            context_cols = config.parent_context_cols\n            if context_cols:\n                 # Prepare parent data for merge\n                 right_side = parent_df[[driver_parent_pk] + context_cols].copy()\n\n                 rename_map = {c: f\"__ctx__{c}\" for c in context_cols}\n                 right_side = right_side.rename(columns=rename_map)\n\n                 joined = target_pdf.merge(\n                     right_side,\n                     left_on=driver_fk,\n                     right_on=driver_parent_pk,\n                     how=\"left\"\n                 )\n\n                 context_df = joined[list(rename_map.values())].copy()\n                 context_df.columns = context_cols\n            else:\n                context_df = None\n\n            model = CTGAN(\n                self.metadata, \n                batch_size=batch_size, \n                epochs=epochs,\n                **model_kwargs\n            )\n            # Note: We exclude ALL FK columns from CTGAN modeling to avoid them being treated as continuous/categorical features\n            # The DataTransformer handles excluding PK/FK if they are marked in metadata.\n            # But we must ensure metadata knows about ALL FKs. (It does via config.fk)\n            model.fit(target_pdf, context=context_df, table_name=table_name)\n            self.models[table_name] = model\n</code></pre>"},{"location":"api/relational/#syntho_hive.relational.orchestrator.StagedOrchestrator.generate","title":"generate","text":"<pre><code>generate(num_rows_root: Dict[str, int], output_path_base: Optional[str] = None) -&gt; Dict[str, pd.DataFrame]\n</code></pre> <p>Execute the multi-stage generation pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>num_rows_root</code> <code>Dict[str, int]</code> <p>Mapping of root table name to number of rows to generate.</p> required <code>output_path_base</code> <code>Optional[str]</code> <p>Base path where generated tables will be stored. If None, returns DataFrames in memory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, DataFrame]</code> <p>Dictionary of generated DataFrames {table_name: dataframe}.</p> Source code in <code>syntho_hive/relational/orchestrator.py</code> <pre><code>def generate(self, num_rows_root: Dict[str, int], output_path_base: Optional[str] = None) -&gt; Dict[str, pd.DataFrame]:\n    \"\"\"Execute the multi-stage generation pipeline.\n\n    Args:\n        num_rows_root: Mapping of root table name to number of rows to generate.\n        output_path_base: Base path where generated tables will be stored. If None, returns DataFrames in memory.\n\n    Returns:\n        Dictionary of generated DataFrames {table_name: dataframe}.\n    \"\"\"\n    generation_order = self.graph.get_generation_order()\n\n    generated_tables = {}\n\n    for table_name in generation_order:\n        config = self.metadata.get_table(table_name)\n        is_root = not config.fk\n\n        model = self.models[table_name]\n\n        generated_pdf = None\n\n        if is_root:\n            print(f\"Generating root table: {table_name}\")\n            n_rows = num_rows_root.get(table_name, 1000)\n            generated_pdf = model.sample(n_rows)\n            # Assign PKs\n            generated_pdf[config.pk] = range(1, n_rows + 1)\n        else:\n            print(f\"Generating child table: {table_name}\")\n\n            # 1. Handle Driver Parent (Cardinality &amp; Context)\n            pk_map = config.fk\n            sorted_fks = sorted(pk_map.keys())\n            driver_fk = sorted_fks[0]\n            driver_ref = pk_map[driver_fk]\n            driver_parent_table, driver_parent_pk = driver_ref.split(\".\")\n\n            # Read Driver Parent Data (From Output or Memory)\n            if output_path_base:\n                 parent_path = f\"{output_path_base}/{driver_parent_table}\"\n                 parent_df = self.io.read_dataset(parent_path).toPandas()\n            else:\n                 parent_df = generated_tables[driver_parent_table]\n\n            linkage = self.linkage_models[table_name]\n\n            # Sample Counts\n            counts = linkage.sample_counts(parent_df)\n\n            # Construct Context from Driver\n            parent_ids_repeated = np.repeat(parent_df[driver_parent_pk].values, counts)\n\n            context_cols = config.parent_context_cols\n            if context_cols:\n                context_repeated_vals = {}\n                for col in context_cols:\n                    context_repeated_vals[col] = np.repeat(parent_df[col].values, counts)\n                context_df = pd.DataFrame(context_repeated_vals)\n            else:\n                context_df = None\n\n            total_child_rows = len(parent_ids_repeated)\n\n            # 2. Generate Data\n            if total_child_rows &gt; 0:\n                 generated_pdf = model.sample(total_child_rows, context=context_df)\n\n                 # Assign Driver FK\n                 generated_pdf[driver_fk] = parent_ids_repeated\n\n                 # Assign Secondary FKs (Random Sampling from respective Parents)\n                 for fk_col in sorted_fks[1:]:\n                     ref = pk_map[fk_col]\n                     p_table, p_pk = ref.split(\".\")\n\n                     # Read Secondary Parent\n                     if output_path_base:\n                         p_path = f\"{output_path_base}/{p_table}\"\n                         p_df = self.io.read_dataset(p_path).toPandas()\n                     else:\n                         p_df = generated_tables[p_table]\n\n                     valid_pks = p_df[p_pk].values\n\n                     # Randomly sample valid PKs for this column\n                     generated_pdf[fk_col] = np.random.choice(valid_pks, size=total_child_rows)\n\n                 # Assign PKs\n                 generated_pdf[config.pk] = range(1, total_child_rows + 1)\n\n        if generated_pdf is not None:\n            if output_path_base:\n                output_path = f\"{output_path_base}/{table_name}\"\n                self.io.write_pandas(generated_pdf, output_path)\n\n            # Always store in memory for downstream children if needed (and return if requested)\n            # For massive datasets this might be risky, but consistent with user request: \"save to a df object\"\n            generated_tables[table_name] = generated_pdf\n\n    return generated_tables\n</code></pre>"},{"location":"api/relational/#linkage","title":"Linkage","text":""},{"location":"api/relational/#syntho_hive.relational.linkage.LinkageModel","title":"syntho_hive.relational.linkage.LinkageModel","text":"<p>Model cardinality relationships between parent and child tables.</p> Source code in <code>syntho_hive/relational/linkage.py</code> <pre><code>class LinkageModel:\n    \"\"\"Model cardinality relationships between parent and child tables.\"\"\"\n\n    def __init__(self, method: str = \"gmm\"):\n        \"\"\"Create a linkage model.\n\n        Args:\n            method: Distribution family used to model child counts.\n        \"\"\"\n        self.method = method\n        self.model = None\n        self.max_children = 0\n\n    def fit(self, parent_df: pd.DataFrame, child_df: pd.DataFrame, fk_col: str, pk_col: str = \"id\"):\n        \"\"\"Fit the distribution of child counts per parent.\n\n        Args:\n            parent_df: Parent table with unique primary keys.\n            child_df: Child table containing foreign keys to parents.\n            fk_col: Name of the foreign key column in the child table.\n            pk_col: Name of the primary key column in the parent table.\n        \"\"\"\n        # 1. Aggregate child counts\n        # Assumes parent_df has unique PKs\n        counts = child_df[fk_col].value_counts()\n\n        # Merge with all parents to include 0-count parents\n        parent_ids = pd.DataFrame(parent_df[pk_col].unique(), columns=[pk_col])\n\n        # Ensure Types Match (Cast Child FK to Parent PK type)\n        try:\n            target_type = parent_ids[pk_col].dtype\n            if not np.issubdtype(counts.index.dtype, target_type):\n                counts.index = counts.index.astype(target_type)\n        except Exception as e:\n            # Fallback to string if direct cast fails\n            print(f\"Warning: Could not cast FK to match PK type ({e}). Falling back to string.\")\n            parent_ids[pk_col] = parent_ids[pk_col].astype(str)\n            counts.index = counts.index.astype(str)\n\n        # Use merge to get counts, fillna(0) for parents with no children\n        # Note: In real Spark env this aggregation happens differently\n        count_df = parent_ids.merge(\n            counts.rename(\"child_count\"), \n            left_on=pk_col, \n            right_index=True, \n            how=\"left\"\n        ).fillna(0)\n\n        X = count_df[\"child_count\"].values.reshape(-1, 1)\n        self.max_children = int(X.max())\n\n        if self.method == \"gmm\":\n            # Using GMM to learn continuous approximation of counts\n            # Could also use NegativeBinomial or KDE\n            self.model = GaussianMixture(n_components=min(5, len(np.unique(X))), random_state=42)\n            self.model.fit(X)\n\n    def sample_counts(self, parent_context: pd.DataFrame) -&gt; np.ndarray:\n        \"\"\"Sample child counts for a set of parents.\n\n        Args:\n            parent_context: Parent dataframe (only length is used here).\n\n        Returns:\n            Numpy array of integer child counts aligned with parents.\n\n        Raises:\n            ValueError: If called before fitting the model.\n        \"\"\"\n        n_samples = len(parent_context)\n        if self.model is None:\n            raise ValueError(\"LinkageModel not fitted\")\n\n        # Sample from GMM\n        counts, _ = self.model.sample(n_samples)\n\n        # Post-process: Round to nearest int, clip to [0, max]\n        counts = np.round(counts).flatten().astype(int)\n        counts = np.clip(counts, 0, None) # Ensure non-negative\n\n        return counts\n</code></pre>"},{"location":"api/relational/#syntho_hive.relational.linkage.LinkageModel.fit","title":"fit","text":"<pre><code>fit(parent_df: DataFrame, child_df: DataFrame, fk_col: str, pk_col: str = 'id')\n</code></pre> <p>Fit the distribution of child counts per parent.</p> <p>Parameters:</p> Name Type Description Default <code>parent_df</code> <code>DataFrame</code> <p>Parent table with unique primary keys.</p> required <code>child_df</code> <code>DataFrame</code> <p>Child table containing foreign keys to parents.</p> required <code>fk_col</code> <code>str</code> <p>Name of the foreign key column in the child table.</p> required <code>pk_col</code> <code>str</code> <p>Name of the primary key column in the parent table.</p> <code>'id'</code> Source code in <code>syntho_hive/relational/linkage.py</code> <pre><code>def fit(self, parent_df: pd.DataFrame, child_df: pd.DataFrame, fk_col: str, pk_col: str = \"id\"):\n    \"\"\"Fit the distribution of child counts per parent.\n\n    Args:\n        parent_df: Parent table with unique primary keys.\n        child_df: Child table containing foreign keys to parents.\n        fk_col: Name of the foreign key column in the child table.\n        pk_col: Name of the primary key column in the parent table.\n    \"\"\"\n    # 1. Aggregate child counts\n    # Assumes parent_df has unique PKs\n    counts = child_df[fk_col].value_counts()\n\n    # Merge with all parents to include 0-count parents\n    parent_ids = pd.DataFrame(parent_df[pk_col].unique(), columns=[pk_col])\n\n    # Ensure Types Match (Cast Child FK to Parent PK type)\n    try:\n        target_type = parent_ids[pk_col].dtype\n        if not np.issubdtype(counts.index.dtype, target_type):\n            counts.index = counts.index.astype(target_type)\n    except Exception as e:\n        # Fallback to string if direct cast fails\n        print(f\"Warning: Could not cast FK to match PK type ({e}). Falling back to string.\")\n        parent_ids[pk_col] = parent_ids[pk_col].astype(str)\n        counts.index = counts.index.astype(str)\n\n    # Use merge to get counts, fillna(0) for parents with no children\n    # Note: In real Spark env this aggregation happens differently\n    count_df = parent_ids.merge(\n        counts.rename(\"child_count\"), \n        left_on=pk_col, \n        right_index=True, \n        how=\"left\"\n    ).fillna(0)\n\n    X = count_df[\"child_count\"].values.reshape(-1, 1)\n    self.max_children = int(X.max())\n\n    if self.method == \"gmm\":\n        # Using GMM to learn continuous approximation of counts\n        # Could also use NegativeBinomial or KDE\n        self.model = GaussianMixture(n_components=min(5, len(np.unique(X))), random_state=42)\n        self.model.fit(X)\n</code></pre>"},{"location":"api/relational/#syntho_hive.relational.linkage.LinkageModel.sample_counts","title":"sample_counts","text":"<pre><code>sample_counts(parent_context: DataFrame) -&gt; np.ndarray\n</code></pre> <p>Sample child counts for a set of parents.</p> <p>Parameters:</p> Name Type Description Default <code>parent_context</code> <code>DataFrame</code> <p>Parent dataframe (only length is used here).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Numpy array of integer child counts aligned with parents.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If called before fitting the model.</p> Source code in <code>syntho_hive/relational/linkage.py</code> <pre><code>def sample_counts(self, parent_context: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Sample child counts for a set of parents.\n\n    Args:\n        parent_context: Parent dataframe (only length is used here).\n\n    Returns:\n        Numpy array of integer child counts aligned with parents.\n\n    Raises:\n        ValueError: If called before fitting the model.\n    \"\"\"\n    n_samples = len(parent_context)\n    if self.model is None:\n        raise ValueError(\"LinkageModel not fitted\")\n\n    # Sample from GMM\n    counts, _ = self.model.sample(n_samples)\n\n    # Post-process: Round to nearest int, clip to [0, max]\n    counts = np.round(counts).flatten().astype(int)\n    counts = np.clip(counts, 0, None) # Ensure non-negative\n\n    return counts\n</code></pre>"},{"location":"api/relational/#graph-schema","title":"Graph &amp; Schema","text":""},{"location":"api/relational/#syntho_hive.relational.graph.SchemaGraph","title":"syntho_hive.relational.graph.SchemaGraph","text":"<p>DAG representation of table dependencies derived from metadata.</p> Source code in <code>syntho_hive/relational/graph.py</code> <pre><code>class SchemaGraph:\n    \"\"\"DAG representation of table dependencies derived from metadata.\"\"\"\n    def __init__(self, metadata: Metadata):\n        \"\"\"Create a dependency graph from table metadata.\n\n        Args:\n            metadata: Dataset metadata containing FK relationships.\n        \"\"\"\n        self.metadata = metadata\n        self.adj_list: Dict[str, Set[str]] = {}\n        self._build_graph()\n\n    def _build_graph(self):\n        \"\"\"Build an adjacency list from FK relationships.\"\"\"\n        for table_name in self.metadata.tables:\n            self.adj_list[table_name] = set()\n\n        for table_name, config in self.metadata.tables.items():\n            for ref_col, ref_path in config.fk.items():\n                parent_table, _ = ref_path.split(\".\")\n                # Dependency: Parent -&gt; Child (we generate Parent first)\n                if parent_table in self.adj_list:\n                    self.adj_list[parent_table].add(table_name)\n\n    def get_generation_order(self) -&gt; List[str]:\n        \"\"\"Return a topologically sorted list of tables.\n\n        Returns:\n            List of table names ordered for parent-before-child generation.\n\n        Raises:\n            ValueError: If a cycle is detected in FK relationships.\n        \"\"\"\n        visited = set()\n        stack = []\n        path = set()\n\n        def visit(node):\n            if node in path:\n                raise ValueError(f\"Cycle detected involving {node}\")\n            if node in visited:\n                return\n\n            path.add(node)\n            visited.add(node)\n\n            # Note: For generation order (Parent -&gt; Child), we want to visit parents, then children.\n            # Standard topological sort gives reverse dependency order if edge is Dependency -&gt; Dependent\n            # Here Edge is Parent -&gt; Child. So generic topological sort:\n            # Visit Parent, allow it to finish, add to stack? No.\n            # If A -&gt; B (A is parent of B).\n            # We want [A, B].\n            # Normal DFS topo sort on A -&gt; B puts B on stack, then A. Stack: [A, B] (LIFO) -&gt; Pop A, Pop B. \n            # Yes, standard topological sort on (Parent -&gt; Child) edges returns [Parent, Child].\n\n            for neighbor in self.adj_list.get(node, []):\n                visit(neighbor)\n\n            path.remove(node)\n            stack.append(node)\n\n        # Iterate over all nodes, not just roots, to catch disconnected components\n        # Sort keys for deterministic order\n        for node in sorted(self.adj_list.keys()):\n            visit(node)\n\n        return stack[::-1] # Reverse stack to get topological order\n</code></pre>"},{"location":"api/relational/#syntho_hive.relational.graph.SchemaGraph.get_generation_order","title":"get_generation_order","text":"<pre><code>get_generation_order() -&gt; List[str]\n</code></pre> <p>Return a topologically sorted list of tables.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of table names ordered for parent-before-child generation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a cycle is detected in FK relationships.</p> Source code in <code>syntho_hive/relational/graph.py</code> <pre><code>def get_generation_order(self) -&gt; List[str]:\n    \"\"\"Return a topologically sorted list of tables.\n\n    Returns:\n        List of table names ordered for parent-before-child generation.\n\n    Raises:\n        ValueError: If a cycle is detected in FK relationships.\n    \"\"\"\n    visited = set()\n    stack = []\n    path = set()\n\n    def visit(node):\n        if node in path:\n            raise ValueError(f\"Cycle detected involving {node}\")\n        if node in visited:\n            return\n\n        path.add(node)\n        visited.add(node)\n\n        # Note: For generation order (Parent -&gt; Child), we want to visit parents, then children.\n        # Standard topological sort gives reverse dependency order if edge is Dependency -&gt; Dependent\n        # Here Edge is Parent -&gt; Child. So generic topological sort:\n        # Visit Parent, allow it to finish, add to stack? No.\n        # If A -&gt; B (A is parent of B).\n        # We want [A, B].\n        # Normal DFS topo sort on A -&gt; B puts B on stack, then A. Stack: [A, B] (LIFO) -&gt; Pop A, Pop B. \n        # Yes, standard topological sort on (Parent -&gt; Child) edges returns [Parent, Child].\n\n        for neighbor in self.adj_list.get(node, []):\n            visit(neighbor)\n\n        path.remove(node)\n        stack.append(node)\n\n    # Iterate over all nodes, not just roots, to catch disconnected components\n    # Sort keys for deterministic order\n    for node in sorted(self.adj_list.keys()):\n        visit(node)\n\n    return stack[::-1] # Reverse stack to get topological order\n</code></pre>"},{"location":"api/validation/","title":"Validation","text":""},{"location":"api/validation/#syntho_hive.validation.report_generator.ValidationReport","title":"syntho_hive.validation.report_generator.ValidationReport","text":"<p>Generate summary reports of validation metrics.</p> Source code in <code>syntho_hive/validation/report_generator.py</code> <pre><code>class ValidationReport:\n    \"\"\"Generate summary reports of validation metrics.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize statistical validator and metric store.\"\"\"\n        self.validator = StatisticalValidator()\n        self.metrics = {}\n\n    def _calculate_detailed_stats(self, real_df: pd.DataFrame, synth_df: pd.DataFrame) -&gt; Dict[str, Any]:\n        \"\"\"Calculate descriptive statistics for side-by-side comparison.\n\n        Args:\n            real_df: Real dataframe.\n            synth_df: Synthetic dataframe aligned to the real columns.\n\n        Returns:\n            Nested dict of summary stats for each column.\n        \"\"\"\n        stats = {}\n        for col in real_df.columns:\n            if col not in synth_df.columns:\n                continue\n\n            col_stats = {\"real\": {}, \"synth\": {}}\n\n            for name, df, res in [(\"real\", real_df, col_stats[\"real\"]), (\"synth\", synth_df, col_stats[\"synth\"])]:\n                series = df[col]\n                if pd.api.types.is_numeric_dtype(series):\n                    res[\"mean\"] = series.mean()\n                    res[\"std\"] = series.std()\n                    res[\"min\"] = series.min()\n                    res[\"max\"] = series.max()\n                else:\n                    res[\"unique_count\"] = series.nunique()\n                    res[\"top_value\"] = series.mode().iloc[0] if not series.mode().empty else \"N/A\"\n                    res[\"top_freq\"] = series.value_counts().iloc[0] if not series.empty else 0\n\n            stats[col] = col_stats\n        return stats\n\n    def generate(self, real_data: Dict[str, pd.DataFrame], synth_data: Dict[str, pd.DataFrame], output_path: str):\n        \"\"\"Run validation and save a report.\n\n        Args:\n            real_data: Mapping of table name to real dataframe.\n            synth_data: Mapping of table name to synthetic dataframe.\n            output_path: Destination path for HTML or JSON report.\n        \"\"\"\n        report = {\n            \"tables\": {},\n            \"summary\": \"Validation Report\"\n        }\n\n        for table_name, real_df in real_data.items():\n            if table_name not in synth_data:\n                continue\n\n            synth_df = synth_data[table_name]\n\n            # 1. Column comparisons\n            col_metrics = self.validator.compare_columns(real_df, synth_df)\n\n            # 2. Correlation\n            corr_diff = self.validator.check_correlations(real_df, synth_df)\n\n            # 3. Detailed Stats\n            stats = self._calculate_detailed_stats(real_df, synth_df)\n\n            # 4. Data Preview\n            # Use Pandas to_html for easy formatting, strict constraints\n            preview = {\n                \"real_html\": real_df.head(10).to_html(index=False, classes='scroll-table', border=0),\n                \"synth_html\": synth_df.head(10).to_html(index=False, classes='scroll-table', border=0)\n            }\n\n            report[\"tables\"][table_name] = {\n                \"column_metrics\": col_metrics,\n                \"correlation_distance\": corr_diff,\n                \"detailed_stats\": stats,\n                \"preview\": preview\n            }\n\n        if output_path.endswith(\".html\"):\n            self._save_html(report, output_path)\n        else:\n            # Save to JSON for now (PDF requires more deps)\n            with open(output_path, \"w\") as f:\n                json.dump(report, f, indent=2, default=str)\n\n        import os\n        print(f\"Report saved to {os.path.abspath(output_path)}\")\n\n    def _save_html(self, report: Dict[str, Any], output_path: str):\n        \"\"\"Render a rich HTML report with metric explanations, stats, and previews.\n\n        Args:\n            report: Structured report dictionary produced by ``generate``.\n            output_path: Filesystem path to write the HTML file.\n        \"\"\"\n        html_content = [\n            \"\"\"&lt;html&gt;\n            &lt;head&gt;\n                &lt;style&gt;\n                    body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 20px; background-color: #f9f9f9; color: #333; }\n                    h1, h2, h3 { color: #2c3e50; }\n                    .container { max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }\n\n                    /* Tables */\n                    table { border-collapse: collapse; width: 100%; margin-bottom: 20px; font-size: 14px; }\n                    th, td { border: 1px solid #e1e4e8; padding: 10px; text-align: left; }\n                    th { background-color: #f1f8ff; color: #0366d6; font-weight: 600; }\n                    tr:nth-child(even) { background-color: #f8f9fa; }\n\n                    /* Status Colors */\n                    .pass { color: #28a745; font-weight: bold; }\n                    .fail { color: #dc3545; font-weight: bold; }\n\n                    /* Layout */\n                    .section { margin-top: 40px; border-top: 1px solid #eee; padding-top: 20px; }\n                    .metric-box { background: #f0f4f8; padding: 15px; border-radius: 5px; margin-bottom: 20px; border-left: 5px solid #0366d6; }\n                    .row { display: flex; gap: 20px; }\n                    .col { flex: 1; overflow-x: auto; }\n\n                    /* Tabs/Previews */\n                    .preview-header { font-weight: bold; margin-bottom: 10px; color: #555; }\n                    .scroll-table { max-height: 400px; overflow-y: auto; display: block; }\n                &lt;/style&gt;\n            &lt;/head&gt;\n            &lt;body&gt;\n            &lt;div class=\"container\"&gt;\n                &lt;h1&gt;Validation Report&lt;/h1&gt;\n\n                &lt;div class=\"metric-box\"&gt;\n                    &lt;h3&gt;Metric Explanations&lt;/h3&gt;\n                    &lt;ul&gt;\n                        &lt;li&gt;&lt;strong&gt;KS Test (Kolmogorov-Smirnov):&lt;/strong&gt; Used for continuous numerical columns. Compares the cumulative distribution functions of the real and synthetic data. &lt;br&gt;\n                            &lt;em&gt;Result:&lt;/em&gt; Returns a p-value. If p &gt; 0.05, we fail to reject the null hypothesis (i.e., distributions are likely the same).&lt;/li&gt;\n                        &lt;li&gt;&lt;strong&gt;TVD (Total Variation Distance):&lt;/strong&gt; Used for categorical or discrete columns. Measures the maximum difference between probabilities assigned to the same event by two distributions. &lt;br&gt;\n                            &lt;em&gt;Result:&lt;/em&gt; Value between 0 and 1. Lower is better (0 means identical). We consider &lt; 0.1 as passing.&lt;/li&gt;\n                        &lt;li&gt;&lt;strong&gt;Correlation Distance:&lt;/strong&gt; Measures how well the pairwise correlations between numerical columns are preserved. Calculated as the Frobenius norm of the difference between correlation matrices. &lt;br&gt;\n                            &lt;em&gt;Result:&lt;/em&gt; Lower is better (0 means identical correlation structure).&lt;/li&gt;\n                    &lt;/ul&gt;\n                &lt;/div&gt;\n            \"\"\"]\n\n        for table_name, data in report[\"tables\"].items():\n            html_content.append(f\"&lt;div class='section'&gt;&lt;h2&gt;Table: {table_name}&lt;/h2&gt;\")\n\n            # --- 1. Correlation &amp; Overall ---\n            corr_dist = data.get('correlation_distance', 0.0)\n            html_content.append(f\"&lt;p&gt;&lt;strong&gt;Correlation Distance:&lt;/strong&gt; {corr_dist:.4f}&lt;/p&gt;\")\n\n            # --- 2. Column Metrics ---\n            html_content.append(\"&lt;h3&gt;Column Validation Metrics&lt;/h3&gt;\")\n            html_content.append(\"&lt;table&gt;&lt;tr&gt;&lt;th&gt;Column&lt;/th&gt;&lt;th&gt;Test Type&lt;/th&gt;&lt;th&gt;Statistic&lt;/th&gt;&lt;th&gt;P-Value / Score&lt;/th&gt;&lt;th&gt;Status&lt;/th&gt;&lt;/tr&gt;\")\n\n            for col, metrics in data[\"column_metrics\"].items():\n                if \"error\" in metrics:\n                    html_content.append(f\"&lt;tr&gt;&lt;td&gt;{col}&lt;/td&gt;&lt;td colspan='4' class='fail'&gt;Error: {metrics['error']}&lt;/td&gt;&lt;/tr&gt;\")\n                    continue\n\n                status = \"PASS\" if metrics.get(\"passed\", False) else \"FAIL\"\n                cls = \"pass\" if status == \"PASS\" else \"fail\"\n\n                stat = f\"{metrics.get('statistic', 0):.4f}\"\n                # TVD doesn't have a p-value, KS does.\n                pval = f\"{metrics.get('p_value', 0):.4f}\" if metrics.get('p_value') is not None else \"N/A\"\n                test_name = metrics.get('test', 'N/A')\n\n                html_content.append(f\"&lt;tr&gt;&lt;td&gt;{col}&lt;/td&gt;&lt;td&gt;{test_name}&lt;/td&gt;&lt;td&gt;{stat}&lt;/td&gt;&lt;td&gt;{pval}&lt;/td&gt;&lt;td class='{cls}'&gt;{status}&lt;/td&gt;&lt;/tr&gt;\")\n\n            html_content.append(\"&lt;/table&gt;\")\n\n            # --- 3. Detailed Statistics ---\n            if \"detailed_stats\" in data:\n                html_content.append(\"&lt;h3&gt;Detailed Statistics (Real vs Synthetic)&lt;/h3&gt;\")\n                html_content.append(\"&lt;table&gt;&lt;tr&gt;&lt;th&gt;Column&lt;/th&gt;&lt;th&gt;Metric&lt;/th&gt;&lt;th&gt;Real&lt;/th&gt;&lt;th&gt;Synthetic&lt;/th&gt;&lt;/tr&gt;\")\n\n                for col, stats in data[\"detailed_stats\"].items():\n                    # stats has \"real\": {...}, \"synth\": {...}\n                    real_s = stats.get(\"real\", {})\n                    synth_s = stats.get(\"synth\", {})\n\n                    # Merge keys to show\n                    all_keys = sorted(list(set(real_s.keys()) | set(synth_s.keys())))\n                    # Usually we want mean, std, min, max or unique, top\n\n                    first = True\n                    for k in all_keys:\n                        r_val = real_s.get(k, \"-\")\n                        s_val = synth_s.get(k, \"-\")\n\n                        # Format floats\n                        if isinstance(r_val, (float, np.floating)): r_val = f\"{r_val:.4f}\"\n                        if isinstance(s_val, (float, np.floating)): s_val = f\"{s_val:.4f}\"\n\n                        row_start = f\"&lt;tr&gt;&lt;td rowspan='{len(all_keys)}'&gt;{col}&lt;/td&gt;\" if first else \"&lt;tr&gt;\"\n                        row_end = f\"&lt;td&gt;{k}&lt;/td&gt;&lt;td&gt;{r_val}&lt;/td&gt;&lt;td&gt;{s_val}&lt;/td&gt;&lt;/tr&gt;\"\n                        html_content.append(row_start + row_end)\n                        first = False\n                html_content.append(\"&lt;/table&gt;\")\n\n            # --- 4. Data Preview ---\n            if \"preview\" in data:\n                html_content.append(\"&lt;h3&gt;Data Preview (First 10 Rows)&lt;/h3&gt;\")\n                html_content.append(\"&lt;div class='row'&gt;\")\n\n                # Real\n                html_content.append(\"&lt;div class='col'&gt;\")\n                html_content.append(\"&lt;div class='preview-header'&gt;Original Data (Real)&lt;/div&gt;\")\n                html_content.append(data[\"preview\"][\"real_html\"])\n                html_content.append(\"&lt;/div&gt;\")\n\n                # Synth\n                html_content.append(\"&lt;div class='col'&gt;\")\n                html_content.append(\"&lt;div class='preview-header'&gt;Synthetic Data (Generated)&lt;/div&gt;\")\n                html_content.append(data[\"preview\"][\"synth_html\"])\n                html_content.append(\"&lt;/div&gt;\")\n\n                html_content.append(\"&lt;/div&gt;\") # End row\n\n            html_content.append(\"&lt;/div&gt;\") # End section\n\n        html_content.append(\"&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;\")\n\n        with open(output_path, \"w\") as f:\n            f.write(\"\\n\".join(html_content))\n</code></pre>"},{"location":"api/validation/#syntho_hive.validation.report_generator.ValidationReport.generate","title":"generate","text":"<pre><code>generate(real_data: Dict[str, DataFrame], synth_data: Dict[str, DataFrame], output_path: str)\n</code></pre> <p>Run validation and save a report.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>Dict[str, DataFrame]</code> <p>Mapping of table name to real dataframe.</p> required <code>synth_data</code> <code>Dict[str, DataFrame]</code> <p>Mapping of table name to synthetic dataframe.</p> required <code>output_path</code> <code>str</code> <p>Destination path for HTML or JSON report.</p> required Source code in <code>syntho_hive/validation/report_generator.py</code> <pre><code>def generate(self, real_data: Dict[str, pd.DataFrame], synth_data: Dict[str, pd.DataFrame], output_path: str):\n    \"\"\"Run validation and save a report.\n\n    Args:\n        real_data: Mapping of table name to real dataframe.\n        synth_data: Mapping of table name to synthetic dataframe.\n        output_path: Destination path for HTML or JSON report.\n    \"\"\"\n    report = {\n        \"tables\": {},\n        \"summary\": \"Validation Report\"\n    }\n\n    for table_name, real_df in real_data.items():\n        if table_name not in synth_data:\n            continue\n\n        synth_df = synth_data[table_name]\n\n        # 1. Column comparisons\n        col_metrics = self.validator.compare_columns(real_df, synth_df)\n\n        # 2. Correlation\n        corr_diff = self.validator.check_correlations(real_df, synth_df)\n\n        # 3. Detailed Stats\n        stats = self._calculate_detailed_stats(real_df, synth_df)\n\n        # 4. Data Preview\n        # Use Pandas to_html for easy formatting, strict constraints\n        preview = {\n            \"real_html\": real_df.head(10).to_html(index=False, classes='scroll-table', border=0),\n            \"synth_html\": synth_df.head(10).to_html(index=False, classes='scroll-table', border=0)\n        }\n\n        report[\"tables\"][table_name] = {\n            \"column_metrics\": col_metrics,\n            \"correlation_distance\": corr_diff,\n            \"detailed_stats\": stats,\n            \"preview\": preview\n        }\n\n    if output_path.endswith(\".html\"):\n        self._save_html(report, output_path)\n    else:\n        # Save to JSON for now (PDF requires more deps)\n        with open(output_path, \"w\") as f:\n            json.dump(report, f, indent=2, default=str)\n\n    import os\n    print(f\"Report saved to {os.path.abspath(output_path)}\")\n</code></pre>"},{"location":"api/validation/#syntho_hive.validation.statistical.StatisticalValidator","title":"syntho_hive.validation.statistical.StatisticalValidator","text":"<p>Perform statistical checks between real and synthetic data.</p> Source code in <code>syntho_hive/validation/statistical.py</code> <pre><code>class StatisticalValidator:\n    \"\"\"Perform statistical checks between real and synthetic data.\"\"\"\n\n    def compare_columns(self, real_df: pd.DataFrame, synth_df: pd.DataFrame) -&gt; Dict[str, Any]:\n        \"\"\"Compare column-wise distributions between real and synthetic data.\n\n        Args:\n            real_df: Real dataframe.\n            synth_df: Synthetic dataframe aligned to the same schema.\n\n        Returns:\n            Mapping of column name to test results or error descriptions.\n        \"\"\"\n        results = {}\n\n        if real_df.empty or synth_df.empty:\n            return {\"error\": \"One or both DataFrames are empty.\"}\n\n        for col in real_df.columns:\n            if col not in synth_df.columns:\n                results[col] = {\"error\": \"Column missing in synthetic data\"}\n                continue\n\n            real_data = real_df[col].dropna()\n            synth_data = synth_df[col].dropna()\n\n            if real_data.empty or synth_data.empty:\n                results[col] = {\"error\": \"Column data is empty after dropping NaNs\"}\n                continue\n\n            # Check for type mismatch\n            if real_data.dtype != synth_data.dtype:\n                # Try to cast if compatible (e.g. float vs int)\n                if pd.api.types.is_numeric_dtype(real_data) and pd.api.types.is_numeric_dtype(synth_data):\n                    pass # Compatible enough for stats\n                else:\n                    results[col] = {\"error\": f\"Type mismatch: Real {real_data.dtype} vs Synth {synth_data.dtype}\"}\n                    continue\n\n            if pd.api.types.is_numeric_dtype(real_data):\n                # KS Test\n                try:\n                    stat, p_value = ks_2samp(real_data, synth_data)\n                    results[col] = {\n                        \"test\": \"ks_test\",\n                        \"statistic\": stat,\n                        \"p_value\": p_value,\n                        \"passed\": p_value &gt; 0.05 # Null hypothesis: Same distribution\n                    }\n                except Exception as e:\n                    results[col] = {\"error\": f\"KS Test failed: {str(e)}\"}\n            else:\n                # TVD (Total Variation Distance)\n                try:\n                    real_counts = real_data.value_counts(normalize=True)\n                    synth_counts = synth_data.value_counts(normalize=True)\n\n                    # Align categories\n                    all_cats = set(real_counts.index).union(set(synth_counts.index))\n\n                    tvd = 0.5 * sum(abs(real_counts.get(c, 0) - synth_counts.get(c, 0)) for c in all_cats)\n\n                    results[col] = {\n                        \"test\": \"tvd\",\n                        \"statistic\": tvd,\n                        \"passed\": tvd &lt; 0.1 # Threshold arbitrary\n                    }\n                except Exception as e:\n                    results[col] = {\"error\": f\"TVD Checks failed: {str(e)}\"}\n\n        return results\n\n    def check_correlations(self, real_df: pd.DataFrame, synth_df: pd.DataFrame) -&gt; float:\n        \"\"\"Compare correlation matrices using Frobenius norm.\n\n        Args:\n            real_df: Real dataframe.\n            synth_df: Synthetic dataframe.\n\n        Returns:\n            Frobenius norm distance between correlation matrices (0 when identical).\n        \"\"\"\n        # Numeric only\n        real_corr = real_df.select_dtypes(include=[np.number]).corr().fillna(0)\n        synth_corr = synth_df.select_dtypes(include=[np.number]).corr().fillna(0)\n\n        if real_corr.empty or synth_corr.empty:\n            return 0.0\n\n        diff = real_corr - synth_corr\n        frobenius_norm = np.linalg.norm(diff.values)\n\n        return float(frobenius_norm)\n</code></pre>"},{"location":"api/validation/#syntho_hive.validation.statistical.StatisticalValidator.check_correlations","title":"check_correlations","text":"<pre><code>check_correlations(real_df: DataFrame, synth_df: DataFrame) -&gt; float\n</code></pre> <p>Compare correlation matrices using Frobenius norm.</p> <p>Parameters:</p> Name Type Description Default <code>real_df</code> <code>DataFrame</code> <p>Real dataframe.</p> required <code>synth_df</code> <code>DataFrame</code> <p>Synthetic dataframe.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Frobenius norm distance between correlation matrices (0 when identical).</p> Source code in <code>syntho_hive/validation/statistical.py</code> <pre><code>def check_correlations(self, real_df: pd.DataFrame, synth_df: pd.DataFrame) -&gt; float:\n    \"\"\"Compare correlation matrices using Frobenius norm.\n\n    Args:\n        real_df: Real dataframe.\n        synth_df: Synthetic dataframe.\n\n    Returns:\n        Frobenius norm distance between correlation matrices (0 when identical).\n    \"\"\"\n    # Numeric only\n    real_corr = real_df.select_dtypes(include=[np.number]).corr().fillna(0)\n    synth_corr = synth_df.select_dtypes(include=[np.number]).corr().fillna(0)\n\n    if real_corr.empty or synth_corr.empty:\n        return 0.0\n\n    diff = real_corr - synth_corr\n    frobenius_norm = np.linalg.norm(diff.values)\n\n    return float(frobenius_norm)\n</code></pre>"},{"location":"api/validation/#syntho_hive.validation.statistical.StatisticalValidator.compare_columns","title":"compare_columns","text":"<pre><code>compare_columns(real_df: DataFrame, synth_df: DataFrame) -&gt; Dict[str, Any]\n</code></pre> <p>Compare column-wise distributions between real and synthetic data.</p> <p>Parameters:</p> Name Type Description Default <code>real_df</code> <code>DataFrame</code> <p>Real dataframe.</p> required <code>synth_df</code> <code>DataFrame</code> <p>Synthetic dataframe aligned to the same schema.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Mapping of column name to test results or error descriptions.</p> Source code in <code>syntho_hive/validation/statistical.py</code> <pre><code>def compare_columns(self, real_df: pd.DataFrame, synth_df: pd.DataFrame) -&gt; Dict[str, Any]:\n    \"\"\"Compare column-wise distributions between real and synthetic data.\n\n    Args:\n        real_df: Real dataframe.\n        synth_df: Synthetic dataframe aligned to the same schema.\n\n    Returns:\n        Mapping of column name to test results or error descriptions.\n    \"\"\"\n    results = {}\n\n    if real_df.empty or synth_df.empty:\n        return {\"error\": \"One or both DataFrames are empty.\"}\n\n    for col in real_df.columns:\n        if col not in synth_df.columns:\n            results[col] = {\"error\": \"Column missing in synthetic data\"}\n            continue\n\n        real_data = real_df[col].dropna()\n        synth_data = synth_df[col].dropna()\n\n        if real_data.empty or synth_data.empty:\n            results[col] = {\"error\": \"Column data is empty after dropping NaNs\"}\n            continue\n\n        # Check for type mismatch\n        if real_data.dtype != synth_data.dtype:\n            # Try to cast if compatible (e.g. float vs int)\n            if pd.api.types.is_numeric_dtype(real_data) and pd.api.types.is_numeric_dtype(synth_data):\n                pass # Compatible enough for stats\n            else:\n                results[col] = {\"error\": f\"Type mismatch: Real {real_data.dtype} vs Synth {synth_data.dtype}\"}\n                continue\n\n        if pd.api.types.is_numeric_dtype(real_data):\n            # KS Test\n            try:\n                stat, p_value = ks_2samp(real_data, synth_data)\n                results[col] = {\n                    \"test\": \"ks_test\",\n                    \"statistic\": stat,\n                    \"p_value\": p_value,\n                    \"passed\": p_value &gt; 0.05 # Null hypothesis: Same distribution\n                }\n            except Exception as e:\n                results[col] = {\"error\": f\"KS Test failed: {str(e)}\"}\n        else:\n            # TVD (Total Variation Distance)\n            try:\n                real_counts = real_data.value_counts(normalize=True)\n                synth_counts = synth_data.value_counts(normalize=True)\n\n                # Align categories\n                all_cats = set(real_counts.index).union(set(synth_counts.index))\n\n                tvd = 0.5 * sum(abs(real_counts.get(c, 0) - synth_counts.get(c, 0)) for c in all_cats)\n\n                results[col] = {\n                    \"test\": \"tvd\",\n                    \"statistic\": tvd,\n                    \"passed\": tvd &lt; 0.1 # Threshold arbitrary\n                }\n            except Exception as e:\n                results[col] = {\"error\": f\"TVD Checks failed: {str(e)}\"}\n\n    return results\n</code></pre>"},{"location":"demos/02_privacy_sanitization/","title":"Privacy Sanitization Demo","text":"<p>Path: <code>examples/demos/02_privacy_sanitization</code></p>"},{"location":"demos/02_privacy_sanitization/#goal","title":"Goal","text":"<p>Detect PII, apply sanitization (mask/hash/fake), and compare raw vs sanitized CSV outputs.</p>"},{"location":"demos/02_privacy_sanitization/#run","title":"Run","text":"<pre><code>python examples/demos/02_privacy_sanitization/run.py\n</code></pre>"},{"location":"demos/02_privacy_sanitization/#outputs","title":"Outputs","text":"<ul> <li><code>outputs/raw_users.csv</code></li> <li><code>outputs/sanitized_users.csv</code></li> </ul>"},{"location":"demos/02_privacy_sanitization/#notes","title":"Notes","text":"<ul> <li>Uses <code>PIISanitizer</code> + <code>ContextualFaker</code>.</li> <li>Adjust rules or actions in the demo script to experiment with masking vs hashing vs faking. </li> </ul>"},{"location":"demos/03_validation_report/","title":"Validation Report Demo","text":"<p>Path: <code>examples/demos/03_validation_report</code></p>"},{"location":"demos/03_validation_report/#goal","title":"Goal","text":"<p>Compare real vs synthetic tables, compute KS/TVD metrics, correlation distance, and render an HTML report.</p>"},{"location":"demos/03_validation_report/#run","title":"Run","text":"<pre><code>python examples/demos/03_validation_report/run.py\n</code></pre>"},{"location":"demos/03_validation_report/#outputs","title":"Outputs","text":"<ul> <li><code>outputs/validation_metrics.json</code></li> <li><code>outputs/validation_report.html</code></li> <li>Sample inputs: <code>outputs/real_users.csv</code>, <code>outputs/synthetic_users.csv</code></li> </ul>"},{"location":"demos/03_validation_report/#notes","title":"Notes","text":"<ul> <li>Uses <code>ValidationReport.generate</code> for both JSON and HTML formats.</li> <li>Open the HTML in a browser to inspect column-level results and previews. </li> </ul>"},{"location":"demos/04_relational_linkage_ctgan/","title":"Relational Linkage CTGAN Demo","text":"<p>Path: <code>examples/demos/04_relational_linkage_ctgan</code></p>"},{"location":"demos/04_relational_linkage_ctgan/#goal","title":"Goal","text":"<p>Train relational CTGAN on users/orders with parent-child linkage and generate synthetic tables preserving FKs.</p>"},{"location":"demos/04_relational_linkage_ctgan/#run","title":"Run","text":"<pre><code>python examples/demos/04_relational_linkage_ctgan/run.py\n</code></pre>"},{"location":"demos/04_relational_linkage_ctgan/#outputs","title":"Outputs","text":"<ul> <li><code>outputs/users.csv</code></li> <li><code>outputs/orders.csv</code></li> </ul>"},{"location":"demos/04_relational_linkage_ctgan/#notes","title":"Notes","text":"<ul> <li>Demonstrates <code>StagedOrchestrator</code>, <code>LinkageModel</code>, and conditional CTGAN.</li> <li>Requires Spark/Delta for IO; ensure SparkSession is available. </li> </ul>"},{"location":"demos/05_transformer_embeddings/","title":"Transformer Embeddings Demo","text":"<p>Path: <code>examples/demos/05_transformer_embeddings</code></p>"},{"location":"demos/05_transformer_embeddings/#goal","title":"Goal","text":"<p>Show transformer-based embeddings and recovery of transformed features.</p>"},{"location":"demos/05_transformer_embeddings/#run","title":"Run","text":"<pre><code>python examples/demos/05_transformer_embeddings/run.py\n</code></pre>"},{"location":"demos/05_transformer_embeddings/#outputs","title":"Outputs","text":"<ul> <li><code>outputs/transformed.npy</code></li> <li><code>outputs/recovered.csv</code></li> </ul>"},{"location":"demos/05_transformer_embeddings/#notes","title":"Notes","text":"<ul> <li>Highlights <code>DataTransformer</code> behavior with embedding thresholds.</li> <li>Useful for inspecting how categorical embeddings are produced and inverted. </li> </ul>"},{"location":"demos/overview/","title":"Demo Overview","text":"<p>The <code>examples/demos</code> folder contains runnable scenarios. Ensure dependencies (Spark/Delta for relational pieces) are installed, then run from repo root:</p> <pre><code>pip install -e .[docs]\n</code></pre> <p>Each demo has a <code>run.py</code> script and an <code>outputs/</code> folder with sample artifacts.</p>"},{"location":"demos/overview/#available-demos","title":"Available demos","text":"<ul> <li>02_privacy_sanitization: Detect and sanitize PII, compare raw vs sanitized outputs.</li> <li>03_validation_report: Generate validation metrics and HTML report comparing real vs synthetic.</li> <li>04_relational_linkage_ctgan: Train relational CTGAN with linkage, synthesize users/orders.</li> <li>05_transformer_embeddings: Demonstrate transformer-based embeddings and recovery. </li> </ul>"},{"location":"guides/fitting/","title":"Fitting Models","text":""},{"location":"guides/fitting/#inputs","title":"Inputs","text":"<ul> <li><code>Metadata</code> with tables, PK/FK, constraints, and parent context columns.</li> <li>Optional <code>PrivacyConfig</code> for PII handling.</li> <li>SparkSession (required for full relational flows in current prototype).</li> </ul>"},{"location":"guides/fitting/#minimal-relational-fit","title":"Minimal relational fit","text":"<pre><code>from syntho_hive.interface.config import Metadata, PrivacyConfig\nfrom syntho_hive.interface.synthesizer import Synthesizer\n\nmetadata = Metadata()\nmetadata.add_table(\n    name=\"users\",\n    pk=\"user_id\",\n    fk={},  # root\n    parent_context_cols=[],\n)\nmetadata.add_table(\n    name=\"orders\",\n    pk=\"order_id\",\n    fk={\"user_id\": \"users.user_id\"},\n    parent_context_cols=[\"region\"],\n)\n\nprivacy = PrivacyConfig()\nsynth = Synthesizer(metadata=metadata, privacy_config=privacy, spark_session=spark)\n\nsynth.fit(\n    database=\"my_db\",\n    sampling_strategy=\"relational_stratified\",\n    sample_size=5_000_000,\n    epochs=300,\n    batch_size=500,\n    embedding_threshold=50,\n)\n</code></pre>"},{"location":"guides/fitting/#what-happens-during-fit","title":"What happens during fit","text":"<ol> <li>Each table is read via <code>SparkIO</code>.</li> <li><code>DataTransformer.fit</code> profiles columns (continuous VGM, categorical OHE/embedding).</li> <li><code>LinkageModel.fit</code> learns child counts (per driver parent).</li> <li><code>CTGAN.fit</code> trains generator/discriminator; context columns are merged for conditional learning.</li> </ol>"},{"location":"guides/fitting/#tips","title":"Tips","text":"<ul> <li>Ensure PK/FK are correctly set in <code>Metadata</code> so transformers exclude them from modeling.</li> <li>For high-cardinality categoricals, tune <code>embedding_threshold</code>.</li> <li>Start with smaller <code>epochs</code>/<code>batch_size</code> to validate the flow, then scale up. </li> </ul>"},{"location":"guides/privacy/","title":"Privacy Guardrails","text":"<p>Synthetic data is only useful if it effectively masks sensitive information while retaining utility. SynthoHive applies privacy controls before any finding or training occurs, ensuring that no raw PII ever enters the generative models.</p>"},{"location":"guides/privacy/#the-sanitize-workflow","title":"The Sanitize workflow","text":"<pre><code>graph LR\n    A[Raw Data] --&gt; B{PII Analyzer}\n    B -- Regex/NER --&gt; C[Detected Map]\n    C --&gt; D[Sanitizer Engine]\n    D -- Rules --&gt; E(Clean Data)\n    E --&gt; F[Training]\n</code></pre>"},{"location":"guides/privacy/#detection-strategies","title":"Detection Strategies","text":"<p>We use a combination of heuristics to detect Personal Identifiable Information (PII):</p> <ol> <li>Column Naming: Checks for keywords like \"email\", \"ssn\", \"phone\", \"address\".</li> <li>Pattern Matching: Scans a sample of data against a library of regular expressions (Email, IPv4, Credit Cards, Social Security Numbers).</li> <li>Thresholding: A column is flagged only if $&gt; 50\\%$ of non-null values match a pattern.</li> </ol>"},{"location":"guides/privacy/#sanitization-actions","title":"Sanitization Actions","text":"<p>Once PII is detected (or manually configured), you can apply one of several actions:</p> Action Description Use Case <code>drop</code> Removes the column entirely. High-risk fields with zero utility (e.g., internal system IDs). <code>mask</code> Replaces all but the last 4 characters with <code>*</code>. Credit cards, phone numbers where visual format matters. <code>hash</code> Replaces value with SHA-256 hash. Maintaining distinctness for joining without revealing value (e.g., User IDs). <code>fake</code> Replaces with realistic fake data. Names, Addresses, Emails that need to look \"real\" for the model."},{"location":"guides/privacy/#context-aware-faking","title":"Context-Aware Faking","text":"<p>Standard fakers generate random data. Contextual Faking ensures consistency.</p> <p>Example: If a record has <code>Country=\"US\"</code>, the sanitizer will generate a US-formatted phone number, not a UK one. This preserves the statistical structure of the data even in faked columns.</p>"},{"location":"guides/privacy/#configuration","title":"Configuration","text":"<p>You can customize rules via the <code>PrivacyConfig</code> object:</p> <pre><code>from syntho_hive.interface.config import PrivacyConfig\nfrom syntho_hive.privacy.sanitizer import PiiRule\n\nconfig = PrivacyConfig(rules=[\n    PiiRule(name=\"custom_code\", patterns=[r\"^[A-Z]{2}-\\d{4}$\"], action=\"mask\")\n])\n</code></pre>"},{"location":"guides/relational/","title":"Relational Data Generation","text":"<p>SynthoHive specializes in maintaining referential integrity and statistical correlations across multiple tables. This guide explains the core concepts behind our orchestration engine.</p>"},{"location":"guides/relational/#the-driver-parent-concept","title":"The \"Driver Parent\" Concept","text":"<p>In a complex schema, a child table might refer to multiple parent tables. For example, an <code>Orders</code> table might refer to both <code>Users</code> and <code>Products</code>.</p> <p>When generating a synthetic <code>Order</code>: 1.  Which parent dictates existence? We treat one foreign key relationship as the Driver. Usually, this is the entity that \"owns\" the record (e.g., <code>User</code>). 2.  How many records? We use a <code>LinkageModel</code> to learn the distribution of child records per driver parent (e.g., \"Users typically have 0-5 orders\").</p>"},{"location":"guides/relational/#secondary-parents","title":"Secondary Parents","text":"<p>Other foreign keys (e.g., <code>Product</code>) are treated as Secondary. These are assigned to ensure referential integrity, but they do not drive the count of generated records.</p>"},{"location":"guides/relational/#contextual-conditioning","title":"Contextual Conditioning","text":"<p>To preserve correlations across tables (e.g., \"Users in NY order Winter Coats\"), we use Conditional Generation.</p> <ol> <li>Fit Phase: We join relevant columns from the Driver Parent (e.g., <code>User.City</code>) to the Child Table.</li> <li>Training: The CTGAN model learns not just $P(Order)$, but $P(Order | User.City)$.</li> <li>Generation Phase:<ul> <li>We generate a synthetic User: <code>{ID: 1, City: \"NY\"}</code>.</li> <li>The <code>LinkageModel</code> says \"Generate 3 orders for User 1\".</li> <li>We pass <code>City=\"NY\"</code> as context to the Order Generator.</li> <li>The generator produces orders statistically likely for a NY user.</li> </ul> </li> </ol>"},{"location":"guides/relational/#the-orchestration-flow","title":"The Orchestration Flow","text":"<ol> <li>Schema Analysis: Construct a Directed Acyclic Graph (DAG) of the schema.</li> <li>Topological Sort: Determine generation order (Parents -&gt; Children).</li> <li>Root Generation: Generate independent root tables using standard CTGAN.</li> <li>Child Loop:<ul> <li>Load synthetic parent data.</li> <li>Sample child counts for each parent row.</li> <li>Repeat parent IDs and Context attributes.</li> <li>Generate child rows conditioned on repeated context.</li> <li>Sample valid FKs for secondary parents.</li> <li>Assign Primary Keys.</li> </ul> </li> </ol>"},{"location":"guides/sampling/","title":"Sampling &amp; Relational Generation","text":""},{"location":"guides/sampling/#root-tables","title":"Root tables","text":"<ul> <li><code>CTGAN.sample</code> generates <code>num_rows</code> rows.</li> <li>PKs are assigned sequentially after generation.</li> </ul>"},{"location":"guides/sampling/#child-tables","title":"Child tables","text":"<ol> <li><code>LinkageModel.sample_counts</code> predicts child counts per parent.</li> <li>Parent PKs are repeated based on sampled counts to form FKs.</li> <li>Optional parent context columns are repeated for conditional sampling.</li> <li>Secondary FKs (multiple parents) are filled by random selection from respective parent PKs.</li> </ol>"},{"location":"guides/sampling/#code-sketch-after-fit","title":"Code sketch (after fit)","text":"<pre><code>num_rows = {\"users\": 1000, \"orders\": 4000}\noutput_paths = synth.sample(num_rows=num_rows, output_format=\"delta\")\n# Returns mapping of table -&gt; output path\n</code></pre>"},{"location":"guides/sampling/#stratified-sampling-from-source","title":"Stratified sampling from source","text":"<ul> <li><code>RelationalSampler</code> can downsample roots and cascade to children while keeping distribution via semijoins.</li> <li>Configure <code>sample_size</code> and optional <code>stratify_by</code> to balance classes. </li> </ul>"},{"location":"guides/validation/","title":"Quality Validation","text":"<p>How do you know your synthetic data is good? SynthoHive provides a comprehensive <code>ValidationReport</code> to quantify fidelity.</p>"},{"location":"guides/validation/#metrics","title":"Metrics","text":""},{"location":"guides/validation/#1-kolmogorov-smirnov-ks-test","title":"1. Kolmogorov-Smirnov (KS) Test","text":"<ul> <li>Target: Continuous Columns (Float/Int)</li> <li>Measure: The maximum distance between the cumulative distribution functions (CDFs) of real and synthetic data.</li> <li>Interpretation:<ul> <li><code>0.0</code> = Perfect fit (Distributions are identical).</li> <li><code>1.0</code> = Totally different.</li> <li>Typically, $&lt; 0.1$ is considered excellent quality.</li> </ul> </li> </ul>"},{"location":"guides/validation/#2-total-variation-distance-tvd","title":"2. Total Variation Distance (TVD)","text":"<ul> <li>Target: Categorical Columns</li> <li>Measure: Half the sum of absolute differences between category probabilities.</li> <li>Interpretation:<ul> <li><code>0.0</code> = Perfect fit (Category frequencies match exactly).</li> <li><code>1.0</code> = Totally different.</li> </ul> </li> </ul>"},{"location":"guides/validation/#3-correlation-distance","title":"3. Correlation Distance","text":"<ul> <li>Target: Column Pairs</li> <li>Measure: We compute correlation matrices (Pearson for continuous, Theil's U for categorical) for both Real and Synthetic datasets. The score is the L2 norm of the difference matrix.</li> <li>Goal: Measures how well the model captured relationships between columns (e.g., Age vs. Income).</li> </ul>"},{"location":"guides/validation/#the-html-report","title":"The HTML Report","text":"<p>The <code>ValidationReport.generate()</code> method produces a self-contained HTML file containing:</p> <ol> <li>Summary Score: Aggregate utility score (0-100%).</li> <li>Column Distributions: Overlay plots (Histogram/KDE) for every column.</li> <li>Correlation Heatmaps: Side-by-side comparison of Real vs. Synthetic associations.</li> <li>Row Previews: Snippets of raw data to verify formatting.</li> </ol>"},{"location":"guides/validation/#usage","title":"Usage","text":"<pre><code>from syntho_hive.validation.report_generator import ValidationReport\n\nreport = ValidationReport()\nreport.generate(\n    real_data=real_dfs,      # Dict[str, pd.DataFrame]\n    synth_data=synth_dfs,    # Dict[str, pd.DataFrame]\n    output_path=\"report.html\"\n)\n</code></pre>"},{"location":"reference/config-examples/","title":"Config Examples","text":""},{"location":"reference/config-examples/#simple-single-table-metadata","title":"Simple single-table metadata","text":"<pre><code>from syntho_hive.interface.config import Metadata\n\nmetadata = Metadata()\nmetadata.add_table(\n    name=\"customers\",\n    pk=\"customer_id\",\n    fk={},\n    parent_context_cols=[],\n    constraints={}\n)\n</code></pre>"},{"location":"reference/config-examples/#relational-metadata-with-constraints","title":"Relational metadata with constraints","text":"<pre><code>metadata.add_table(\n    name=\"users\",\n    pk=\"user_id\",\n    fk={},\n    constraints={\n        \"age\": {\"dtype\": \"int\", \"min\": 18, \"max\": 99},\n    },\n    parent_context_cols=[]\n)\n\nmetadata.add_table(\n    name=\"orders\",\n    pk=\"order_id\",\n    fk={\"user_id\": \"users.user_id\"},\n    parent_context_cols=[\"region\"],\n    constraints={\"amount\": {\"dtype\": \"float\", \"min\": 0.0}}\n)\n</code></pre>"},{"location":"reference/config-examples/#privacy-config","title":"Privacy config","text":"<pre><code>from syntho_hive.interface.config import PrivacyConfig\n\nprivacy = PrivacyConfig(\n    enable_differential_privacy=False,\n    epsilon=1.0,\n    pii_strategy=\"context_aware_faker\",\n    k_anonymity_threshold=5,\n    pii_columns=[\"email\", \"phone\"]\n)\n</code></pre>"},{"location":"reference/troubleshooting/","title":"Troubleshooting","text":"<ul> <li>Spark not found: Ensure <code>pyspark</code> is installed and <code>SPARK_HOME</code> is set. For local-only runs, some demos may be limited.</li> <li>Delta support: Install <code>delta-spark</code> and use Spark 3.2+.</li> <li>GPU vs CPU: CTGAN runs on CPU by default; set <code>device=\"cuda\"</code> when available.</li> <li>High-cardinality categoricals: Increase <code>embedding_threshold</code> to use embeddings instead of OHE.</li> <li>Validation failures: Inspect KS/TVD results; large TVD often means categorical imbalance\u2014check sampling strategy or increase training epochs.</li> <li>Doc build errors: Run <code>pip install -e .[docs]</code>; ensure <code>mkdocs</code> and <code>mkdocstrings</code> are installed. </li> </ul>"}]}