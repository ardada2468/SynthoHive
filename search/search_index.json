{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SynthoHive Documentation","text":"<p>SynthoHive is a synthetic data engine that preserves relational integrity while offering privacy safeguards and validation tooling. These docs cover core concepts, guided workflows, demos, and API reference.</p>"},{"location":"#what-youll-find","title":"What you'll find","text":"<ul> <li>Quick start for installing and running demos.</li> <li>Concept pages for architecture and data flow.</li> <li>Guides for fitting, sampling, privacy, and validation.</li> <li>Demo walk-throughs mirroring the <code>examples/demos</code> folder.</li> <li>API reference generated from in-code docstrings via <code>mkdocstrings</code>.</li> </ul>"},{"location":"#at-a-glance","title":"At a glance","text":"<ol> <li>Install docs deps: <code>pip install .[docs]</code></li> <li>Preview docs locally: <code>mkdocs serve</code></li> <li>Build/deploy (GitHub Pages): <code>mkdocs gh-deploy --force</code></li> </ol>"},{"location":"architecture/","title":"Architecture","text":"<p>SynthoHive is organized into clear packages:</p> <ul> <li>interface: <code>Synthesizer</code> fa\u00e7ade, <code>Metadata</code>, <code>TableConfig</code>, <code>PrivacyConfig</code> entry points.</li> <li>core: <code>DataTransformer</code> for normalization/encoding, and <code>CTGAN</code> (Conditional WGAN-GP) for deep generative modeling. See API.</li> <li>relational: <code>StagedOrchestrator</code> managing the generation DAG, and <code>LinkageModel</code> for parent-child cardinality learning. See API.</li> <li>privacy: <code>PIISanitizer</code> with regex-based detection, and <code>ContextualFaker</code> for locale-aware obfuscation. See API.</li> <li>validation: <code>ValidationReport</code> and <code>StatisticalValidator</code> measuring KS/TVD metrics. See API.</li> <li>connectors: <code>SparkIO</code> for scalable I/O.</li> </ul>"},{"location":"architecture/#key-flows","title":"Key flows","text":"<ol> <li>Fit: transformers profile tables, CTGAN trains (optionally conditioned on parent context), linkage models learn child counts.</li> <li>Sample: generators produce rows, linkage models drive child counts, referential integrity enforced via FK assignment.</li> <li>Privacy: sanitizer detects/masks/fakes PII; contextual faker injects locale-aware replacements.</li> <li>Validation: KS/TVD per column, correlation distance, preview tables, HTML/JSON report.</li> </ol> <p>See Data Flow for a stepwise diagram and Guides for hands-on steps. </p>"},{"location":"changelog/","title":"Changelog","text":"<ul> <li>1.2.3</li> <li>Fix <code>TypeError</code> in <code>DataTransformer</code> when applying numeric constraints (min, max, dtype) to categorical/string columns.</li> <li> <p>Added robust type coercion to ensure constraints are applied correctly to transformed data.</p> </li> <li> <p>1.2.2</p> </li> <li>Fix CTGAN embedding cardinality to avoid IndexError when using high-cardinality categorical columns.</li> <li>Databricks example returns in-memory DataFrames and cleans timestamps/nulls for safer Arrow/pandas conversion.</li> <li>Initial MkDocs site scaffold with guides, demos, and API reference.</li> </ul>"},{"location":"data-flow/","title":"Data Flow","text":"<pre><code>flowchart TD\n    A[\"Real tables\"] --&gt; B[\"DataTransformer fit/transform\"]\n    B --&gt; C[\"CTGAN training\"]\n    A --&gt; D[\"LinkageModel (child counts)\"]\n    C --&gt; E[\"Generator\"]\n    D --&gt; E\n    E --&gt; F[\"Inverse transform\"]\n    F --&gt; G[\"Privacy sanitizer (optional)\"]\n    G --&gt; H[\"Validation report\"]</code></pre>"},{"location":"data-flow/#steps","title":"Steps","text":"<ol> <li>Transform: <code>DataTransformer.fit/transform</code> profiles each column (continuous via VGM, categorical via OHE or embeddings) and excludes PK/FK where configured.</li> <li>Train: <code>CTGAN.fit</code> learns distributions; conditional context from parent tables can be merged before fitting.</li> <li>Linkage: <code>LinkageModel.fit</code> learns child-row cardinalities from FK counts.</li> <li>Sample: <code>CTGAN.sample</code> generates rows; linkage drives child counts; FKs are assigned to maintain integrity.</li> <li>Inverse: transformer rebuilds the original schema; constraints (clip/round) are applied.</li> <li>Privacy: <code>PIISanitizer</code> masks/hashes/fakes PII; <code>ContextualFaker</code> injects locale-aware values.</li> <li>Validate: <code>ValidationReport</code> compares distributions (KS/TVD), correlations, and provides previews.</li> </ol>"},{"location":"getting-started/","title":"Getting Started","text":"<p>SynthoHive is a production-grade synthetic data engine that generates high-fidelity relational data while ensuring privacy compliance.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>SynthoHive requires Python 3.9+ and PySpark.</p> <ul> <li>Installation:   <pre><code>pip install synthohive pyspark pandas pyarrow\n</code></pre></li> </ul>"},{"location":"getting-started/#quick-start-single-script","title":"Quick Start: Single Script","text":"<p>You can run this entire workflow in a single file. Save the code below as <code>quickstart.py</code> and run it with <code>python quickstart.py</code>.</p> <pre><code>import os\nimport shutil\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nfrom syntho_hive.interface.config import Metadata, PrivacyConfig\nfrom syntho_hive.interface.synthesizer import Synthesizer\n\n# --- 1. SETUP SPARKSESSION ---\n# Initialize a local Spark session. In production, this would connect to your cluster.\nspark = SparkSession.builder \\\n    .appName(\"SynthoHive_QuickStart\") \\\n    .master(\"local[1]\") \\\n    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n    .getOrCreate()\n\n# --- 2. CREATE DUMMY DATA ---\n# For this demo, we create a small dataset and save it to disk. \n# SynthoHive normally reads from data lakes (Delta/Parquet) or Hive tables.\ndata_dir = \"./quickstart_data\"\nif os.path.exists(data_dir):\n    shutil.rmtree(data_dir)\nos.makedirs(data_dir)\n\nraw_data = pd.DataFrame({\n    \"user_id\": range(1, 101),\n    \"age\": [20, 30, 40, 50] * 25,\n    \"city\": [\"New York\", \"London\", \"Tokyo\", \"Paris\"] * 25,\n    \"income\": [50000.0, 60000.0, 75000.0, 90000.0] * 25\n})\n\ninput_path = f\"{data_dir}/users_input.parquet\"\nraw_data.to_parquet(input_path)\nprint(f\"\u2705 Dummy data created at {input_path}\")\n\n# --- 3. DEFINE METADATA ---\n# Tell SynthoHive about the schema.\n# We explicitly mark the Primary Key (pk).\nmetadata = Metadata()\nmetadata.add_table(\"users\", pk=\"user_id\")\n\n# --- 4. CONFIGURE &amp; TRAIN ---\n# PrivacyConfig controls sanitization (e.g. masking PII), default is safe.\nprivacy = PrivacyConfig()\n\n# Initialize the Synthesizer\n# It acts as the coordinator for reading data, privacy enforcement, and training.\nsynth = Synthesizer(\n    metadata=metadata,\n    privacy_config=privacy,\n    spark_session=spark\n)\n\n# Fit the model\n# We point 'users' to the parquet file we just made.\nprint(\"\ud83d\ude80 Training model...\")\nsynth.fit(\n    data={\"users\": input_path}, \n    epochs=10,        # Use 300+ for production quality\n    batch_size=50\n)\n\n# --- 5. GENERATE DATA ---\n# Sample new synthetic records from the learned distribution.\nprint(\"\u2728 Generating data...\")\noutput_base_path = f\"{data_dir}/output\"\noutput_paths = synth.sample(\n    num_rows={\"users\": 50},\n    output_path=output_base_path,\n    output_format=\"parquet\"\n)\n\n# --- 6. INSPECT RESULTS ---\n# output_paths is a dict mapping table name to the output directory\nsynth_df = pd.read_parquet(output_paths[\"users\"])\nprint(f\"\\n\ud83d\udcca Generated {len(synth_df)} synthetic records:\")\nprint(synth_df.head())\n\n# Clean up\ntry:\n    spark.stop()\nexcept:\n    pass\n</code></pre>"},{"location":"getting-started/#explanation","title":"Explanation","text":"<p>Here is what is happening in the script above:</p>"},{"location":"getting-started/#1-setup-spark","title":"1. Setup Spark","text":"<p>SynthoHive relies on PySpark for scalable data processing. In this example, we create a local session (<code>local[1]</code>) so you can run it on your laptop without a cluster.</p>"},{"location":"getting-started/#2-define-metadata","title":"2. Define Metadata","text":"<p>Use the <code>Metadata</code> object to define your schema. - <code>add_table</code>: Registers a table. - <code>pk</code>: Specifies the Primary Key. SynthoHive ensures this is unique in generated data.</p>"},{"location":"getting-started/#3-initialize-synthesizer","title":"3. Initialize Synthesizer","text":"<p>The <code>Synthesizer</code> class is the main entry point. It takes your metadata and privacy config and orchestrates the entire pipeline. - <code>privacy_config</code>: Used to define PII columns and anonymization strategies (e.g., masking emails).</p>"},{"location":"getting-started/#3-missing-data","title":"3. Missing Data","text":"<p>SynthoHive automatically handles missing values (<code>NaN</code>, <code>None</code>) in your dataset.  - Continuous columns: Missing values are modeled using a null indicator. - Categorical columns: Missing values are treated as a distinct category. No manual imputation is required before training.</p>"},{"location":"getting-started/#4-training-ctgan","title":"4. Training (CTGAN)","text":"<p>The <code>fit</code> method learns the statistical distribution of your real data. - <code>data</code>: A dictionary checking table names to their file paths (or Hive table names). - <code>epochs</code>: Low (10) for this demo, but should be higher (300-500) for high-fidelity results. - <code>checkpoint_dir</code>: (Optional) Directory to save the best model and training metrics.</p>"},{"location":"getting-started/#5-generate-sample","title":"5. Generate (<code>sample</code>)","text":"<p>The <code>sample</code> method creates new data based on the trained model. - <code>num_rows</code>: How many records you want. - <code>output_path</code>: Where to save the data. If omitted (set to <code>None</code>), it returns the generated DataFrames directly in memory!</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you've generated your first table, explore more advanced features:</p> <ul> <li>Relational Data: Learn how to model complex schemas with Foreign Keys.</li> <li>Privacy Guardrails: Configure PII detection and sanitization.</li> <li>Validation Reports: Generate HTML reports proving the quality of your data.</li> </ul>"},{"location":"api/connectors/","title":"Connectors","text":""},{"location":"api/connectors/#syntho_hive.connectors.spark_io.SparkIO","title":"syntho_hive.connectors.spark_io.SparkIO","text":"<p>Utility for reading and writing datasets via Spark and Delta Lake.</p> Source code in <code>syntho_hive/connectors/spark_io.py</code> <pre><code>class SparkIO:\n    \"\"\"Utility for reading and writing datasets via Spark and Delta Lake.\"\"\"\n    def __init__(self, spark: SparkSession):\n        \"\"\"Initialize the IO helper.\n\n        Args:\n            spark: Active SparkSession used for all IO.\n        \"\"\"\n        self.spark = spark\n\n    def read_dataset(self, path_or_table: str, format: str = None, **kwargs: Union[str, int, bool, float]) -&gt; DataFrame:\n        \"\"\"Read a dataset from a table name or filesystem path.\n\n        Args:\n            path_or_table: Hive table name or filesystem/URI path.\n            format: Optional explicit format override (e.g., ``\"csv\"``).\n            **kwargs: Additional Spark read options.\n\n        Returns:\n            Spark DataFrame loaded from the specified source.\n        \"\"\"\n        # Simple heuristic\n        if \"/\" in path_or_table or \"\\\\\" in path_or_table or path_or_table.startswith(\"file://\"):\n            if format:\n                return self.spark.read.format(format).load(path_or_table, **kwargs)\n\n            if path_or_table.endswith(\".csv\"):\n                return self.spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\", \"true\").load(path_or_table, **kwargs)\n            elif path_or_table.endswith(\".parquet\"):\n                return self.spark.read.format(\"parquet\").load(path_or_table, **kwargs)\n            else:\n                # Default to parquet for directories/tables (matching write default)\n                return self.spark.read.format(\"parquet\").load(path_or_table, **kwargs)\n        return self.spark.table(path_or_table)\n\n    def write_dataset(self, df: DataFrame, target_path: str, mode: str = \"overwrite\", partition_by: Optional[str] = None, format: str = \"parquet\"):\n        \"\"\"Write a Spark DataFrame to storage.\n\n        Args:\n            df: Spark DataFrame to persist.\n            target_path: Output path (directory or table location).\n            mode: Save mode, e.g., ``\"overwrite\"`` or ``\"append\"``.\n            partition_by: Optional column name to partition by.\n            format: Output format, defaults to ``\"parquet\"``.\n        \"\"\"\n        writer = df.write.format(format).mode(mode)\n        if partition_by:\n            writer = writer.partitionBy(partition_by)\n        writer.save(target_path)\n\n    def write_pandas(self, pdf: pd.DataFrame, target_path: str, mode: str = \"append\", format: str = \"parquet\"):\n        \"\"\"Write a Pandas DataFrame using Spark-backed persistence.\n\n        Args:\n            pdf: Pandas DataFrame to persist.\n            target_path: Output path for the written dataset.\n            mode: Save mode for Spark writer (default ``\"append\"``).\n            format: Storage format, defaults to ``\"parquet\"``.\n        \"\"\"\n        sdf = self.spark.createDataFrame(pdf)\n        self.write_dataset(sdf, target_path, mode=mode, format=format)\n</code></pre>"},{"location":"api/connectors/#syntho_hive.connectors.spark_io.SparkIO.read_dataset","title":"read_dataset","text":"<pre><code>read_dataset(path_or_table: str, format: str = None, **kwargs: Union[str, int, bool, float]) -&gt; DataFrame\n</code></pre> <p>Read a dataset from a table name or filesystem path.</p> <p>Parameters:</p> Name Type Description Default <code>path_or_table</code> <code>str</code> <p>Hive table name or filesystem/URI path.</p> required <code>format</code> <code>str</code> <p>Optional explicit format override (e.g., <code>\"csv\"</code>).</p> <code>None</code> <code>**kwargs</code> <code>Union[str, int, bool, float]</code> <p>Additional Spark read options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Spark DataFrame loaded from the specified source.</p> Source code in <code>syntho_hive/connectors/spark_io.py</code> <pre><code>def read_dataset(self, path_or_table: str, format: str = None, **kwargs: Union[str, int, bool, float]) -&gt; DataFrame:\n    \"\"\"Read a dataset from a table name or filesystem path.\n\n    Args:\n        path_or_table: Hive table name or filesystem/URI path.\n        format: Optional explicit format override (e.g., ``\"csv\"``).\n        **kwargs: Additional Spark read options.\n\n    Returns:\n        Spark DataFrame loaded from the specified source.\n    \"\"\"\n    # Simple heuristic\n    if \"/\" in path_or_table or \"\\\\\" in path_or_table or path_or_table.startswith(\"file://\"):\n        if format:\n            return self.spark.read.format(format).load(path_or_table, **kwargs)\n\n        if path_or_table.endswith(\".csv\"):\n            return self.spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\", \"true\").load(path_or_table, **kwargs)\n        elif path_or_table.endswith(\".parquet\"):\n            return self.spark.read.format(\"parquet\").load(path_or_table, **kwargs)\n        else:\n            # Default to parquet for directories/tables (matching write default)\n            return self.spark.read.format(\"parquet\").load(path_or_table, **kwargs)\n    return self.spark.table(path_or_table)\n</code></pre>"},{"location":"api/connectors/#syntho_hive.connectors.spark_io.SparkIO.write_dataset","title":"write_dataset","text":"<pre><code>write_dataset(df: DataFrame, target_path: str, mode: str = 'overwrite', partition_by: Optional[str] = None, format: str = 'parquet')\n</code></pre> <p>Write a Spark DataFrame to storage.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame to persist.</p> required <code>target_path</code> <code>str</code> <p>Output path (directory or table location).</p> required <code>mode</code> <code>str</code> <p>Save mode, e.g., <code>\"overwrite\"</code> or <code>\"append\"</code>.</p> <code>'overwrite'</code> <code>partition_by</code> <code>Optional[str]</code> <p>Optional column name to partition by.</p> <code>None</code> <code>format</code> <code>str</code> <p>Output format, defaults to <code>\"parquet\"</code>.</p> <code>'parquet'</code> Source code in <code>syntho_hive/connectors/spark_io.py</code> <pre><code>def write_dataset(self, df: DataFrame, target_path: str, mode: str = \"overwrite\", partition_by: Optional[str] = None, format: str = \"parquet\"):\n    \"\"\"Write a Spark DataFrame to storage.\n\n    Args:\n        df: Spark DataFrame to persist.\n        target_path: Output path (directory or table location).\n        mode: Save mode, e.g., ``\"overwrite\"`` or ``\"append\"``.\n        partition_by: Optional column name to partition by.\n        format: Output format, defaults to ``\"parquet\"``.\n    \"\"\"\n    writer = df.write.format(format).mode(mode)\n    if partition_by:\n        writer = writer.partitionBy(partition_by)\n    writer.save(target_path)\n</code></pre>"},{"location":"api/connectors/#syntho_hive.connectors.spark_io.SparkIO.write_pandas","title":"write_pandas","text":"<pre><code>write_pandas(pdf: DataFrame, target_path: str, mode: str = 'append', format: str = 'parquet')\n</code></pre> <p>Write a Pandas DataFrame using Spark-backed persistence.</p> <p>Parameters:</p> Name Type Description Default <code>pdf</code> <code>DataFrame</code> <p>Pandas DataFrame to persist.</p> required <code>target_path</code> <code>str</code> <p>Output path for the written dataset.</p> required <code>mode</code> <code>str</code> <p>Save mode for Spark writer (default <code>\"append\"</code>).</p> <code>'append'</code> <code>format</code> <code>str</code> <p>Storage format, defaults to <code>\"parquet\"</code>.</p> <code>'parquet'</code> Source code in <code>syntho_hive/connectors/spark_io.py</code> <pre><code>def write_pandas(self, pdf: pd.DataFrame, target_path: str, mode: str = \"append\", format: str = \"parquet\"):\n    \"\"\"Write a Pandas DataFrame using Spark-backed persistence.\n\n    Args:\n        pdf: Pandas DataFrame to persist.\n        target_path: Output path for the written dataset.\n        mode: Save mode for Spark writer (default ``\"append\"``).\n        format: Storage format, defaults to ``\"parquet\"``.\n    \"\"\"\n    sdf = self.spark.createDataFrame(pdf)\n    self.write_dataset(sdf, target_path, mode=mode, format=format)\n</code></pre>"},{"location":"api/connectors/#syntho_hive.connectors.sampling.RelationalSampler","title":"syntho_hive.connectors.sampling.RelationalSampler","text":"<p>Relational stratified sampler for parent-child table hierarchies.</p> Source code in <code>syntho_hive/connectors/sampling.py</code> <pre><code>class RelationalSampler:\n    \"\"\"Relational stratified sampler for parent-child table hierarchies.\"\"\"\n\n    def __init__(self, metadata: Metadata, spark: SparkSession):\n        \"\"\"Initialize the sampler.\n\n        Args:\n            metadata: Metadata describing tables and their keys.\n            spark: Active SparkSession for table access.\n        \"\"\"\n        self.metadata = metadata\n        self.spark = spark\n\n    def sample_relational(\n        self, \n        root_table: str, \n        sample_size: int, \n        stratify_by: Optional[str] = None\n    ) -&gt; Dict[str, DataFrame]:\n        \"\"\"Sample a root table and cascade the sample to child tables.\n\n        Args:\n            root_table: Name of the parent/root table to sample.\n            sample_size: Approximate number of rows to retain from the root.\n            stratify_by: Optional column for stratified sampling.\n\n        Returns:\n            Dictionary mapping table name to sampled Spark DataFrame.\n        \"\"\"\n        sampled_data = {}\n\n        # 1. Sample Root\n        print(f\"Sampling root table: {root_table}\")\n        # Placeholder for real table loading\n        root_df = self.spark.table(root_table)\n\n        if stratify_by:\n            # Approximate stratified sampling\n            fractions = root_df.select(stratify_by).distinct().withColumn(\"fraction\", F.lit(0.1)).rdd.collectAsMap() \n            # Note: fractions logic needs to be calculated based on sample_size / total_count\n            sampled_root = root_df.sampleBy(stratify_by, fractions, seed=42)\n        else:\n            fraction = min(1.0, sample_size / root_df.count())\n            sampled_root = root_df.sample(withReplacement=False, fraction=fraction, seed=42)\n\n        sampled_data[root_table] = sampled_root\n\n        # 2. Cascade to Children\n        # Simple BFS or using Graph\n        parent_pk = self.metadata.get_table(root_table).pk\n\n        # Find children\n        for child_name, config in self.metadata.tables.items():\n            for child_col, parent_ref in config.fk.items():\n                if parent_ref.startswith(f\"{root_table}.\"):\n                    print(f\"Cascading sample to child: {child_name}\")\n                    child_df = self.spark.table(child_name)\n\n                    # Semijoin\n                    # Join on PK-FK to keep only rows matching sampled parents\n                    sampled_child = child_df.join(\n                        sampled_root.select(parent_pk),\n                        child_df[child_col] == sampled_root[parent_pk],\n                        \"inner\"\n                    ).select(child_df.columns) # Keep only child cols\n\n                    sampled_data[child_name] = sampled_child\n\n        return sampled_data\n</code></pre>"},{"location":"api/connectors/#syntho_hive.connectors.sampling.RelationalSampler.sample_relational","title":"sample_relational","text":"<pre><code>sample_relational(root_table: str, sample_size: int, stratify_by: Optional[str] = None) -&gt; Dict[str, DataFrame]\n</code></pre> <p>Sample a root table and cascade the sample to child tables.</p> <p>Parameters:</p> Name Type Description Default <code>root_table</code> <code>str</code> <p>Name of the parent/root table to sample.</p> required <code>sample_size</code> <code>int</code> <p>Approximate number of rows to retain from the root.</p> required <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column for stratified sampling.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, DataFrame]</code> <p>Dictionary mapping table name to sampled Spark DataFrame.</p> Source code in <code>syntho_hive/connectors/sampling.py</code> <pre><code>def sample_relational(\n    self, \n    root_table: str, \n    sample_size: int, \n    stratify_by: Optional[str] = None\n) -&gt; Dict[str, DataFrame]:\n    \"\"\"Sample a root table and cascade the sample to child tables.\n\n    Args:\n        root_table: Name of the parent/root table to sample.\n        sample_size: Approximate number of rows to retain from the root.\n        stratify_by: Optional column for stratified sampling.\n\n    Returns:\n        Dictionary mapping table name to sampled Spark DataFrame.\n    \"\"\"\n    sampled_data = {}\n\n    # 1. Sample Root\n    print(f\"Sampling root table: {root_table}\")\n    # Placeholder for real table loading\n    root_df = self.spark.table(root_table)\n\n    if stratify_by:\n        # Approximate stratified sampling\n        fractions = root_df.select(stratify_by).distinct().withColumn(\"fraction\", F.lit(0.1)).rdd.collectAsMap() \n        # Note: fractions logic needs to be calculated based on sample_size / total_count\n        sampled_root = root_df.sampleBy(stratify_by, fractions, seed=42)\n    else:\n        fraction = min(1.0, sample_size / root_df.count())\n        sampled_root = root_df.sample(withReplacement=False, fraction=fraction, seed=42)\n\n    sampled_data[root_table] = sampled_root\n\n    # 2. Cascade to Children\n    # Simple BFS or using Graph\n    parent_pk = self.metadata.get_table(root_table).pk\n\n    # Find children\n    for child_name, config in self.metadata.tables.items():\n        for child_col, parent_ref in config.fk.items():\n            if parent_ref.startswith(f\"{root_table}.\"):\n                print(f\"Cascading sample to child: {child_name}\")\n                child_df = self.spark.table(child_name)\n\n                # Semijoin\n                # Join on PK-FK to keep only rows matching sampled parents\n                sampled_child = child_df.join(\n                    sampled_root.select(parent_pk),\n                    child_df[child_col] == sampled_root[parent_pk],\n                    \"inner\"\n                ).select(child_df.columns) # Keep only child cols\n\n                sampled_data[child_name] = sampled_child\n\n    return sampled_data\n</code></pre>"},{"location":"api/core/","title":"Core Models &amp; Data","text":"<p>The core module contains the deep learning implementations and data transformation logic.</p>"},{"location":"api/core/#models","title":"Models","text":""},{"location":"api/core/#syntho_hive.core.models.ctgan.CTGAN","title":"syntho_hive.core.models.ctgan.CTGAN","text":"<p>               Bases: <code>ConditionalGenerativeModel</code></p> <p>Conditional Tabular GAN with entity embeddings and parent context.</p> Source code in <code>syntho_hive/core/models/ctgan.py</code> <pre><code>class CTGAN(ConditionalGenerativeModel):\n    \"\"\"Conditional Tabular GAN with entity embeddings and parent context.\"\"\"\n    def __init__(\n        self,\n        metadata: Any,\n        embedding_dim: int = 128,\n        generator_dim: Tuple[int, int] = (256, 256),\n        discriminator_dim: Tuple[int, int] = (256, 256),\n        batch_size: int = 500,\n        epochs: int = 300,\n        device: str = \"cpu\",\n        embedding_threshold: int = 50,\n        discriminator_steps: int = 5\n    ):\n        \"\"\"Create a CTGAN instance configured for tabular synthesis.\n\n        Args:\n            metadata: Table metadata describing columns and constraints.\n            embedding_dim: Dimension of input noise vector.\n            generator_dim: Hidden layer widths for the generator.\n            discriminator_dim: Hidden layer widths for the discriminator.\n            batch_size: Training batch size.\n            epochs: Number of training epochs.\n            device: Torch device string, e.g. ``\"cpu\"`` or ``\"cuda\"``.\n            embedding_threshold: Cardinality threshold for switching to embeddings.\n            discriminator_steps: Number of discriminator steps per generator step.\n        \"\"\"\n        self.metadata = metadata\n        self.embedding_dim = embedding_dim\n        self.generator_dim = generator_dim\n        self.discriminator_dim = discriminator_dim\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.epochs = epochs\n        self.device = torch.device(device)\n        self.discriminator_steps = discriminator_steps\n        # Prioritize init arg, fallback to metadata if available, else default (already 50)\n        self.embedding_threshold = embedding_threshold\n\n        self.generator = None\n        self.discriminator = None\n        self.transformer = DataTransformer(metadata, embedding_threshold=self.embedding_threshold)\n        self.context_transformer = DataTransformer(metadata, embedding_threshold=self.embedding_threshold)\n\n        # Embedding Layers\n        self.embedding_layers = nn.ModuleDict()\n        self.data_column_info = [] # List of tuples: (dim, type, related_info)\n\n\n    def _compile_layout(self, transformer):\n        \"\"\"Analyze transformer output to map column indices and types.\n\n        Args:\n            transformer: Fitted ``DataTransformer`` for the child table.\n        \"\"\"\n        self.data_column_info = []\n        self.embedding_layers = nn.ModuleDict()\n\n        current_idx = 0\n        for col, info in transformer._column_info.items():\n            if info['type'] == 'categorical_embedding':\n                # Create Embedding Layer\n                num_categories = info['num_categories']\n                # Heuristic for embedding dimension: min(50, num_categories/2)\n                emb_dim = min(50, (num_categories + 1) // 2)\n\n                self.embedding_layers[col] = EntityEmbeddingLayer(num_categories, emb_dim).to(self.device)\n\n                self.data_column_info.append({\n                    'name': col,\n                    'type': 'embedding', \n                    'input_idx': current_idx, \n                    'input_dim': 1,\n                    'output_dim': emb_dim,\n                    'num_categories': num_categories \n                })\n                current_idx += 1\n            else:\n                self.data_column_info.append({\n                    'name': col,\n                    'type': 'normal',\n                    'input_idx': current_idx,\n                    'input_dim': info['dim'],\n                    'output_dim': info['dim']\n                })\n                current_idx += info['dim']\n\n    def _apply_embeddings(self, data, is_fake=False):\n        \"\"\"Convert a mixed categorical/continuous tensor into embedding space.\n\n        Args:\n            data: Input tensor with mixed column representations.\n            is_fake: Whether the tensor came from the generator (logits) or real data (indices).\n\n        Returns:\n            Tensor with embeddings applied to categorical columns.\n        \"\"\"\n        parts = []\n        for info in self.data_column_info:\n            idx = info['input_idx']\n            dim = info['input_dim']\n            col_data = data[:, idx:idx+dim]\n\n            if info['type'] == 'embedding':\n                layer = self.embedding_layers[info['name']]\n                if is_fake:\n                    # col_data contains Softmax logits from Generator\n                    # Needs hard Gumbel-Softmax or Softmax? Generator outputs unnormalized logits usually.\n                    # Ideally Generator outputs (N, num_cats). \n                    # Wait, 'data' passed here is strictly what Generator produced.\n                    # Discriminator expects (N, EmbDim).\n\n                    # Logic: Generator outputs Logits. We apply Softmax -&gt; Dense.\n                    # But wait, logic above says Generator outputs: \n                    # Embedding: Logits (dim=num_cats)\n                    # Normal: Values (dim=original_dim)\n\n                    # So 'dim' in loop here must match GENERATOR output structure, not Transformer output.\n                    # Compile Layout logic is slightly tricky because Generator output shape != Transformer output shape for Embeddings.\n\n                    # RE-THINK:\n                    # Transformer Output (Real): [Index] (1 dim)\n                    # Generator Output (Fake): [Logits] (num_cats dim)\n\n                    # This function strictly transforms Real Data (Index) -&gt; Embedding.\n                    # Or Fake Data (Logits) -&gt; Soft Embedding.\n\n                    # Problem: input 'data' has different shapes for Real vs Fake.\n                    # We need to handle them separately or have this function assume inputs are already sliced?\n                    # Let's pass sliced inputs or rely on info having both dims.\n                    pass\n                else:\n                    # Real Data: Indices -&gt; Embedding\n                    # input is (N, 1) indices\n                    embeddings = layer(col_data.long().squeeze(1))\n                    parts.append(embeddings)\n            else:\n                parts.append(col_data)\n\n        # Re-implementing clearer separated logic in Build Model / Forward\n        return torch.cat(parts, dim=1)\n\n    def _build_model(self, transformer_output_dim: int, context_dim: int = 0):\n        \"\"\"Instantiate generator and discriminator modules.\n\n        Args:\n            transformer_output_dim: Flattened dimension of transformed child data.\n            context_dim: Flattened dimension of transformed context (if any).\n        \"\"\"\n        # 1. Compile Layout first\n        self._compile_layout(self.transformer)\n\n        # 2. Calculate Generator Output Dim &amp; Discriminator Input Dim\n        gen_output_dim = 0\n        disc_input_dim_base = 0\n\n        for info in self.data_column_info:\n            if info['type'] == 'embedding':\n                gen_output_dim += info['num_categories'] # Generator outputs logits\n                disc_input_dim_base += info['output_dim'] # D sees embeddings\n            else:\n                gen_output_dim += info['output_dim']\n                disc_input_dim_base += info['output_dim']\n\n        # Generator: Noise + Context -&gt; Data (Logits/Values)\n        gen_input_dim = self.embedding_dim + context_dim\n\n        self.generator = nn.Sequential(\n            ResidualLayer(gen_input_dim, self.generator_dim[0]),\n            ResidualLayer(self.generator_dim[0], self.generator_dim[1]),\n            nn.Linear(self.generator_dim[1], gen_output_dim)\n        ).to(self.device)\n\n        # Discriminator: Data(Embeddings) + Context -&gt; Score\n        disc_input_dim = disc_input_dim_base + context_dim\n\n        self.discriminator = Discriminator(disc_input_dim, self.discriminator_dim[0]).to(self.device)\n\n    def fit(self, data: pd.DataFrame, context: Optional[pd.DataFrame] = None, table_name: Optional[str] = None, checkpoint_dir: Optional[str] = None, log_metrics: bool = True, seed: Optional[int] = None, **kwargs: Any) -&gt; None:\n        \"\"\"Train the CTGAN model on tabular data.\n\n        Args:\n            data: Child table data (target) to model.\n            context: Parent attributes to condition on (aligned row-wise).\n            table_name: Table name for metadata lookup and constraint handling.\n            checkpoint_dir: Directory to save checkpoints (best model, metrics). Defaults to None.\n            log_metrics: Whether to save training metrics to a CSV file. Defaults to True.\n            seed: Integer seed for deterministic training. When None, an integer is\n                  auto-generated and logged so the run can be reproduced later.\n            **kwargs: Extra training options (unused placeholder for compatibility).\n        \"\"\"\n        import random as _random\n\n        # Seed handling \u2014 auto-generate when not provided so every run is reproducible.\n        if seed is None:\n            seed = _random.randint(0, 2**31 - 1)\n            log.info(\n                \"training_seed\",\n                seed=seed,\n                message=\"No seed provided \u2014 auto-generated. Log this value to reproduce this run.\",\n            )\n        else:\n            log.info(\"training_seed\", seed=seed)\n\n        _set_seed(seed)\n\n        # 0. Setup Checkpointing\n        if checkpoint_dir:\n            os.makedirs(checkpoint_dir, exist_ok=True)\n\n        best_loss = float('inf')\n        history = []\n        # 1. Fit and Transform Data\n        self.transformer.fit(data, table_name=table_name, seed=seed)\n        train_data = self.transformer.transform(data)\n        train_data = torch.from_numpy(train_data).float().to(self.device)\n\n        # 2. Handle Context\n        if context is not None:\n            assert len(data) == len(context), \"Data and context must have same number of rows\"\n\n            # Use dedicated transformer for context\n            # NOTE: We abuse metdata here slightly. Ideally context comes from a known table (Parent).\n            # But context might be a mix of parent columns. \n            # For fit, we pass table_name=None to fit on just the columns present in context df.\n            self.context_transformer.fit(context)\n            context_transformed = self.context_transformer.transform(context)\n            context_data = torch.from_numpy(context_transformed).float().to(self.device)\n            context_dim = context_data.shape[1]\n        else:\n            context_data = None\n            context_dim = 0\n\n        data_dim = train_data.shape[1]\n\n        # 3. Build Model\n        if self.generator is None:\n            self._build_model(data_dim, context_dim)\n\n        optimizer_G = optim.Adam(self.generator.parameters(), lr=2e-4, betas=(0.5, 0.9))\n        optimizer_D = optim.Adam(self.discriminator.parameters(), lr=2e-4, betas=(0.5, 0.9))\n\n        # 4. Training Loop (WGAN-GP)\n        steps_per_epoch = max(len(train_data) // self.batch_size, 1)\n\n        for epoch in range(self.epochs):\n            for i in range(steps_per_epoch):\n                # --- Train Discriminator ---\n                for _ in range(self.discriminator_steps):\n                    # Sample real data\n                    idx = np.random.randint(0, len(train_data), self.batch_size)\n                    real_data_batch = train_data[idx]\n                    if context_data is not None:\n                        real_context_batch = context_data[idx]\n                        real_input = torch.cat([real_data_batch, real_context_batch], dim=1)\n                    else:\n                        real_context_batch = None\n                        real_input = real_data_batch\n\n                    # Generate fake data\n                    noise = torch.randn(self.batch_size, self.embedding_dim, device=self.device)\n                    if real_context_batch is not None:\n                        gen_input = torch.cat([noise, real_context_batch], dim=1)\n                    else:\n                        gen_input = noise\n\n                    fake_raw = self.generator(gen_input)\n\n                    # Apply Embeddings / Softmax to Fake Data\n                    fake_parts = []\n                    fake_ptr = 0\n                    for info in self.data_column_info:\n                        if info['type'] == 'embedding':\n                            dim = info['num_categories']\n                            logits = fake_raw[:, fake_ptr:fake_ptr+dim]\n                            fake_ptr += dim\n\n                            # Gumbel Softmax or Softmax? WGAN prefers generic softmax for differentiability\n                            # Note: Gumbel Softmax allows hard sampling with gradients.\n                            probs = F.softmax(logits, dim=1)\n                            emb_vect = self.embedding_layers[info['name']].forward_soft(probs)\n                            fake_parts.append(emb_vect)\n                        else:\n                            dim = info['output_dim']\n                            val = fake_raw[:, fake_ptr:fake_ptr+dim]\n                            fake_ptr += dim\n                            fake_parts.append(val)\n\n                    fake_data_batch = torch.cat(fake_parts, dim=1)\n\n                    # Apply Embeddings to Real Data\n                    real_parts = []\n                    real_ptr = 0\n                    # Need to iterate column info again to slice real data correctly\n                    # Real data from transformer is concatenated (Indices, Values...)\n                    for info in self.data_column_info:\n                        dim = info['input_dim'] # 1 for embedding (index)\n                        col_data = real_data_batch[:, real_ptr:real_ptr+dim]\n                        real_ptr += dim\n\n                        if info['type'] == 'embedding':\n                            emb_vect = self.embedding_layers[info['name']](col_data.long().squeeze(1))\n                            real_parts.append(emb_vect)\n                        else:\n                            real_parts.append(col_data)\n\n                    real_data_processed = torch.cat(real_parts, dim=1)\n\n                    if real_context_batch is not None:\n                        fake_input = torch.cat([fake_data_batch, real_context_batch], dim=1)\n                        real_input_processed = torch.cat([real_data_processed, real_context_batch], dim=1)\n                    else:\n                        fake_input = fake_data_batch\n                        real_input_processed = real_data_processed\n\n                    # Compute WGAN loss\n                    d_real = self.discriminator(real_input_processed)\n                    d_fake = self.discriminator(fake_input)\n\n                    # Gradient Penalty\n                    gp = compute_gradient_penalty(self.discriminator, real_input_processed, fake_input, self.device)\n\n                    loss_D = -torch.mean(d_real) + torch.mean(d_fake) + 10.0 * gp\n\n                    optimizer_D.zero_grad()\n                    loss_D.backward()\n                    optimizer_D.step()\n\n                # --- Train Generator ---\n                # Train generator once after n_critic discriminator steps\n                noise = torch.randn(self.batch_size, self.embedding_dim, device=self.device)\n                if real_context_batch is not None:\n                    # Re-sample context for generator training?? \n                    # Ideally yes, but reusing batch is fine for conditional stability\n                    # We'll stick to reusing the last seen batch for simplicity/stability\n                    gen_input = torch.cat([noise, real_context_batch], dim=1)\n                else:\n                    gen_input = noise\n\n                fake_raw = self.generator(gen_input)\n\n                # Apply Embeddings / Softmax (Same logic as above)\n                fake_parts = []\n                fake_ptr = 0\n                for info in self.data_column_info:\n                    if info['type'] == 'embedding':\n                        dim = info['num_categories']\n                        logits = fake_raw[:, fake_ptr:fake_ptr+dim]\n                        fake_ptr += dim\n                        probs = F.softmax(logits, dim=1)\n                        emb_vect = self.embedding_layers[info['name']].forward_soft(probs)\n                        fake_parts.append(emb_vect)\n                    else:\n                        dim = info['output_dim']\n                        val = fake_raw[:, fake_ptr:fake_ptr+dim]\n                        fake_ptr += dim\n                        fake_parts.append(val)\n\n                fake_data_batch = torch.cat(fake_parts, dim=1)\n\n                if real_context_batch is not None:\n                    fake_input = torch.cat([fake_data_batch, real_context_batch], dim=1)\n                else:\n                    fake_input = fake_data_batch\n\n                d_fake = self.discriminator(fake_input)\n                loss_G = -torch.mean(d_fake)\n\n                optimizer_G.zero_grad()\n                loss_G.backward()\n                optimizer_G.step()\n\n            if epoch % 10 == 0:\n                print(f\"Epoch {epoch}: Loss D={loss_D.item():.4f}, Loss G={loss_G.item():.4f}\")\n\n            # --- Checkpointing &amp; Logging ---\n            current_loss_g = loss_G.item()\n            current_loss_d = loss_D.item()\n\n            if log_metrics:\n                history.append({\n                    'epoch': epoch,\n                    'loss_g': current_loss_g,\n                    'loss_d': current_loss_d\n                })\n\n            if checkpoint_dir and current_loss_g &lt; best_loss:\n                best_loss = current_loss_g\n                self.save(os.path.join(checkpoint_dir, \"best_model.pt\"))\n\n        # End of training: Save metrics and last model\n        if checkpoint_dir:\n            self.save(os.path.join(checkpoint_dir, \"last_model.pt\"))\n\n            if log_metrics and history:\n                metrics_path = os.path.join(checkpoint_dir, \"training_metrics.csv\")\n                keys = history[0].keys()\n                with open(metrics_path, 'w', newline='') as f:\n                    dict_writer = csv.DictWriter(f, fieldnames=keys)\n                    dict_writer.writeheader()\n                    dict_writer.writerows(history)\n                print(f\"Training metrics saved to {metrics_path}\")\n\n    def sample(self, num_rows: int, context: Optional[pd.DataFrame] = None, seed: Optional[int] = None, enforce_constraints: bool = False, **kwargs: Any) -&gt; pd.DataFrame:\n        \"\"\"Generate synthetic samples, optionally conditioned on parent context.\n\n        Args:\n            num_rows: Number of rows to generate.\n            context: Optional parent attributes aligned to the requested rows.\n            seed: Optional integer seed for deterministic sampling. Only applied\n                  when provided; no auto-generation (fits and samples may use\n                  independent seeds per CONTEXT.md decision).\n            enforce_constraints: When True, inspects generated rows against column\n                  constraints defined in the table's Metadata config.  Any rows\n                  that violate a min/max constraint are dropped and a structlog\n                  WARNING is emitted listing each violation.  When False (default),\n                  constraint checking is skipped entirely \u2014 this matches the\n                  pre-existing behavior where inverse_transform() already clips\n                  values within each column's defined range.\n            **kwargs: Additional sampling controls (unused placeholder).\n\n        Returns:\n            DataFrame of synthetic rows mapped back to original schema.\n        \"\"\"\n        if seed is not None:\n            _set_seed(seed)\n\n        self.generator.eval()\n        with torch.no_grad():\n            noise = torch.randn(num_rows, self.embedding_dim, device=self.device)\n\n            if context is not None:\n                # Assuming context is provided for exactly num_rows\n                assert len(context) == num_rows\n\n                # Transform context using the fitted context transformer\n                context_transformed = self.context_transformer.transform(context)\n                context_data = torch.from_numpy(context_transformed).float().to(self.device)\n\n                gen_input = torch.cat([noise, context_data], dim=1)\n            else:\n                gen_input = noise\n\n            fake_raw = self.generator(gen_input)\n\n            # Post-process logits to indices for output\n            output_parts = []\n            fake_ptr = 0\n            for info in self.data_column_info:\n                if info['type'] == 'embedding':\n                    dim = info['num_categories']\n                    logits = fake_raw[:, fake_ptr:fake_ptr+dim]\n                    fake_ptr += dim\n\n                    # Argmax to get index\n                    indices = torch.argmax(logits, dim=1, keepdim=True)\n                    output_parts.append(indices.cpu().numpy())\n                else:\n                    dim = info['output_dim']\n                    val = fake_raw[:, fake_ptr:fake_ptr+dim]\n                    fake_ptr += dim\n                    output_parts.append(val.cpu().numpy())\n\n            fake_data_np = np.concatenate(output_parts, axis=1)\n\n        result_df = self.transformer.inverse_transform(fake_data_np)\n\n        # Constraint violation checking (opt-in via enforce_constraints=True).\n        # Note: inverse_transform() already clips values within defined column ranges,\n        # so enforce_constraints=True is primarily useful for post-hoc auditing or\n        # catching any residual violations before returning rows to the caller.\n        if enforce_constraints:\n            table_config = None\n            table_name = getattr(self.transformer, 'table_name', None)\n            if hasattr(self, 'metadata') and table_name:\n                try:\n                    table_config = self.metadata.get_table(table_name)\n                except Exception as exc:\n                    log.warning(\n                        \"constraint_config_lookup_failed\",\n                        table_name=table_name,\n                        error=str(exc),\n                        note=\"Skipping constraint enforcement \u2014 table config could not be retrieved\",\n                    )\n                    table_config = None\n\n            # If the table has constraints defined, scan generated rows.\n            # If no constraints are configured this block is a no-op.\n            if table_config is not None and table_config.constraints:\n                violations = []\n                valid_mask = pd.Series([True] * len(result_df), index=result_df.index)\n\n                for col_name, constraint in table_config.constraints.items():\n                    if col_name not in result_df.columns:\n                        continue\n                    col_data = result_df[col_name]\n\n                    # Check min constraint\n                    min_val = constraint.min\n                    if min_val is not None:\n                        try:\n                            col_numeric = pd.to_numeric(col_data, errors='coerce')\n                            bad = col_numeric &lt; min_val\n                            if bad.any():\n                                observed = col_numeric[bad].min()\n                                violations.append(\n                                    f\"{col_name}: got {observed:.4g} (min={min_val})\"\n                                )\n                                valid_mask &amp;= ~bad\n                        except Exception as exc:\n                            log.warning(\n                                \"constraint_min_check_skipped\",\n                                column=col_name,\n                                error=str(exc),\n                                note=\"Column is non-numeric or comparison failed \u2014 skipping min check\",\n                            )\n\n                    # Check max constraint\n                    max_val = constraint.max\n                    if max_val is not None:\n                        try:\n                            col_numeric = pd.to_numeric(col_data, errors='coerce')\n                            bad = col_numeric &gt; max_val\n                            if bad.any():\n                                observed = col_numeric[bad].max()\n                                violations.append(\n                                    f\"{col_name}: got {observed:.4g} (max={max_val})\"\n                                )\n                                valid_mask &amp;= ~bad\n                        except Exception as exc:\n                            log.warning(\n                                \"constraint_max_check_skipped\",\n                                column=col_name,\n                                error=str(exc),\n                                note=\"Column is non-numeric or comparison failed \u2014 skipping max check\",\n                            )\n\n                if violations:\n                    summary = \"; \".join(violations)\n                    # ROADMAP success criterion 4 and REQUIREMENTS.md QUAL-04 require\n                    # violations to \"raise with the column name and observed value\".\n                    # Use sample(enforce_constraints=False) (the default) if you want\n                    # the previous warn-and-return behavior.\n                    raise ConstraintViolationError(\n                        f\"ConstraintViolationError: {len(violations)} violation(s) found \u2014 \"\n                        f\"{summary}\"\n                    )\n\n        return result_df\n\n    def save(self, path: str, *, overwrite: bool = False) -&gt; None:\n        \"\"\"Persist full model state to a directory checkpoint.\n\n        Saves all components required for a cold load-and-sample without the\n        original training data: network weights, DataTransformer state,\n        context_transformer state, embedding layer weights, column layout, and\n        human-readable metadata.\n\n        The directory contains:\n            - generator.pt \u2014 generator state_dict\n            - discriminator.pt \u2014 discriminator state_dict\n            - transformer.joblib \u2014 fitted DataTransformer for child table\n            - context_transformer.joblib \u2014 fitted DataTransformer for context\n            - embedding_layers.joblib \u2014 nn.ModuleDict with entity embedding weights\n            - data_column_info.joblib \u2014 column layout list\n            - metadata.json \u2014 hyperparameters and version info\n\n        Args:\n            path: Directory path to save into.\n            overwrite: If False (default), raises SerializationError if path already exists.\n\n        Raises:\n            SerializationError: If path exists and overwrite=False, or if any\n                component fails to serialize.\n        \"\"\"\n        import joblib\n        import json\n        from pathlib import Path\n        from datetime import datetime, timezone\n\n        p = Path(path)\n        if p.exists() and not overwrite:\n            raise SerializationError(\n                f\"SerializationError: Save path '{path}' already exists. \"\n                f\"Pass overwrite=True to replace it.\"\n            )\n\n        try:\n            p.mkdir(parents=True, exist_ok=True)\n\n            # Network weights \u2014 torch native format\n            torch.save(self.generator.state_dict(), p / \"generator.pt\")\n            torch.save(self.discriminator.state_dict(), p / \"discriminator.pt\")\n\n            # sklearn and numpy-heavy objects \u2014 joblib for efficient NumPy serialization\n            joblib.dump(self.transformer, p / \"transformer.joblib\")\n            joblib.dump(self.context_transformer, p / \"context_transformer.joblib\")\n\n            # Embedding layers (nn.ModuleDict) \u2014 joblib serializes via pickle\n            joblib.dump(self.embedding_layers, p / \"embedding_layers.joblib\")\n\n            # Column layout list (list of dicts describing each column)\n            joblib.dump(self.data_column_info, p / \"data_column_info.joblib\")\n\n            # Metadata \u2014 human-readable, enables version mismatch detection on load\n            try:\n                from syntho_hive import __version__\n                current_version = __version__\n            except Exception as exc:\n                log.warning(\n                    \"version_lookup_failed\",\n                    error=str(exc),\n                    note=\"Could not determine SynthoHive version \u2014 using 'unknown'\",\n                )\n                current_version = \"unknown\"\n\n            meta = {\n                \"synthohive_version\": current_version,\n                \"embedding_dim\": self.embedding_dim,\n                \"generator_dim\": list(self.generator_dim),\n                \"discriminator_dim\": list(self.discriminator_dim),\n                \"saved_at\": datetime.now(timezone.utc).isoformat(),\n            }\n            with open(p / \"metadata.json\", \"w\") as f:\n                json.dump(meta, f, indent=2)\n\n            log.info(\"model_saved\", path=str(p))\n\n        except SerializationError:\n            raise\n        except Exception as exc:\n            raise SerializationError(\n                f\"SerializationError: Failed to save model to '{path}'. \"\n                f\"Original error: {exc}\"\n            ) from exc\n\n    def load(self, path: str) -&gt; None:\n        \"\"\"Load full model state from a directory checkpoint.\n\n        Reconstructs the complete model \u2014 DataTransformer, context_transformer,\n        embedding_layers, column layout, and network weights \u2014 without requiring\n        the original training data.\n\n        Args:\n            path: Directory path produced by save().\n\n        Raises:\n            SerializationError: If path does not exist, is missing required files,\n                or if any component fails to deserialize.\n        \"\"\"\n        import joblib\n        import json\n        from pathlib import Path\n\n        p = Path(path)\n        if not p.exists():\n            raise SerializationError(\n                f\"SerializationError: Checkpoint path '{path}' does not exist.\"\n            )\n\n        required_files = [\n            \"generator.pt\", \"discriminator.pt\",\n            \"transformer.joblib\", \"context_transformer.joblib\",\n            \"embedding_layers.joblib\", \"data_column_info.joblib\",\n        ]\n        missing = [f for f in required_files if not (p / f).exists()]\n        if missing:\n            raise SerializationError(\n                f\"SerializationError: Checkpoint at '{path}' is incomplete. \"\n                f\"Missing files: {', '.join(missing)}. \"\n                f\"The checkpoint may have been saved by an older version or is corrupt.\"\n            )\n\n        saved_version = \"unknown\"\n        try:\n            # Version check \u2014 warn but do not fail\n            meta_path = p / \"metadata.json\"\n            if meta_path.exists():\n                with open(meta_path) as f:\n                    meta = json.load(f)\n                try:\n                    from syntho_hive import __version__\n                    current_version = __version__\n                except Exception as exc:\n                    log.warning(\n                        \"version_lookup_failed\",\n                        error=str(exc),\n                        note=\"Could not determine SynthoHive version \u2014 using 'unknown'\",\n                    )\n                    current_version = \"unknown\"\n                saved_version = meta.get(\"synthohive_version\", \"unknown\")\n                if saved_version != current_version:\n                    log.warning(\n                        \"checkpoint_version_mismatch\",\n                        saved_version=saved_version,\n                        current_version=current_version,\n                        path=str(p),\n                        note=\"Attempting load \u2014 schema changes between versions may cause failures\",\n                    )\n                # Restore hyperparams from metadata so _build_model() uses correct dims\n                if \"embedding_dim\" in meta:\n                    self.embedding_dim = meta[\"embedding_dim\"]\n                if \"generator_dim\" in meta:\n                    self.generator_dim = tuple(meta[\"generator_dim\"])\n                if \"discriminator_dim\" in meta:\n                    self.discriminator_dim = tuple(meta[\"discriminator_dim\"])\n\n            # Load sklearn objects first \u2014 transformer must be in place before _build_model()\n            self.transformer = joblib.load(p / \"transformer.joblib\")\n            self.context_transformer = joblib.load(p / \"context_transformer.joblib\")\n\n            # Load saved column layout and embedding layers (will be restored after _build_model)\n            saved_data_column_info = joblib.load(p / \"data_column_info.joblib\")\n            saved_embedding_layers = joblib.load(p / \"embedding_layers.joblib\")\n\n            # Validate transformer round-trip integrity\n            if not hasattr(self.transformer, 'output_dim') or self.transformer.output_dim &lt;= 0:\n                raise SerializationError(\n                    f\"SerializationError: Loaded transformer has invalid output_dim \"\n                    f\"({getattr(self.transformer, 'output_dim', 'missing')}). \"\n                    f\"The checkpoint may be corrupt.\"\n                )\n\n            # Derive dimensions needed to reconstruct the generator/discriminator architecture.\n            # context_transformer.output_dim is 0 when no context was used during training.\n            data_dim = self.transformer.output_dim\n            context_dim = getattr(self.context_transformer, 'output_dim', 0)\n\n            # Reconstruct generator/discriminator architecture.\n            # _build_model() internally calls _compile_layout(self.transformer) which overwrites\n            # self.data_column_info and self.embedding_layers with freshly-initialised layers.\n            # We restore the saved values immediately after so weights can be loaded correctly.\n            self._build_model(data_dim, context_dim)\n\n            # Restore saved column layout and trained embedding weights (overwrite fresh ones)\n            self.data_column_info = saved_data_column_info\n            self.embedding_layers = saved_embedding_layers\n\n            # Load network weights \u2014 weights_only=False REQUIRED for PyTorch 2.6+\n            # (PyTorch 2.6 changed default to weights_only=True; custom objects fail without False)\n            self.generator.load_state_dict(\n                torch.load(p / \"generator.pt\", weights_only=False)\n            )\n            self.discriminator.load_state_dict(\n                torch.load(p / \"discriminator.pt\", weights_only=False)\n            )\n\n            # Set model to eval mode for inference\n            self.generator.eval()\n            self.discriminator.eval()\n\n            log.info(\"model_loaded\", path=str(p), version=saved_version)\n\n        except SerializationError:\n            raise\n        except Exception as exc:\n            raise SerializationError(\n                f\"SerializationError: Failed to load model from '{path}'. \"\n                f\"Original error: {exc}\"\n            ) from exc\n</code></pre>"},{"location":"api/core/#syntho_hive.core.models.ctgan.CTGAN.fit","title":"fit","text":"<pre><code>fit(data: DataFrame, context: Optional[DataFrame] = None, table_name: Optional[str] = None, checkpoint_dir: Optional[str] = None, log_metrics: bool = True, seed: Optional[int] = None, **kwargs: Any) -&gt; None\n</code></pre> <p>Train the CTGAN model on tabular data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Child table data (target) to model.</p> required <code>context</code> <code>Optional[DataFrame]</code> <p>Parent attributes to condition on (aligned row-wise).</p> <code>None</code> <code>table_name</code> <code>Optional[str]</code> <p>Table name for metadata lookup and constraint handling.</p> <code>None</code> <code>checkpoint_dir</code> <code>Optional[str]</code> <p>Directory to save checkpoints (best model, metrics). Defaults to None.</p> <code>None</code> <code>log_metrics</code> <code>bool</code> <p>Whether to save training metrics to a CSV file. Defaults to True.</p> <code>True</code> <code>seed</code> <code>Optional[int]</code> <p>Integer seed for deterministic training. When None, an integer is   auto-generated and logged so the run can be reproduced later.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Extra training options (unused placeholder for compatibility).</p> <code>{}</code> Source code in <code>syntho_hive/core/models/ctgan.py</code> <pre><code>def fit(self, data: pd.DataFrame, context: Optional[pd.DataFrame] = None, table_name: Optional[str] = None, checkpoint_dir: Optional[str] = None, log_metrics: bool = True, seed: Optional[int] = None, **kwargs: Any) -&gt; None:\n    \"\"\"Train the CTGAN model on tabular data.\n\n    Args:\n        data: Child table data (target) to model.\n        context: Parent attributes to condition on (aligned row-wise).\n        table_name: Table name for metadata lookup and constraint handling.\n        checkpoint_dir: Directory to save checkpoints (best model, metrics). Defaults to None.\n        log_metrics: Whether to save training metrics to a CSV file. Defaults to True.\n        seed: Integer seed for deterministic training. When None, an integer is\n              auto-generated and logged so the run can be reproduced later.\n        **kwargs: Extra training options (unused placeholder for compatibility).\n    \"\"\"\n    import random as _random\n\n    # Seed handling \u2014 auto-generate when not provided so every run is reproducible.\n    if seed is None:\n        seed = _random.randint(0, 2**31 - 1)\n        log.info(\n            \"training_seed\",\n            seed=seed,\n            message=\"No seed provided \u2014 auto-generated. Log this value to reproduce this run.\",\n        )\n    else:\n        log.info(\"training_seed\", seed=seed)\n\n    _set_seed(seed)\n\n    # 0. Setup Checkpointing\n    if checkpoint_dir:\n        os.makedirs(checkpoint_dir, exist_ok=True)\n\n    best_loss = float('inf')\n    history = []\n    # 1. Fit and Transform Data\n    self.transformer.fit(data, table_name=table_name, seed=seed)\n    train_data = self.transformer.transform(data)\n    train_data = torch.from_numpy(train_data).float().to(self.device)\n\n    # 2. Handle Context\n    if context is not None:\n        assert len(data) == len(context), \"Data and context must have same number of rows\"\n\n        # Use dedicated transformer for context\n        # NOTE: We abuse metdata here slightly. Ideally context comes from a known table (Parent).\n        # But context might be a mix of parent columns. \n        # For fit, we pass table_name=None to fit on just the columns present in context df.\n        self.context_transformer.fit(context)\n        context_transformed = self.context_transformer.transform(context)\n        context_data = torch.from_numpy(context_transformed).float().to(self.device)\n        context_dim = context_data.shape[1]\n    else:\n        context_data = None\n        context_dim = 0\n\n    data_dim = train_data.shape[1]\n\n    # 3. Build Model\n    if self.generator is None:\n        self._build_model(data_dim, context_dim)\n\n    optimizer_G = optim.Adam(self.generator.parameters(), lr=2e-4, betas=(0.5, 0.9))\n    optimizer_D = optim.Adam(self.discriminator.parameters(), lr=2e-4, betas=(0.5, 0.9))\n\n    # 4. Training Loop (WGAN-GP)\n    steps_per_epoch = max(len(train_data) // self.batch_size, 1)\n\n    for epoch in range(self.epochs):\n        for i in range(steps_per_epoch):\n            # --- Train Discriminator ---\n            for _ in range(self.discriminator_steps):\n                # Sample real data\n                idx = np.random.randint(0, len(train_data), self.batch_size)\n                real_data_batch = train_data[idx]\n                if context_data is not None:\n                    real_context_batch = context_data[idx]\n                    real_input = torch.cat([real_data_batch, real_context_batch], dim=1)\n                else:\n                    real_context_batch = None\n                    real_input = real_data_batch\n\n                # Generate fake data\n                noise = torch.randn(self.batch_size, self.embedding_dim, device=self.device)\n                if real_context_batch is not None:\n                    gen_input = torch.cat([noise, real_context_batch], dim=1)\n                else:\n                    gen_input = noise\n\n                fake_raw = self.generator(gen_input)\n\n                # Apply Embeddings / Softmax to Fake Data\n                fake_parts = []\n                fake_ptr = 0\n                for info in self.data_column_info:\n                    if info['type'] == 'embedding':\n                        dim = info['num_categories']\n                        logits = fake_raw[:, fake_ptr:fake_ptr+dim]\n                        fake_ptr += dim\n\n                        # Gumbel Softmax or Softmax? WGAN prefers generic softmax for differentiability\n                        # Note: Gumbel Softmax allows hard sampling with gradients.\n                        probs = F.softmax(logits, dim=1)\n                        emb_vect = self.embedding_layers[info['name']].forward_soft(probs)\n                        fake_parts.append(emb_vect)\n                    else:\n                        dim = info['output_dim']\n                        val = fake_raw[:, fake_ptr:fake_ptr+dim]\n                        fake_ptr += dim\n                        fake_parts.append(val)\n\n                fake_data_batch = torch.cat(fake_parts, dim=1)\n\n                # Apply Embeddings to Real Data\n                real_parts = []\n                real_ptr = 0\n                # Need to iterate column info again to slice real data correctly\n                # Real data from transformer is concatenated (Indices, Values...)\n                for info in self.data_column_info:\n                    dim = info['input_dim'] # 1 for embedding (index)\n                    col_data = real_data_batch[:, real_ptr:real_ptr+dim]\n                    real_ptr += dim\n\n                    if info['type'] == 'embedding':\n                        emb_vect = self.embedding_layers[info['name']](col_data.long().squeeze(1))\n                        real_parts.append(emb_vect)\n                    else:\n                        real_parts.append(col_data)\n\n                real_data_processed = torch.cat(real_parts, dim=1)\n\n                if real_context_batch is not None:\n                    fake_input = torch.cat([fake_data_batch, real_context_batch], dim=1)\n                    real_input_processed = torch.cat([real_data_processed, real_context_batch], dim=1)\n                else:\n                    fake_input = fake_data_batch\n                    real_input_processed = real_data_processed\n\n                # Compute WGAN loss\n                d_real = self.discriminator(real_input_processed)\n                d_fake = self.discriminator(fake_input)\n\n                # Gradient Penalty\n                gp = compute_gradient_penalty(self.discriminator, real_input_processed, fake_input, self.device)\n\n                loss_D = -torch.mean(d_real) + torch.mean(d_fake) + 10.0 * gp\n\n                optimizer_D.zero_grad()\n                loss_D.backward()\n                optimizer_D.step()\n\n            # --- Train Generator ---\n            # Train generator once after n_critic discriminator steps\n            noise = torch.randn(self.batch_size, self.embedding_dim, device=self.device)\n            if real_context_batch is not None:\n                # Re-sample context for generator training?? \n                # Ideally yes, but reusing batch is fine for conditional stability\n                # We'll stick to reusing the last seen batch for simplicity/stability\n                gen_input = torch.cat([noise, real_context_batch], dim=1)\n            else:\n                gen_input = noise\n\n            fake_raw = self.generator(gen_input)\n\n            # Apply Embeddings / Softmax (Same logic as above)\n            fake_parts = []\n            fake_ptr = 0\n            for info in self.data_column_info:\n                if info['type'] == 'embedding':\n                    dim = info['num_categories']\n                    logits = fake_raw[:, fake_ptr:fake_ptr+dim]\n                    fake_ptr += dim\n                    probs = F.softmax(logits, dim=1)\n                    emb_vect = self.embedding_layers[info['name']].forward_soft(probs)\n                    fake_parts.append(emb_vect)\n                else:\n                    dim = info['output_dim']\n                    val = fake_raw[:, fake_ptr:fake_ptr+dim]\n                    fake_ptr += dim\n                    fake_parts.append(val)\n\n            fake_data_batch = torch.cat(fake_parts, dim=1)\n\n            if real_context_batch is not None:\n                fake_input = torch.cat([fake_data_batch, real_context_batch], dim=1)\n            else:\n                fake_input = fake_data_batch\n\n            d_fake = self.discriminator(fake_input)\n            loss_G = -torch.mean(d_fake)\n\n            optimizer_G.zero_grad()\n            loss_G.backward()\n            optimizer_G.step()\n\n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch}: Loss D={loss_D.item():.4f}, Loss G={loss_G.item():.4f}\")\n\n        # --- Checkpointing &amp; Logging ---\n        current_loss_g = loss_G.item()\n        current_loss_d = loss_D.item()\n\n        if log_metrics:\n            history.append({\n                'epoch': epoch,\n                'loss_g': current_loss_g,\n                'loss_d': current_loss_d\n            })\n\n        if checkpoint_dir and current_loss_g &lt; best_loss:\n            best_loss = current_loss_g\n            self.save(os.path.join(checkpoint_dir, \"best_model.pt\"))\n\n    # End of training: Save metrics and last model\n    if checkpoint_dir:\n        self.save(os.path.join(checkpoint_dir, \"last_model.pt\"))\n\n        if log_metrics and history:\n            metrics_path = os.path.join(checkpoint_dir, \"training_metrics.csv\")\n            keys = history[0].keys()\n            with open(metrics_path, 'w', newline='') as f:\n                dict_writer = csv.DictWriter(f, fieldnames=keys)\n                dict_writer.writeheader()\n                dict_writer.writerows(history)\n            print(f\"Training metrics saved to {metrics_path}\")\n</code></pre>"},{"location":"api/core/#syntho_hive.core.models.ctgan.CTGAN.sample","title":"sample","text":"<pre><code>sample(num_rows: int, context: Optional[DataFrame] = None, seed: Optional[int] = None, enforce_constraints: bool = False, **kwargs: Any) -&gt; pd.DataFrame\n</code></pre> <p>Generate synthetic samples, optionally conditioned on parent context.</p> <p>Parameters:</p> Name Type Description Default <code>num_rows</code> <code>int</code> <p>Number of rows to generate.</p> required <code>context</code> <code>Optional[DataFrame]</code> <p>Optional parent attributes aligned to the requested rows.</p> <code>None</code> <code>seed</code> <code>Optional[int]</code> <p>Optional integer seed for deterministic sampling. Only applied   when provided; no auto-generation (fits and samples may use   independent seeds per CONTEXT.md decision).</p> <code>None</code> <code>enforce_constraints</code> <code>bool</code> <p>When True, inspects generated rows against column   constraints defined in the table's Metadata config.  Any rows   that violate a min/max constraint are dropped and a structlog   WARNING is emitted listing each violation.  When False (default),   constraint checking is skipped entirely \u2014 this matches the   pre-existing behavior where inverse_transform() already clips   values within each column's defined range.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional sampling controls (unused placeholder).</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame of synthetic rows mapped back to original schema.</p> Source code in <code>syntho_hive/core/models/ctgan.py</code> <pre><code>def sample(self, num_rows: int, context: Optional[pd.DataFrame] = None, seed: Optional[int] = None, enforce_constraints: bool = False, **kwargs: Any) -&gt; pd.DataFrame:\n    \"\"\"Generate synthetic samples, optionally conditioned on parent context.\n\n    Args:\n        num_rows: Number of rows to generate.\n        context: Optional parent attributes aligned to the requested rows.\n        seed: Optional integer seed for deterministic sampling. Only applied\n              when provided; no auto-generation (fits and samples may use\n              independent seeds per CONTEXT.md decision).\n        enforce_constraints: When True, inspects generated rows against column\n              constraints defined in the table's Metadata config.  Any rows\n              that violate a min/max constraint are dropped and a structlog\n              WARNING is emitted listing each violation.  When False (default),\n              constraint checking is skipped entirely \u2014 this matches the\n              pre-existing behavior where inverse_transform() already clips\n              values within each column's defined range.\n        **kwargs: Additional sampling controls (unused placeholder).\n\n    Returns:\n        DataFrame of synthetic rows mapped back to original schema.\n    \"\"\"\n    if seed is not None:\n        _set_seed(seed)\n\n    self.generator.eval()\n    with torch.no_grad():\n        noise = torch.randn(num_rows, self.embedding_dim, device=self.device)\n\n        if context is not None:\n            # Assuming context is provided for exactly num_rows\n            assert len(context) == num_rows\n\n            # Transform context using the fitted context transformer\n            context_transformed = self.context_transformer.transform(context)\n            context_data = torch.from_numpy(context_transformed).float().to(self.device)\n\n            gen_input = torch.cat([noise, context_data], dim=1)\n        else:\n            gen_input = noise\n\n        fake_raw = self.generator(gen_input)\n\n        # Post-process logits to indices for output\n        output_parts = []\n        fake_ptr = 0\n        for info in self.data_column_info:\n            if info['type'] == 'embedding':\n                dim = info['num_categories']\n                logits = fake_raw[:, fake_ptr:fake_ptr+dim]\n                fake_ptr += dim\n\n                # Argmax to get index\n                indices = torch.argmax(logits, dim=1, keepdim=True)\n                output_parts.append(indices.cpu().numpy())\n            else:\n                dim = info['output_dim']\n                val = fake_raw[:, fake_ptr:fake_ptr+dim]\n                fake_ptr += dim\n                output_parts.append(val.cpu().numpy())\n\n        fake_data_np = np.concatenate(output_parts, axis=1)\n\n    result_df = self.transformer.inverse_transform(fake_data_np)\n\n    # Constraint violation checking (opt-in via enforce_constraints=True).\n    # Note: inverse_transform() already clips values within defined column ranges,\n    # so enforce_constraints=True is primarily useful for post-hoc auditing or\n    # catching any residual violations before returning rows to the caller.\n    if enforce_constraints:\n        table_config = None\n        table_name = getattr(self.transformer, 'table_name', None)\n        if hasattr(self, 'metadata') and table_name:\n            try:\n                table_config = self.metadata.get_table(table_name)\n            except Exception as exc:\n                log.warning(\n                    \"constraint_config_lookup_failed\",\n                    table_name=table_name,\n                    error=str(exc),\n                    note=\"Skipping constraint enforcement \u2014 table config could not be retrieved\",\n                )\n                table_config = None\n\n        # If the table has constraints defined, scan generated rows.\n        # If no constraints are configured this block is a no-op.\n        if table_config is not None and table_config.constraints:\n            violations = []\n            valid_mask = pd.Series([True] * len(result_df), index=result_df.index)\n\n            for col_name, constraint in table_config.constraints.items():\n                if col_name not in result_df.columns:\n                    continue\n                col_data = result_df[col_name]\n\n                # Check min constraint\n                min_val = constraint.min\n                if min_val is not None:\n                    try:\n                        col_numeric = pd.to_numeric(col_data, errors='coerce')\n                        bad = col_numeric &lt; min_val\n                        if bad.any():\n                            observed = col_numeric[bad].min()\n                            violations.append(\n                                f\"{col_name}: got {observed:.4g} (min={min_val})\"\n                            )\n                            valid_mask &amp;= ~bad\n                    except Exception as exc:\n                        log.warning(\n                            \"constraint_min_check_skipped\",\n                            column=col_name,\n                            error=str(exc),\n                            note=\"Column is non-numeric or comparison failed \u2014 skipping min check\",\n                        )\n\n                # Check max constraint\n                max_val = constraint.max\n                if max_val is not None:\n                    try:\n                        col_numeric = pd.to_numeric(col_data, errors='coerce')\n                        bad = col_numeric &gt; max_val\n                        if bad.any():\n                            observed = col_numeric[bad].max()\n                            violations.append(\n                                f\"{col_name}: got {observed:.4g} (max={max_val})\"\n                            )\n                            valid_mask &amp;= ~bad\n                    except Exception as exc:\n                        log.warning(\n                            \"constraint_max_check_skipped\",\n                            column=col_name,\n                            error=str(exc),\n                            note=\"Column is non-numeric or comparison failed \u2014 skipping max check\",\n                        )\n\n            if violations:\n                summary = \"; \".join(violations)\n                # ROADMAP success criterion 4 and REQUIREMENTS.md QUAL-04 require\n                # violations to \"raise with the column name and observed value\".\n                # Use sample(enforce_constraints=False) (the default) if you want\n                # the previous warn-and-return behavior.\n                raise ConstraintViolationError(\n                    f\"ConstraintViolationError: {len(violations)} violation(s) found \u2014 \"\n                    f\"{summary}\"\n                )\n\n    return result_df\n</code></pre>"},{"location":"api/core/#syntho_hive.core.models.ctgan.CTGAN.save","title":"save","text":"<pre><code>save(path: str, *, overwrite: bool = False) -&gt; None\n</code></pre> <p>Persist full model state to a directory checkpoint.</p> <p>Saves all components required for a cold load-and-sample without the original training data: network weights, DataTransformer state, context_transformer state, embedding layer weights, column layout, and human-readable metadata.</p> The directory contains <ul> <li>generator.pt \u2014 generator state_dict</li> <li>discriminator.pt \u2014 discriminator state_dict</li> <li>transformer.joblib \u2014 fitted DataTransformer for child table</li> <li>context_transformer.joblib \u2014 fitted DataTransformer for context</li> <li>embedding_layers.joblib \u2014 nn.ModuleDict with entity embedding weights</li> <li>data_column_info.joblib \u2014 column layout list</li> <li>metadata.json \u2014 hyperparameters and version info</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path to save into.</p> required <code>overwrite</code> <code>bool</code> <p>If False (default), raises SerializationError if path already exists.</p> <code>False</code> <p>Raises:</p> Type Description <code>SerializationError</code> <p>If path exists and overwrite=False, or if any component fails to serialize.</p> Source code in <code>syntho_hive/core/models/ctgan.py</code> <pre><code>def save(self, path: str, *, overwrite: bool = False) -&gt; None:\n    \"\"\"Persist full model state to a directory checkpoint.\n\n    Saves all components required for a cold load-and-sample without the\n    original training data: network weights, DataTransformer state,\n    context_transformer state, embedding layer weights, column layout, and\n    human-readable metadata.\n\n    The directory contains:\n        - generator.pt \u2014 generator state_dict\n        - discriminator.pt \u2014 discriminator state_dict\n        - transformer.joblib \u2014 fitted DataTransformer for child table\n        - context_transformer.joblib \u2014 fitted DataTransformer for context\n        - embedding_layers.joblib \u2014 nn.ModuleDict with entity embedding weights\n        - data_column_info.joblib \u2014 column layout list\n        - metadata.json \u2014 hyperparameters and version info\n\n    Args:\n        path: Directory path to save into.\n        overwrite: If False (default), raises SerializationError if path already exists.\n\n    Raises:\n        SerializationError: If path exists and overwrite=False, or if any\n            component fails to serialize.\n    \"\"\"\n    import joblib\n    import json\n    from pathlib import Path\n    from datetime import datetime, timezone\n\n    p = Path(path)\n    if p.exists() and not overwrite:\n        raise SerializationError(\n            f\"SerializationError: Save path '{path}' already exists. \"\n            f\"Pass overwrite=True to replace it.\"\n        )\n\n    try:\n        p.mkdir(parents=True, exist_ok=True)\n\n        # Network weights \u2014 torch native format\n        torch.save(self.generator.state_dict(), p / \"generator.pt\")\n        torch.save(self.discriminator.state_dict(), p / \"discriminator.pt\")\n\n        # sklearn and numpy-heavy objects \u2014 joblib for efficient NumPy serialization\n        joblib.dump(self.transformer, p / \"transformer.joblib\")\n        joblib.dump(self.context_transformer, p / \"context_transformer.joblib\")\n\n        # Embedding layers (nn.ModuleDict) \u2014 joblib serializes via pickle\n        joblib.dump(self.embedding_layers, p / \"embedding_layers.joblib\")\n\n        # Column layout list (list of dicts describing each column)\n        joblib.dump(self.data_column_info, p / \"data_column_info.joblib\")\n\n        # Metadata \u2014 human-readable, enables version mismatch detection on load\n        try:\n            from syntho_hive import __version__\n            current_version = __version__\n        except Exception as exc:\n            log.warning(\n                \"version_lookup_failed\",\n                error=str(exc),\n                note=\"Could not determine SynthoHive version \u2014 using 'unknown'\",\n            )\n            current_version = \"unknown\"\n\n        meta = {\n            \"synthohive_version\": current_version,\n            \"embedding_dim\": self.embedding_dim,\n            \"generator_dim\": list(self.generator_dim),\n            \"discriminator_dim\": list(self.discriminator_dim),\n            \"saved_at\": datetime.now(timezone.utc).isoformat(),\n        }\n        with open(p / \"metadata.json\", \"w\") as f:\n            json.dump(meta, f, indent=2)\n\n        log.info(\"model_saved\", path=str(p))\n\n    except SerializationError:\n        raise\n    except Exception as exc:\n        raise SerializationError(\n            f\"SerializationError: Failed to save model to '{path}'. \"\n            f\"Original error: {exc}\"\n        ) from exc\n</code></pre>"},{"location":"api/core/#syntho_hive.core.models.ctgan.CTGAN.load","title":"load","text":"<pre><code>load(path: str) -&gt; None\n</code></pre> <p>Load full model state from a directory checkpoint.</p> <p>Reconstructs the complete model \u2014 DataTransformer, context_transformer, embedding_layers, column layout, and network weights \u2014 without requiring the original training data.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path produced by save().</p> required <p>Raises:</p> Type Description <code>SerializationError</code> <p>If path does not exist, is missing required files, or if any component fails to deserialize.</p> Source code in <code>syntho_hive/core/models/ctgan.py</code> <pre><code>def load(self, path: str) -&gt; None:\n    \"\"\"Load full model state from a directory checkpoint.\n\n    Reconstructs the complete model \u2014 DataTransformer, context_transformer,\n    embedding_layers, column layout, and network weights \u2014 without requiring\n    the original training data.\n\n    Args:\n        path: Directory path produced by save().\n\n    Raises:\n        SerializationError: If path does not exist, is missing required files,\n            or if any component fails to deserialize.\n    \"\"\"\n    import joblib\n    import json\n    from pathlib import Path\n\n    p = Path(path)\n    if not p.exists():\n        raise SerializationError(\n            f\"SerializationError: Checkpoint path '{path}' does not exist.\"\n        )\n\n    required_files = [\n        \"generator.pt\", \"discriminator.pt\",\n        \"transformer.joblib\", \"context_transformer.joblib\",\n        \"embedding_layers.joblib\", \"data_column_info.joblib\",\n    ]\n    missing = [f for f in required_files if not (p / f).exists()]\n    if missing:\n        raise SerializationError(\n            f\"SerializationError: Checkpoint at '{path}' is incomplete. \"\n            f\"Missing files: {', '.join(missing)}. \"\n            f\"The checkpoint may have been saved by an older version or is corrupt.\"\n        )\n\n    saved_version = \"unknown\"\n    try:\n        # Version check \u2014 warn but do not fail\n        meta_path = p / \"metadata.json\"\n        if meta_path.exists():\n            with open(meta_path) as f:\n                meta = json.load(f)\n            try:\n                from syntho_hive import __version__\n                current_version = __version__\n            except Exception as exc:\n                log.warning(\n                    \"version_lookup_failed\",\n                    error=str(exc),\n                    note=\"Could not determine SynthoHive version \u2014 using 'unknown'\",\n                )\n                current_version = \"unknown\"\n            saved_version = meta.get(\"synthohive_version\", \"unknown\")\n            if saved_version != current_version:\n                log.warning(\n                    \"checkpoint_version_mismatch\",\n                    saved_version=saved_version,\n                    current_version=current_version,\n                    path=str(p),\n                    note=\"Attempting load \u2014 schema changes between versions may cause failures\",\n                )\n            # Restore hyperparams from metadata so _build_model() uses correct dims\n            if \"embedding_dim\" in meta:\n                self.embedding_dim = meta[\"embedding_dim\"]\n            if \"generator_dim\" in meta:\n                self.generator_dim = tuple(meta[\"generator_dim\"])\n            if \"discriminator_dim\" in meta:\n                self.discriminator_dim = tuple(meta[\"discriminator_dim\"])\n\n        # Load sklearn objects first \u2014 transformer must be in place before _build_model()\n        self.transformer = joblib.load(p / \"transformer.joblib\")\n        self.context_transformer = joblib.load(p / \"context_transformer.joblib\")\n\n        # Load saved column layout and embedding layers (will be restored after _build_model)\n        saved_data_column_info = joblib.load(p / \"data_column_info.joblib\")\n        saved_embedding_layers = joblib.load(p / \"embedding_layers.joblib\")\n\n        # Validate transformer round-trip integrity\n        if not hasattr(self.transformer, 'output_dim') or self.transformer.output_dim &lt;= 0:\n            raise SerializationError(\n                f\"SerializationError: Loaded transformer has invalid output_dim \"\n                f\"({getattr(self.transformer, 'output_dim', 'missing')}). \"\n                f\"The checkpoint may be corrupt.\"\n            )\n\n        # Derive dimensions needed to reconstruct the generator/discriminator architecture.\n        # context_transformer.output_dim is 0 when no context was used during training.\n        data_dim = self.transformer.output_dim\n        context_dim = getattr(self.context_transformer, 'output_dim', 0)\n\n        # Reconstruct generator/discriminator architecture.\n        # _build_model() internally calls _compile_layout(self.transformer) which overwrites\n        # self.data_column_info and self.embedding_layers with freshly-initialised layers.\n        # We restore the saved values immediately after so weights can be loaded correctly.\n        self._build_model(data_dim, context_dim)\n\n        # Restore saved column layout and trained embedding weights (overwrite fresh ones)\n        self.data_column_info = saved_data_column_info\n        self.embedding_layers = saved_embedding_layers\n\n        # Load network weights \u2014 weights_only=False REQUIRED for PyTorch 2.6+\n        # (PyTorch 2.6 changed default to weights_only=True; custom objects fail without False)\n        self.generator.load_state_dict(\n            torch.load(p / \"generator.pt\", weights_only=False)\n        )\n        self.discriminator.load_state_dict(\n            torch.load(p / \"discriminator.pt\", weights_only=False)\n        )\n\n        # Set model to eval mode for inference\n        self.generator.eval()\n        self.discriminator.eval()\n\n        log.info(\"model_loaded\", path=str(p), version=saved_version)\n\n    except SerializationError:\n        raise\n    except Exception as exc:\n        raise SerializationError(\n            f\"SerializationError: Failed to load model from '{path}'. \"\n            f\"Original error: {exc}\"\n        ) from exc\n</code></pre>"},{"location":"api/core/#syntho_hive.core.models.base.ConditionalGenerativeModel","title":"syntho_hive.core.models.base.ConditionalGenerativeModel","text":"<p>               Bases: <code>GenerativeModel</code></p> <p>Contract for models that condition on parent context during training/sampling.</p> Source code in <code>syntho_hive/core/models/base.py</code> <pre><code>class ConditionalGenerativeModel(GenerativeModel):\n    \"\"\"Contract for models that condition on parent context during training/sampling.\"\"\"\n\n    @abstractmethod\n    def fit(self, data: pd.DataFrame, context: Optional[pd.DataFrame] = None, **kwargs: Any) -&gt; None:\n        \"\"\"Train the model with optional parent context.\n\n        Args:\n            data: Child table data to learn from.\n            context: Optional parent attributes used for conditioning.\n            **kwargs: Model-specific training options.\n        \"\"\"\n        pass  # pragma: no cover\n\n    @abstractmethod\n    def sample(self, num_rows: int, context: Optional[pd.DataFrame] = None, **kwargs: Any) -&gt; pd.DataFrame:\n        \"\"\"Generate synthetic rows with optional conditioning context.\n\n        Args:\n            num_rows: Number of rows to generate.\n            context: Optional parent attributes aligned to the requested rows.\n            **kwargs: Additional sampling controls.\n\n        Returns:\n            DataFrame of synthetic samples aligned to the provided context (if any).\n        \"\"\"\n        pass  # pragma: no cover\n</code></pre>"},{"location":"api/core/#syntho_hive.core.models.base.ConditionalGenerativeModel.fit","title":"fit  <code>abstractmethod</code>","text":"<pre><code>fit(data: DataFrame, context: Optional[DataFrame] = None, **kwargs: Any) -&gt; None\n</code></pre> <p>Train the model with optional parent context.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Child table data to learn from.</p> required <code>context</code> <code>Optional[DataFrame]</code> <p>Optional parent attributes used for conditioning.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Model-specific training options.</p> <code>{}</code> Source code in <code>syntho_hive/core/models/base.py</code> <pre><code>@abstractmethod\ndef fit(self, data: pd.DataFrame, context: Optional[pd.DataFrame] = None, **kwargs: Any) -&gt; None:\n    \"\"\"Train the model with optional parent context.\n\n    Args:\n        data: Child table data to learn from.\n        context: Optional parent attributes used for conditioning.\n        **kwargs: Model-specific training options.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"api/core/#syntho_hive.core.models.base.ConditionalGenerativeModel.sample","title":"sample  <code>abstractmethod</code>","text":"<pre><code>sample(num_rows: int, context: Optional[DataFrame] = None, **kwargs: Any) -&gt; pd.DataFrame\n</code></pre> <p>Generate synthetic rows with optional conditioning context.</p> <p>Parameters:</p> Name Type Description Default <code>num_rows</code> <code>int</code> <p>Number of rows to generate.</p> required <code>context</code> <code>Optional[DataFrame]</code> <p>Optional parent attributes aligned to the requested rows.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional sampling controls.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame of synthetic samples aligned to the provided context (if any).</p> Source code in <code>syntho_hive/core/models/base.py</code> <pre><code>@abstractmethod\ndef sample(self, num_rows: int, context: Optional[pd.DataFrame] = None, **kwargs: Any) -&gt; pd.DataFrame:\n    \"\"\"Generate synthetic rows with optional conditioning context.\n\n    Args:\n        num_rows: Number of rows to generate.\n        context: Optional parent attributes aligned to the requested rows.\n        **kwargs: Additional sampling controls.\n\n    Returns:\n        DataFrame of synthetic samples aligned to the provided context (if any).\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"api/core/#data-transformation","title":"Data Transformation","text":""},{"location":"api/core/#syntho_hive.core.data.transformer.DataTransformer","title":"syntho_hive.core.data.transformer.DataTransformer","text":"<p>Reversible transformer for tabular data.</p> <p>Continuous columns use a Bayesian GMM-based normalizer, while categorical columns are either one-hot encoded or mapped to indices for embeddings.</p> Source code in <code>syntho_hive/core/data/transformer.py</code> <pre><code>class DataTransformer:\n    \"\"\"Reversible transformer for tabular data.\n\n    Continuous columns use a Bayesian GMM-based normalizer, while categorical\n    columns are either one-hot encoded or mapped to indices for embeddings.\n    \"\"\"\n\n    def __init__(self, metadata: Any, embedding_threshold: int = 50):\n        \"\"\"Create a transformer configured by table metadata.\n\n        Args:\n            metadata: Metadata object describing tables, keys, and constraints.\n            embedding_threshold: Switch to embedding mode when cardinality exceeds this value.\n        \"\"\"\n        self.metadata = metadata\n        self.embedding_threshold = embedding_threshold\n        self._transformers = {}\n        self._column_info = {} # Maps col_name -&gt; {'type': str, 'dim': int, 'transformer': obj}\n        self.output_dim = 0\n        self._excluded_columns = []\n\n    def _prepare_categorical(self, series: pd.Series) -&gt; pd.Series:\n        \"\"\"Fill nulls with a sentinel value and ensure string type.\"\"\"\n        # Convert to object to handle mixed types (e.g. numbers and NaNs)\n        series = series.astype(object)\n        return series.fillna('&lt;NAN&gt;').astype(str)\n\n\n    def fit(self, data: pd.DataFrame, table_name: Optional[str] = None, seed: Optional[int] = None):\n        \"\"\"Fit per-column transformers and collect column layout metadata.\n\n        Args:\n            data: DataFrame to profile and transform.\n            table_name: Optional table name for applying PK/FK exclusions and constraints.\n            seed: Optional integer seed propagated to each ``ClusterBasedNormalizer``\n                  for deterministic BayesianGMM fitting.  A per-column seed is derived\n                  from ``seed`` to avoid correlated RNG sequences across columns.\n\n        Raises:\n            ValueError: If metadata is missing table configurations.\n        \"\"\"\n        self.table_name = table_name # Store for constraint application later\n        if not self.metadata.tables:\n            raise ValueError(\"Metadata must be populated with table configs\")\n\n        columns_to_transform = data.columns.tolist()\n\n        # Handle relational constraints if table_name is provided\n        if table_name:\n            table_config = self.metadata.get_table(table_name)\n            if table_config:\n                # Exclude PK and FKs from transformation\n                pk = table_config.pk\n                fks = list(table_config.fk.keys())\n                self._excluded_columns = [pk] + fks\n                columns_to_transform = [c for c in columns_to_transform if c not in self._excluded_columns]\n\n        self.output_dim = 0\n\n        for col in columns_to_transform:\n            col_data = data[col]\n\n            if pd.api.types.is_numeric_dtype(col_data):\n                # Derive a per-column deterministic seed from the parent seed to avoid\n                # correlated RNG sequences across columns when all columns share one seed.\n                col_seed = (seed + abs(hash(col)) % 100_000) if seed is not None else None\n                # Continuous column\n                transformer = ClusterBasedNormalizer(n_components=10, seed=col_seed)\n                transformer.fit(col_data)\n\n                # Dim is managed by the transformer now (dynamic based on nulls)\n                dim = transformer.output_dim\n                self._transformers[col] = transformer\n                self._column_info[col] = {\n                    'type': 'continuous',\n                    'dim': dim,\n                    'transformer': transformer\n                }\n                self.output_dim += dim\n\n            else:\n                # Categorical column\n                # Use OneHotEncoder for now. \n                # Categorical column\n                # Check cardinality for embedding suggestion\n                n_unique = col_data.nunique()\n                if n_unique &gt; self.embedding_threshold:\n                    # Use LabelEncoder for Entity Embeddings\n                    from sklearn.preprocessing import LabelEncoder\n                    transformer = LabelEncoder()\n\n                    # Fill Nulls\n                    col_data_filled = self._prepare_categorical(col_data)\n\n                    # LabelEncoder expects 1D array\n                    transformer.fit(col_data_filled)\n\n                    dim = 1 # Just the index\n                    num_categories = len(transformer.classes_)  # include sentinel for nulls\n                    self._transformers[col] = transformer\n                    self._column_info[col] = {\n                        'type': 'categorical_embedding',\n                        'dim': dim,\n                        'num_categories': num_categories,\n                        'transformer': transformer\n                    }\n                    self.output_dim += dim\n                else:\n                    # Use OneHotEncoder\n                    transformer = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n\n                    # Fill Nulls\n                    col_data_filled = self._prepare_categorical(col_data)\n\n                    values = col_data_filled.to_numpy(dtype=str).reshape(-1, 1)\n                    transformer.fit(values)\n\n                    dim = len(transformer.categories_[0])\n                    self._transformers[col] = transformer\n                    self._column_info[col] = {\n                        'type': 'categorical',\n                        'dim': dim,\n                        'transformer': transformer\n                    }\n                    self.output_dim += dim\n\n    def transform(self, data: pd.DataFrame) -&gt; np.ndarray:\n        \"\"\"Transform a dataframe into model-ready numpy arrays.\n\n        Args:\n            data: DataFrame with the same columns used during ``fit``.\n\n        Raises:\n            ValueError: If the transformer has not been fitted or a column is missing.\n\n        Returns:\n            Concatenated numpy array representing all transformed columns.\n        \"\"\"\n        if not self._transformers:\n            raise ValueError(\"Transformer has not been fitted.\")\n\n        output_arrays = []\n\n        # Iterate in the same order as fit/stored in _column_info\n        for col, info in self._column_info.items():\n            if col not in data.columns:\n                raise ValueError(f\"Column {col} missing from input data\")\n\n            transformer = self._transformers[col]\n            col_data = data[col]\n\n            if info['type'] == 'continuous':\n                # Returns (N, n_components + 1 [+ 1 if nulls])\n                transformed = transformer.transform(col_data)\n            elif info['type'] == 'categorical_embedding':\n                # Returns (N, 1)\n                col_data_filled = self._prepare_categorical(col_data)\n                values = transformer.transform(col_data_filled)\n                transformed = values.reshape(-1, 1)\n            else:\n                # Returns (N, n_categories)\n                col_data_filled = self._prepare_categorical(col_data)\n                values = col_data_filled.to_numpy(dtype=str).reshape(-1, 1)\n                transformed = transformer.transform(values)\n\n            output_arrays.append(transformed)\n\n        return np.concatenate(output_arrays, axis=1)\n\n    def inverse_transform(self, data: np.ndarray) -&gt; pd.DataFrame:\n        \"\"\"Convert model outputs back to the original dataframe schema.\n\n        Args:\n            data: Numpy array produced by a model, aligned to transform layout.\n\n        Raises:\n            ValueError: If called before ``fit``.\n\n        Returns:\n            DataFrame with original column names and value types (constraints applied).\n        \"\"\"\n        if not self._transformers:\n            raise ValueError(\"Transformer has not been fitted.\")\n\n        output_df = pd.DataFrame()\n        start_idx = 0\n\n        for col, info in self._column_info.items():\n            dim = info['dim']\n            end_idx = start_idx + dim\n            col_data = data[:, start_idx:end_idx]\n\n            transformer = self._transformers[col]\n\n            if info['type'] == 'continuous':\n                original_values = transformer.inverse_transform(col_data)\n            elif info['type'] == 'categorical_embedding':\n                 # col_data is (N, 1) floats/ints. \n                 # We need ints for LabelEncoder.\n                 indices = np.clip(col_data.flatten().astype(int), 0, info['num_categories'] - 1)\n                 original_values = transformer.inverse_transform(indices)\n                 # Restore NaNs\n                 original_values = pd.Series(original_values).replace('&lt;NAN&gt;', np.nan).values\n            else:\n                original_values = transformer.inverse_transform(col_data).flatten()\n                # Restore NaNs\n                original_values = pd.Series(original_values).replace('&lt;NAN&gt;', np.nan).values\n\n            # Apply Constraints\n            if self.metadata and hasattr(self, 'table_name') and self.table_name:\n                table_config = self.metadata.get_table(self.table_name)\n                if table_config and col in table_config.constraints:\n                    constraint = table_config.constraints[col]\n\n                    _is_numeric_constraint = (constraint.min is not None) or \\\n                                           (constraint.max is not None) or \\\n                                           (constraint.dtype in [\"int\", \"float\", \"number\"])\n\n                    if _is_numeric_constraint:\n                        # Ensure data is numeric. If it was processed as categorical (strings),\n                        # we need to convert it back to numbers to apply numeric constraints.\n                        try:\n                            if isinstance(original_values, np.ndarray):\n                                # flatten to 1D for to_numeric\n                                original_values = pd.to_numeric(original_values.flatten(), errors='coerce')\n                            else:\n                                original_values = pd.to_numeric(original_values, errors='coerce')\n                        except Exception as exc:\n                            log.warning(\n                                \"column_cast_failed\",\n                                column=col,\n                                target_type=\"numeric\",\n                                error=str(exc),\n                            )\n                            # Leave the column as-is (do not set to NaN silently)\n\n                    # 1. Rounding/Type\n                    if constraint.dtype == \"int\":\n                        # Round first\n                        original_values = np.round(original_values)\n                        # Safe cast to int (handles NaNs by skipping cast or filling if appropriate? \n                        # For now, we only cast if possible to avoid crash)\n                        try:\n                            if isinstance(original_values, pd.Series):\n                                if not original_values.isnull().any():\n                                    original_values = original_values.astype(int)\n                            elif isinstance(original_values, np.ndarray):\n                                if not np.isnan(original_values).any():\n                                    original_values = original_values.astype(int)\n                        except Exception as exc:\n                            log.warning(\n                                \"column_cast_failed\",\n                                column=col,\n                                target_type=\"int\",\n                                error=str(exc),\n                            )\n                            # Leave the column as-is (do not set to NaN silently)\n\n                    # 2. Clipping\n                    if constraint.min is not None or constraint.max is not None:\n                        # Handle potential pandas Series or numpy array\n                        if isinstance(original_values, pd.Series):\n                            original_values = original_values.clip(lower=constraint.min, upper=constraint.max)\n                        else:\n                            original_values = np.clip(original_values, constraint.min, constraint.max)\n\n            output_df[col] = original_values\n            start_idx = end_idx\n\n        return output_df\n</code></pre>"},{"location":"api/core/#syntho_hive.core.data.transformer.DataTransformer.fit","title":"fit","text":"<pre><code>fit(data: DataFrame, table_name: Optional[str] = None, seed: Optional[int] = None)\n</code></pre> <p>Fit per-column transformers and collect column layout metadata.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame to profile and transform.</p> required <code>table_name</code> <code>Optional[str]</code> <p>Optional table name for applying PK/FK exclusions and constraints.</p> <code>None</code> <code>seed</code> <code>Optional[int]</code> <p>Optional integer seed propagated to each <code>ClusterBasedNormalizer</code>   for deterministic BayesianGMM fitting.  A per-column seed is derived   from <code>seed</code> to avoid correlated RNG sequences across columns.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If metadata is missing table configurations.</p> Source code in <code>syntho_hive/core/data/transformer.py</code> <pre><code>def fit(self, data: pd.DataFrame, table_name: Optional[str] = None, seed: Optional[int] = None):\n    \"\"\"Fit per-column transformers and collect column layout metadata.\n\n    Args:\n        data: DataFrame to profile and transform.\n        table_name: Optional table name for applying PK/FK exclusions and constraints.\n        seed: Optional integer seed propagated to each ``ClusterBasedNormalizer``\n              for deterministic BayesianGMM fitting.  A per-column seed is derived\n              from ``seed`` to avoid correlated RNG sequences across columns.\n\n    Raises:\n        ValueError: If metadata is missing table configurations.\n    \"\"\"\n    self.table_name = table_name # Store for constraint application later\n    if not self.metadata.tables:\n        raise ValueError(\"Metadata must be populated with table configs\")\n\n    columns_to_transform = data.columns.tolist()\n\n    # Handle relational constraints if table_name is provided\n    if table_name:\n        table_config = self.metadata.get_table(table_name)\n        if table_config:\n            # Exclude PK and FKs from transformation\n            pk = table_config.pk\n            fks = list(table_config.fk.keys())\n            self._excluded_columns = [pk] + fks\n            columns_to_transform = [c for c in columns_to_transform if c not in self._excluded_columns]\n\n    self.output_dim = 0\n\n    for col in columns_to_transform:\n        col_data = data[col]\n\n        if pd.api.types.is_numeric_dtype(col_data):\n            # Derive a per-column deterministic seed from the parent seed to avoid\n            # correlated RNG sequences across columns when all columns share one seed.\n            col_seed = (seed + abs(hash(col)) % 100_000) if seed is not None else None\n            # Continuous column\n            transformer = ClusterBasedNormalizer(n_components=10, seed=col_seed)\n            transformer.fit(col_data)\n\n            # Dim is managed by the transformer now (dynamic based on nulls)\n            dim = transformer.output_dim\n            self._transformers[col] = transformer\n            self._column_info[col] = {\n                'type': 'continuous',\n                'dim': dim,\n                'transformer': transformer\n            }\n            self.output_dim += dim\n\n        else:\n            # Categorical column\n            # Use OneHotEncoder for now. \n            # Categorical column\n            # Check cardinality for embedding suggestion\n            n_unique = col_data.nunique()\n            if n_unique &gt; self.embedding_threshold:\n                # Use LabelEncoder for Entity Embeddings\n                from sklearn.preprocessing import LabelEncoder\n                transformer = LabelEncoder()\n\n                # Fill Nulls\n                col_data_filled = self._prepare_categorical(col_data)\n\n                # LabelEncoder expects 1D array\n                transformer.fit(col_data_filled)\n\n                dim = 1 # Just the index\n                num_categories = len(transformer.classes_)  # include sentinel for nulls\n                self._transformers[col] = transformer\n                self._column_info[col] = {\n                    'type': 'categorical_embedding',\n                    'dim': dim,\n                    'num_categories': num_categories,\n                    'transformer': transformer\n                }\n                self.output_dim += dim\n            else:\n                # Use OneHotEncoder\n                transformer = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n\n                # Fill Nulls\n                col_data_filled = self._prepare_categorical(col_data)\n\n                values = col_data_filled.to_numpy(dtype=str).reshape(-1, 1)\n                transformer.fit(values)\n\n                dim = len(transformer.categories_[0])\n                self._transformers[col] = transformer\n                self._column_info[col] = {\n                    'type': 'categorical',\n                    'dim': dim,\n                    'transformer': transformer\n                }\n                self.output_dim += dim\n</code></pre>"},{"location":"api/core/#syntho_hive.core.data.transformer.DataTransformer.transform","title":"transform","text":"<pre><code>transform(data: DataFrame) -&gt; np.ndarray\n</code></pre> <p>Transform a dataframe into model-ready numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame with the same columns used during <code>fit</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the transformer has not been fitted or a column is missing.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Concatenated numpy array representing all transformed columns.</p> Source code in <code>syntho_hive/core/data/transformer.py</code> <pre><code>def transform(self, data: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Transform a dataframe into model-ready numpy arrays.\n\n    Args:\n        data: DataFrame with the same columns used during ``fit``.\n\n    Raises:\n        ValueError: If the transformer has not been fitted or a column is missing.\n\n    Returns:\n        Concatenated numpy array representing all transformed columns.\n    \"\"\"\n    if not self._transformers:\n        raise ValueError(\"Transformer has not been fitted.\")\n\n    output_arrays = []\n\n    # Iterate in the same order as fit/stored in _column_info\n    for col, info in self._column_info.items():\n        if col not in data.columns:\n            raise ValueError(f\"Column {col} missing from input data\")\n\n        transformer = self._transformers[col]\n        col_data = data[col]\n\n        if info['type'] == 'continuous':\n            # Returns (N, n_components + 1 [+ 1 if nulls])\n            transformed = transformer.transform(col_data)\n        elif info['type'] == 'categorical_embedding':\n            # Returns (N, 1)\n            col_data_filled = self._prepare_categorical(col_data)\n            values = transformer.transform(col_data_filled)\n            transformed = values.reshape(-1, 1)\n        else:\n            # Returns (N, n_categories)\n            col_data_filled = self._prepare_categorical(col_data)\n            values = col_data_filled.to_numpy(dtype=str).reshape(-1, 1)\n            transformed = transformer.transform(values)\n\n        output_arrays.append(transformed)\n\n    return np.concatenate(output_arrays, axis=1)\n</code></pre>"},{"location":"api/core/#syntho_hive.core.data.transformer.DataTransformer.inverse_transform","title":"inverse_transform","text":"<pre><code>inverse_transform(data: ndarray) -&gt; pd.DataFrame\n</code></pre> <p>Convert model outputs back to the original dataframe schema.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Numpy array produced by a model, aligned to transform layout.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If called before <code>fit</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with original column names and value types (constraints applied).</p> Source code in <code>syntho_hive/core/data/transformer.py</code> <pre><code>def inverse_transform(self, data: np.ndarray) -&gt; pd.DataFrame:\n    \"\"\"Convert model outputs back to the original dataframe schema.\n\n    Args:\n        data: Numpy array produced by a model, aligned to transform layout.\n\n    Raises:\n        ValueError: If called before ``fit``.\n\n    Returns:\n        DataFrame with original column names and value types (constraints applied).\n    \"\"\"\n    if not self._transformers:\n        raise ValueError(\"Transformer has not been fitted.\")\n\n    output_df = pd.DataFrame()\n    start_idx = 0\n\n    for col, info in self._column_info.items():\n        dim = info['dim']\n        end_idx = start_idx + dim\n        col_data = data[:, start_idx:end_idx]\n\n        transformer = self._transformers[col]\n\n        if info['type'] == 'continuous':\n            original_values = transformer.inverse_transform(col_data)\n        elif info['type'] == 'categorical_embedding':\n             # col_data is (N, 1) floats/ints. \n             # We need ints for LabelEncoder.\n             indices = np.clip(col_data.flatten().astype(int), 0, info['num_categories'] - 1)\n             original_values = transformer.inverse_transform(indices)\n             # Restore NaNs\n             original_values = pd.Series(original_values).replace('&lt;NAN&gt;', np.nan).values\n        else:\n            original_values = transformer.inverse_transform(col_data).flatten()\n            # Restore NaNs\n            original_values = pd.Series(original_values).replace('&lt;NAN&gt;', np.nan).values\n\n        # Apply Constraints\n        if self.metadata and hasattr(self, 'table_name') and self.table_name:\n            table_config = self.metadata.get_table(self.table_name)\n            if table_config and col in table_config.constraints:\n                constraint = table_config.constraints[col]\n\n                _is_numeric_constraint = (constraint.min is not None) or \\\n                                       (constraint.max is not None) or \\\n                                       (constraint.dtype in [\"int\", \"float\", \"number\"])\n\n                if _is_numeric_constraint:\n                    # Ensure data is numeric. If it was processed as categorical (strings),\n                    # we need to convert it back to numbers to apply numeric constraints.\n                    try:\n                        if isinstance(original_values, np.ndarray):\n                            # flatten to 1D for to_numeric\n                            original_values = pd.to_numeric(original_values.flatten(), errors='coerce')\n                        else:\n                            original_values = pd.to_numeric(original_values, errors='coerce')\n                    except Exception as exc:\n                        log.warning(\n                            \"column_cast_failed\",\n                            column=col,\n                            target_type=\"numeric\",\n                            error=str(exc),\n                        )\n                        # Leave the column as-is (do not set to NaN silently)\n\n                # 1. Rounding/Type\n                if constraint.dtype == \"int\":\n                    # Round first\n                    original_values = np.round(original_values)\n                    # Safe cast to int (handles NaNs by skipping cast or filling if appropriate? \n                    # For now, we only cast if possible to avoid crash)\n                    try:\n                        if isinstance(original_values, pd.Series):\n                            if not original_values.isnull().any():\n                                original_values = original_values.astype(int)\n                        elif isinstance(original_values, np.ndarray):\n                            if not np.isnan(original_values).any():\n                                original_values = original_values.astype(int)\n                    except Exception as exc:\n                        log.warning(\n                            \"column_cast_failed\",\n                            column=col,\n                            target_type=\"int\",\n                            error=str(exc),\n                        )\n                        # Leave the column as-is (do not set to NaN silently)\n\n                # 2. Clipping\n                if constraint.min is not None or constraint.max is not None:\n                    # Handle potential pandas Series or numpy array\n                    if isinstance(original_values, pd.Series):\n                        original_values = original_values.clip(lower=constraint.min, upper=constraint.max)\n                    else:\n                        original_values = np.clip(original_values, constraint.min, constraint.max)\n\n        output_df[col] = original_values\n        start_idx = end_idx\n\n    return output_df\n</code></pre>"},{"location":"api/core/#syntho_hive.core.data.transformer.ClusterBasedNormalizer","title":"syntho_hive.core.data.transformer.ClusterBasedNormalizer","text":"<p>VGM-based normalizer for continuous columns.</p> <p>Projects a value to a cluster assignment and a normalized scalar relative to the chosen component.</p> Source code in <code>syntho_hive/core/data/transformer.py</code> <pre><code>class ClusterBasedNormalizer:\n    \"\"\"VGM-based normalizer for continuous columns.\n\n    Projects a value to a cluster assignment and a normalized scalar relative\n    to the chosen component.\n    \"\"\"\n\n    def __init__(self, n_components: int = 10, seed: Optional[int] = None):\n        \"\"\"Configure the number of mixture components.\n\n        Args:\n            n_components: Number of Gaussian mixture components.\n            seed: Random state for the BayesianGaussianMixture. When None,\n                  defaults to 42 for backward compatibility.\n        \"\"\"\n        self.n_components = n_components\n        # Use provided seed; fall back to 42 for backward compatibility when no seed given.\n        random_state = seed if seed is not None else 42\n        self.model = BayesianGaussianMixture(\n            n_components=n_components,\n            weight_concentration_prior_type='dirichlet_process',\n            n_init=1,\n            random_state=random_state\n        )\n        self.means = None\n        self.stds = None\n        self.has_nulls = False\n        self.fill_value = 0.0\n        self.output_dim = 0\n\n    def fit(self, data: pd.Series):\n        \"\"\"Fit the Bayesian GMM on a continuous series.\n\n        Args:\n            data: Continuous pandas Series to normalize.\n        \"\"\"\n        # 1. Handle Nulls\n        self.has_nulls = data.isnull().any()\n        if self.has_nulls:\n            self.fill_value = data.mean()\n            # Impute for training GMM\n            values = data.fillna(self.fill_value).to_numpy(dtype=float).reshape(-1, 1)\n            self.output_dim = self.n_components + 1 + 1 # +1 for null indicator\n        else:\n            values = data.to_numpy(dtype=float).reshape(-1, 1)\n            self.output_dim = self.n_components + 1\n\n        self.model.fit(values)\n        self.means = self.model.means_.flatten() # (n_components,)\n        self.stds = np.sqrt(self.model.covariances_).flatten() # (n_components,)\n\n    def transform(self, data: pd.Series) -&gt; np.ndarray:\n        \"\"\"Project values to one-hot cluster assignment and normalized scalar.\n\n        Args:\n            data: Continuous pandas Series to transform.\n\n        Returns:\n            Numpy array of shape ``(N, n_components + 1 [+1])`` with one-hot cluster, scaled value, [null_ind].\n        \"\"\"\n        values_raw = data.to_numpy(dtype=float).reshape(-1, 1)\n        n_samples = len(values_raw)\n\n        if self.has_nulls:\n            # 0. Create Null Indicator\n            null_indicator = pd.isnull(data).to_numpy(dtype=float).reshape(-1, 1)\n\n            # 1. Impute for projection\n            values_clean = data.fillna(self.fill_value).to_numpy(dtype=float).reshape(-1, 1)\n        else:\n            values_clean = values_raw\n\n        # 2. Get cluster probabilities: P(c|x)\n        probs = self.model.predict_proba(values_clean) # (N, n_components)\n\n        # 2. Sample component c ~ P(c|x) (Argmax for simplicity/determinism in this impl)\n        # CTGAN uses argmax during interaction but sampling during training prep sometimes. \n        # Using argmax is stable.\n        # 3. Sample component c ~ P(c|x) (Argmax for simplicity/determinism in this impl)\n        # CTGAN uses argmax during interaction but sampling during training prep sometimes. \n        # Using argmax is stable.\n        cluster_assignments = np.argmax(probs, axis=1)\n\n        # 4. Calculate normalized scalar: v = (x - mu_c) / (4 * sigma_c)\n        # Clip to [-1, 1] usually, or roughly there.\n        means = self.means[cluster_assignments]\n        stds = self.stds[cluster_assignments]\n\n        normalized_values = (values_clean.flatten() - means) / (4 * stds)\n        normalized_values = normalized_values.reshape(-1, 1)\n\n        # 5. Create One-Hot encoding of cluster assignment\n        cluster_one_hot = np.zeros((n_samples, self.n_components))\n        cluster_one_hot[np.arange(n_samples), cluster_assignments] = 1\n\n        # Output: [one_hot_cluster, scalar, (null_indicator)]\n        if self.has_nulls:\n            return np.concatenate([cluster_one_hot, normalized_values, null_indicator], axis=1)\n        else:\n            return np.concatenate([cluster_one_hot, normalized_values], axis=1)\n\n    def inverse_transform(self, data: np.ndarray) -&gt; pd.Series:\n        \"\"\"Reconstruct approximate original values from normalized representation.\n\n        Args:\n            data: Array shaped ``(N, n_components + 1 [+1])`` produced by ``transform``.\n\n        Returns:\n            Pandas Series of reconstructed continuous values.\n        \"\"\"\n        # data shape: (N, n_components + 1 [+1])\n\n        current_idx = 0\n\n        # 1. Cluster One-Hot\n        cluster_one_hot = data[:, current_idx : current_idx + self.n_components]\n        current_idx += self.n_components\n\n        # 2. Scalar\n        scalars = data[:, current_idx]\n        current_idx += 1\n\n        # 3. Null Indicator\n        if self.has_nulls:\n            null_indicators = data[:, current_idx]\n            current_idx += 1\n        else:\n            null_indicators = None\n\n        # Identify cluster\n        cluster_assignments = np.argmax(cluster_one_hot, axis=1)\n\n        means = self.means[cluster_assignments]\n        stds = self.stds[cluster_assignments]\n\n        # Reconstruct: x = v * 4 * sigma_c + mu_c\n        reconstructed_values = scalars * 4 * stds + means\n\n        # Apply Null Masking\n        if null_indicators is not None:\n            # If null indicator &gt; 0.5 (generated as sigmoid usually, but here just boolean/float)\n            # Generator should output something close to 0 or 1.\n            # We assume threshold 0.5\n            is_null = null_indicators &gt; 0.5\n            reconstructed_values[is_null] = np.nan\n\n        return pd.Series(reconstructed_values)\n</code></pre>"},{"location":"api/core/#syntho_hive.core.data.transformer.ClusterBasedNormalizer.fit","title":"fit","text":"<pre><code>fit(data: Series)\n</code></pre> <p>Fit the Bayesian GMM on a continuous series.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series</code> <p>Continuous pandas Series to normalize.</p> required Source code in <code>syntho_hive/core/data/transformer.py</code> <pre><code>def fit(self, data: pd.Series):\n    \"\"\"Fit the Bayesian GMM on a continuous series.\n\n    Args:\n        data: Continuous pandas Series to normalize.\n    \"\"\"\n    # 1. Handle Nulls\n    self.has_nulls = data.isnull().any()\n    if self.has_nulls:\n        self.fill_value = data.mean()\n        # Impute for training GMM\n        values = data.fillna(self.fill_value).to_numpy(dtype=float).reshape(-1, 1)\n        self.output_dim = self.n_components + 1 + 1 # +1 for null indicator\n    else:\n        values = data.to_numpy(dtype=float).reshape(-1, 1)\n        self.output_dim = self.n_components + 1\n\n    self.model.fit(values)\n    self.means = self.model.means_.flatten() # (n_components,)\n    self.stds = np.sqrt(self.model.covariances_).flatten() # (n_components,)\n</code></pre>"},{"location":"api/core/#syntho_hive.core.data.transformer.ClusterBasedNormalizer.inverse_transform","title":"inverse_transform","text":"<pre><code>inverse_transform(data: ndarray) -&gt; pd.Series\n</code></pre> <p>Reconstruct approximate original values from normalized representation.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Array shaped <code>(N, n_components + 1 [+1])</code> produced by <code>transform</code>.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Pandas Series of reconstructed continuous values.</p> Source code in <code>syntho_hive/core/data/transformer.py</code> <pre><code>def inverse_transform(self, data: np.ndarray) -&gt; pd.Series:\n    \"\"\"Reconstruct approximate original values from normalized representation.\n\n    Args:\n        data: Array shaped ``(N, n_components + 1 [+1])`` produced by ``transform``.\n\n    Returns:\n        Pandas Series of reconstructed continuous values.\n    \"\"\"\n    # data shape: (N, n_components + 1 [+1])\n\n    current_idx = 0\n\n    # 1. Cluster One-Hot\n    cluster_one_hot = data[:, current_idx : current_idx + self.n_components]\n    current_idx += self.n_components\n\n    # 2. Scalar\n    scalars = data[:, current_idx]\n    current_idx += 1\n\n    # 3. Null Indicator\n    if self.has_nulls:\n        null_indicators = data[:, current_idx]\n        current_idx += 1\n    else:\n        null_indicators = None\n\n    # Identify cluster\n    cluster_assignments = np.argmax(cluster_one_hot, axis=1)\n\n    means = self.means[cluster_assignments]\n    stds = self.stds[cluster_assignments]\n\n    # Reconstruct: x = v * 4 * sigma_c + mu_c\n    reconstructed_values = scalars * 4 * stds + means\n\n    # Apply Null Masking\n    if null_indicators is not None:\n        # If null indicator &gt; 0.5 (generated as sigmoid usually, but here just boolean/float)\n        # Generator should output something close to 0 or 1.\n        # We assume threshold 0.5\n        is_null = null_indicators &gt; 0.5\n        reconstructed_values[is_null] = np.nan\n\n    return pd.Series(reconstructed_values)\n</code></pre>"},{"location":"api/core/#syntho_hive.core.data.transformer.ClusterBasedNormalizer.transform","title":"transform","text":"<pre><code>transform(data: Series) -&gt; np.ndarray\n</code></pre> <p>Project values to one-hot cluster assignment and normalized scalar.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series</code> <p>Continuous pandas Series to transform.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Numpy array of shape <code>(N, n_components + 1 [+1])</code> with one-hot cluster, scaled value, [null_ind].</p> Source code in <code>syntho_hive/core/data/transformer.py</code> <pre><code>def transform(self, data: pd.Series) -&gt; np.ndarray:\n    \"\"\"Project values to one-hot cluster assignment and normalized scalar.\n\n    Args:\n        data: Continuous pandas Series to transform.\n\n    Returns:\n        Numpy array of shape ``(N, n_components + 1 [+1])`` with one-hot cluster, scaled value, [null_ind].\n    \"\"\"\n    values_raw = data.to_numpy(dtype=float).reshape(-1, 1)\n    n_samples = len(values_raw)\n\n    if self.has_nulls:\n        # 0. Create Null Indicator\n        null_indicator = pd.isnull(data).to_numpy(dtype=float).reshape(-1, 1)\n\n        # 1. Impute for projection\n        values_clean = data.fillna(self.fill_value).to_numpy(dtype=float).reshape(-1, 1)\n    else:\n        values_clean = values_raw\n\n    # 2. Get cluster probabilities: P(c|x)\n    probs = self.model.predict_proba(values_clean) # (N, n_components)\n\n    # 2. Sample component c ~ P(c|x) (Argmax for simplicity/determinism in this impl)\n    # CTGAN uses argmax during interaction but sampling during training prep sometimes. \n    # Using argmax is stable.\n    # 3. Sample component c ~ P(c|x) (Argmax for simplicity/determinism in this impl)\n    # CTGAN uses argmax during interaction but sampling during training prep sometimes. \n    # Using argmax is stable.\n    cluster_assignments = np.argmax(probs, axis=1)\n\n    # 4. Calculate normalized scalar: v = (x - mu_c) / (4 * sigma_c)\n    # Clip to [-1, 1] usually, or roughly there.\n    means = self.means[cluster_assignments]\n    stds = self.stds[cluster_assignments]\n\n    normalized_values = (values_clean.flatten() - means) / (4 * stds)\n    normalized_values = normalized_values.reshape(-1, 1)\n\n    # 5. Create One-Hot encoding of cluster assignment\n    cluster_one_hot = np.zeros((n_samples, self.n_components))\n    cluster_one_hot[np.arange(n_samples), cluster_assignments] = 1\n\n    # Output: [one_hot_cluster, scalar, (null_indicator)]\n    if self.has_nulls:\n        return np.concatenate([cluster_one_hot, normalized_values, null_indicator], axis=1)\n    else:\n        return np.concatenate([cluster_one_hot, normalized_values], axis=1)\n</code></pre>"},{"location":"api/interface/","title":"Interface &amp; Config","text":""},{"location":"api/interface/#syntho_hive.interface.synthesizer.Synthesizer","title":"syntho_hive.interface.synthesizer.Synthesizer","text":"<p>Main entry point that wires metadata, privacy, and orchestration.</p> Source code in <code>syntho_hive/interface/synthesizer.py</code> <pre><code>class Synthesizer:\n    \"\"\"Main entry point that wires metadata, privacy, and orchestration.\"\"\"\n    def __init__(\n        self,\n        metadata: Metadata,\n        privacy_config: PrivacyConfig,\n        spark_session: Optional[SparkSession] = None,\n        backend: str = \"CTGAN\",\n        embedding_threshold: int = 50\n    ):\n        \"\"\"Instantiate the synthesizer fa\u00e7ade.\n\n        Args:\n            metadata: Dataset schema and relational configuration.\n            privacy_config: Privacy guardrail configuration.\n            spark_session: Optional SparkSession required for orchestration.\n            backend: Synthesis backend identifier (currently CTGAN).\n            embedding_threshold: Cardinality threshold for switching to embeddings.\n        \"\"\"\n        self.metadata = metadata\n        self.privacy = privacy_config\n        self.spark = spark_session\n        self.backend = backend\n        self.embedding_threshold = embedding_threshold\n\n        # Initialize internal components\n        if self.spark:\n            self.orchestrator = StagedOrchestrator(metadata, self.spark)\n        else:\n            self.orchestrator = None # Mode without Spark (maybe local pandas only in future)\n\n    def fit(\n        self,\n        data: Any, # Str (database name) or Dict[str, str] (table paths)\n        sampling_strategy: str = \"relational_stratified\",\n        sample_size: int = 5_000_000,\n        validate: bool = False,\n        epochs: int = 300,\n        batch_size: int = 500,\n        **model_kwargs: Union[int, str, Tuple[int, int]]\n    ):\n        \"\"\"Fit the generative models on the real database.\n\n        Args:\n            data: Database name (str) or mapping of {table: path} (dict).\n            sampling_strategy: Strategy for sampling real data.\n            sample_size: Number of rows to sample from real data (approx).\n            validate: Whether to run validation after fitting.\n            epochs: Number of training epochs for CTGAN.\n            batch_size: Batch size for training.\n            **model_kwargs: Additional args forwarded to the underlying model (e.g., embedding_dim).\n\n        Raises:\n            SchemaError: If the data argument is invalid.\n            TrainingError: If training fails for any reason.\n        \"\"\"\n        try:\n            if not self.orchestrator:\n                raise ValueError(\"SparkSession required for fit()\")\n\n            if sample_size &lt;= 0:\n                raise ValueError(\"sample_size must be positive\")\n\n            print(f\"Fitting on data source with {sampling_strategy} (target: {sample_size} rows)...\")\n            print(f\"Training Config: epochs={epochs}, batch_size={batch_size}\")\n\n            # Determine paths\n            if isinstance(data, str):\n                real_paths = {t: f\"{data}.{t}\" for t in self.metadata.tables}\n            elif isinstance(data, dict):\n                real_paths = data\n            else:\n                raise SchemaError(\n                    f\"fit() argument 'data' must be a database name (str) or path mapping (dict), \"\n                    f\"got {type(data).__name__}.\"\n                )\n\n            self.orchestrator.fit_all(real_paths, epochs=epochs, batch_size=batch_size, **model_kwargs)\n        except SynthoHiveError:\n            raise\n        except Exception as exc:\n            log.error(\"fit_failed\", error=str(exc))\n            raise TrainingError(\n                f\"fit() failed. Original error: {exc}\"\n            ) from exc\n\n    def sample(self, num_rows: Dict[str, int], output_format: str = \"delta\", output_path: Optional[str] = None) -&gt; Union[Dict[str, str], Dict[str, pd.DataFrame]]:\n        \"\"\"Generate synthetic data for each table.\n\n        Args:\n            num_rows: Mapping of table name to number of rows to generate.\n            output_format: Storage format for generated datasets (default ``\"delta\"``).\n            output_path: Optional path to write files. If None, returns DataFrames in memory.\n\n        Raises:\n            TrainingError: If generation fails for any reason.\n\n        Returns:\n            Mapping of table name to the output path (if wrote to disk) OR Dictionary of DataFrames (if in-memory).\n        \"\"\"\n        try:\n            if not self.orchestrator:\n                raise ValueError(\"SparkSession required for sample()\")\n\n            print(f\"Generating data with {self.backend} backend...\")\n\n            # If output_path is explicitly None, we return DataFrames\n            if output_path is None:\n                 return self.orchestrator.generate(num_rows, output_path_base=None)\n\n            output_base = output_path\n            self.orchestrator.generate(num_rows, output_base)\n\n            # Return paths mapping\n            return {t: f\"{output_base}/{t}\" for t in self.metadata.tables}\n        except SynthoHiveError:\n            raise\n        except Exception as exc:\n            log.error(\"sample_failed\", error=str(exc))\n            raise TrainingError(\n                f\"sample() failed. Original error: {exc}\"\n            ) from exc\n\n    def save(self, path: str) -&gt; None:\n        \"\"\"Persist the synthesizer state to disk.\n\n        Args:\n            path: Filesystem path to write the synthesizer checkpoint to.\n\n        Raises:\n            SerializationError: If saving fails for any reason.\n        \"\"\"\n        try:\n            import joblib\n            joblib.dump(self, path)\n            log.info(\"synthesizer_saved\", path=path)\n        except SynthoHiveError:\n            raise\n        except Exception as exc:\n            log.error(\"save_failed\", path=path, error=str(exc))\n            raise SerializationError(\n                f\"save() failed writing synthesizer to '{path}'. Original error: {exc}\"\n            ) from exc\n\n    @classmethod\n    def load(cls, path: str) -&gt; \"Synthesizer\":\n        \"\"\"Load a synthesizer from a previously saved checkpoint.\n\n        Args:\n            path: Filesystem path to the synthesizer checkpoint.\n\n        Raises:\n            SerializationError: If loading fails for any reason.\n\n        Returns:\n            Loaded Synthesizer instance.\n        \"\"\"\n        try:\n            import joblib\n            instance = joblib.load(path)\n            log.info(\"synthesizer_loaded\", path=path)\n            return instance\n        except SynthoHiveError:\n            raise\n        except Exception as exc:\n            log.error(\"load_failed\", path=path, error=str(exc))\n            raise SerializationError(\n                f\"load() failed reading synthesizer from '{path}'. Original error: {exc}\"\n            ) from exc\n\n    def generate_validation_report(self, real_data: Dict[str, str], synthetic_data: Dict[str, str], output_path: str):\n        \"\"\"Generate a validation report comparing real vs synthetic datasets.\n\n        Args:\n            real_data: Map of table name to real dataset path/table.\n            synthetic_data: Map of table name to generated dataset path.\n            output_path: Filesystem path for the rendered report.\n\n        Raises:\n            SynthoHiveError: If the report generation fails for any reason.\n        \"\"\"\n        try:\n            if not self.spark:\n                 raise ValueError(\"SparkSession required for validation report generation\")\n\n            print(\"Generating validation report...\")\n            report_gen = ValidationReport()\n\n            real_dfs = {}\n            synth_dfs = {}\n\n            # 1. Load Real Data\n            for table, path in real_data.items():\n                print(f\"Loading real data for {table} from {path}...\")\n                # Try reading as table first, then path\n                try:\n                    df = self.spark.read.table(path)\n                except Exception as exc:\n                    log.warning(\"delta_read_fallback_failed\", error=str(exc))\n                    raise SerializationError(\n                        f\"generate_validation_report() failed reading synthetic data. \"\n                        f\"Original error: {exc}\"\n                    ) from exc\n\n                real_dfs[table] = df.toPandas()\n\n            # 2. Load Synthetic Data\n            for table, path in synthetic_data.items():\n                print(f\"Loading synthetic data for {table} from {path}...\")\n                df = self.spark.read.format(\"delta\").load(path)\n                synth_dfs[table] = df.toPandas()\n\n            # 3. Generate Report\n            report_gen.generate(real_dfs, synth_dfs, output_path)\n        except SynthoHiveError:\n            raise\n        except Exception as exc:\n            log.error(\"generate_validation_report_failed\", output_path=output_path, error=str(exc))\n            raise SynthoHiveError(\n                f\"generate_validation_report() failed. Original error: {exc}\"\n            ) from exc\n\n    def save_to_hive(self, synthetic_data: Dict[str, str], target_db: str, overwrite: bool = True):\n        \"\"\"Register generated datasets as Hive tables.\n\n        Args:\n            synthetic_data: Map of table name to generated dataset path.\n            target_db: Hive database where tables should be registered.\n            overwrite: Whether to drop and recreate existing tables.\n\n        Raises:\n            ValueError: If Spark is unavailable.\n        \"\"\"\n        if not self.spark:\n            raise ValueError(\"SparkSession required for Hive registration\")\n\n        # Validate database name against allowlist before any SQL interpolation.\n        # Raises SchemaError immediately \u2014 no Spark context touched for invalid names.\n        if not _SAFE_IDENTIFIER.match(target_db):\n            raise SchemaError(\n                f\"SchemaError: Database name '{target_db}' contains invalid characters. \"\n                f\"Only letters, digits, and underscores [a-zA-Z0-9_] are allowed. \"\n                f\"This validation prevents SQL injection via unsanitized user input.\"\n            )\n\n        # Validate table names from synthetic_data keys\n        for table_name in synthetic_data:\n            if not _SAFE_IDENTIFIER.match(str(table_name)):\n                raise SchemaError(\n                    f\"SchemaError: Table name '{table_name}' contains invalid characters. \"\n                    f\"Only letters, digits, and underscores [a-zA-Z0-9_] are allowed.\"\n                )\n\n        print(f\"Save to Hive database: {target_db}\")\n\n        # Ensure DB exists\n        self.spark.sql(f\"CREATE DATABASE IF NOT EXISTS {target_db}\")\n\n        for table, path in synthetic_data.items():\n            full_table_name = f\"{target_db}.{table}\"\n            print(f\"Registering table {full_table_name} at {path}\")\n\n            if overwrite:\n                self.spark.sql(f\"DROP TABLE IF EXISTS {full_table_name}\")\n\n            # Register External Table\n            self.spark.sql(f\"CREATE TABLE {full_table_name} USING DELTA LOCATION '{path}'\")\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.synthesizer.Synthesizer.fit","title":"fit","text":"<pre><code>fit(data: Any, sampling_strategy: str = 'relational_stratified', sample_size: int = 5000000, validate: bool = False, epochs: int = 300, batch_size: int = 500, **model_kwargs: Union[int, str, Tuple[int, int]])\n</code></pre> <p>Fit the generative models on the real database.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Database name (str) or mapping of {table: path} (dict).</p> required <code>sampling_strategy</code> <code>str</code> <p>Strategy for sampling real data.</p> <code>'relational_stratified'</code> <code>sample_size</code> <code>int</code> <p>Number of rows to sample from real data (approx).</p> <code>5000000</code> <code>validate</code> <code>bool</code> <p>Whether to run validation after fitting.</p> <code>False</code> <code>epochs</code> <code>int</code> <p>Number of training epochs for CTGAN.</p> <code>300</code> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>500</code> <code>**model_kwargs</code> <code>Union[int, str, Tuple[int, int]]</code> <p>Additional args forwarded to the underlying model (e.g., embedding_dim).</p> <code>{}</code> <p>Raises:</p> Type Description <code>SchemaError</code> <p>If the data argument is invalid.</p> <code>TrainingError</code> <p>If training fails for any reason.</p> Source code in <code>syntho_hive/interface/synthesizer.py</code> <pre><code>def fit(\n    self,\n    data: Any, # Str (database name) or Dict[str, str] (table paths)\n    sampling_strategy: str = \"relational_stratified\",\n    sample_size: int = 5_000_000,\n    validate: bool = False,\n    epochs: int = 300,\n    batch_size: int = 500,\n    **model_kwargs: Union[int, str, Tuple[int, int]]\n):\n    \"\"\"Fit the generative models on the real database.\n\n    Args:\n        data: Database name (str) or mapping of {table: path} (dict).\n        sampling_strategy: Strategy for sampling real data.\n        sample_size: Number of rows to sample from real data (approx).\n        validate: Whether to run validation after fitting.\n        epochs: Number of training epochs for CTGAN.\n        batch_size: Batch size for training.\n        **model_kwargs: Additional args forwarded to the underlying model (e.g., embedding_dim).\n\n    Raises:\n        SchemaError: If the data argument is invalid.\n        TrainingError: If training fails for any reason.\n    \"\"\"\n    try:\n        if not self.orchestrator:\n            raise ValueError(\"SparkSession required for fit()\")\n\n        if sample_size &lt;= 0:\n            raise ValueError(\"sample_size must be positive\")\n\n        print(f\"Fitting on data source with {sampling_strategy} (target: {sample_size} rows)...\")\n        print(f\"Training Config: epochs={epochs}, batch_size={batch_size}\")\n\n        # Determine paths\n        if isinstance(data, str):\n            real_paths = {t: f\"{data}.{t}\" for t in self.metadata.tables}\n        elif isinstance(data, dict):\n            real_paths = data\n        else:\n            raise SchemaError(\n                f\"fit() argument 'data' must be a database name (str) or path mapping (dict), \"\n                f\"got {type(data).__name__}.\"\n            )\n\n        self.orchestrator.fit_all(real_paths, epochs=epochs, batch_size=batch_size, **model_kwargs)\n    except SynthoHiveError:\n        raise\n    except Exception as exc:\n        log.error(\"fit_failed\", error=str(exc))\n        raise TrainingError(\n            f\"fit() failed. Original error: {exc}\"\n        ) from exc\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.synthesizer.Synthesizer.generate_validation_report","title":"generate_validation_report","text":"<pre><code>generate_validation_report(real_data: Dict[str, str], synthetic_data: Dict[str, str], output_path: str)\n</code></pre> <p>Generate a validation report comparing real vs synthetic datasets.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>Dict[str, str]</code> <p>Map of table name to real dataset path/table.</p> required <code>synthetic_data</code> <code>Dict[str, str]</code> <p>Map of table name to generated dataset path.</p> required <code>output_path</code> <code>str</code> <p>Filesystem path for the rendered report.</p> required <p>Raises:</p> Type Description <code>SynthoHiveError</code> <p>If the report generation fails for any reason.</p> Source code in <code>syntho_hive/interface/synthesizer.py</code> <pre><code>def generate_validation_report(self, real_data: Dict[str, str], synthetic_data: Dict[str, str], output_path: str):\n    \"\"\"Generate a validation report comparing real vs synthetic datasets.\n\n    Args:\n        real_data: Map of table name to real dataset path/table.\n        synthetic_data: Map of table name to generated dataset path.\n        output_path: Filesystem path for the rendered report.\n\n    Raises:\n        SynthoHiveError: If the report generation fails for any reason.\n    \"\"\"\n    try:\n        if not self.spark:\n             raise ValueError(\"SparkSession required for validation report generation\")\n\n        print(\"Generating validation report...\")\n        report_gen = ValidationReport()\n\n        real_dfs = {}\n        synth_dfs = {}\n\n        # 1. Load Real Data\n        for table, path in real_data.items():\n            print(f\"Loading real data for {table} from {path}...\")\n            # Try reading as table first, then path\n            try:\n                df = self.spark.read.table(path)\n            except Exception as exc:\n                log.warning(\"delta_read_fallback_failed\", error=str(exc))\n                raise SerializationError(\n                    f\"generate_validation_report() failed reading synthetic data. \"\n                    f\"Original error: {exc}\"\n                ) from exc\n\n            real_dfs[table] = df.toPandas()\n\n        # 2. Load Synthetic Data\n        for table, path in synthetic_data.items():\n            print(f\"Loading synthetic data for {table} from {path}...\")\n            df = self.spark.read.format(\"delta\").load(path)\n            synth_dfs[table] = df.toPandas()\n\n        # 3. Generate Report\n        report_gen.generate(real_dfs, synth_dfs, output_path)\n    except SynthoHiveError:\n        raise\n    except Exception as exc:\n        log.error(\"generate_validation_report_failed\", output_path=output_path, error=str(exc))\n        raise SynthoHiveError(\n            f\"generate_validation_report() failed. Original error: {exc}\"\n        ) from exc\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.synthesizer.Synthesizer.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(path: str) -&gt; Synthesizer\n</code></pre> <p>Load a synthesizer from a previously saved checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Filesystem path to the synthesizer checkpoint.</p> required <p>Raises:</p> Type Description <code>SerializationError</code> <p>If loading fails for any reason.</p> <p>Returns:</p> Type Description <code>Synthesizer</code> <p>Loaded Synthesizer instance.</p> Source code in <code>syntho_hive/interface/synthesizer.py</code> <pre><code>@classmethod\ndef load(cls, path: str) -&gt; \"Synthesizer\":\n    \"\"\"Load a synthesizer from a previously saved checkpoint.\n\n    Args:\n        path: Filesystem path to the synthesizer checkpoint.\n\n    Raises:\n        SerializationError: If loading fails for any reason.\n\n    Returns:\n        Loaded Synthesizer instance.\n    \"\"\"\n    try:\n        import joblib\n        instance = joblib.load(path)\n        log.info(\"synthesizer_loaded\", path=path)\n        return instance\n    except SynthoHiveError:\n        raise\n    except Exception as exc:\n        log.error(\"load_failed\", path=path, error=str(exc))\n        raise SerializationError(\n            f\"load() failed reading synthesizer from '{path}'. Original error: {exc}\"\n        ) from exc\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.synthesizer.Synthesizer.sample","title":"sample","text":"<pre><code>sample(num_rows: Dict[str, int], output_format: str = 'delta', output_path: Optional[str] = None) -&gt; Union[Dict[str, str], Dict[str, pd.DataFrame]]\n</code></pre> <p>Generate synthetic data for each table.</p> <p>Parameters:</p> Name Type Description Default <code>num_rows</code> <code>Dict[str, int]</code> <p>Mapping of table name to number of rows to generate.</p> required <code>output_format</code> <code>str</code> <p>Storage format for generated datasets (default <code>\"delta\"</code>).</p> <code>'delta'</code> <code>output_path</code> <code>Optional[str]</code> <p>Optional path to write files. If None, returns DataFrames in memory.</p> <code>None</code> <p>Raises:</p> Type Description <code>TrainingError</code> <p>If generation fails for any reason.</p> <p>Returns:</p> Type Description <code>Union[Dict[str, str], Dict[str, DataFrame]]</code> <p>Mapping of table name to the output path (if wrote to disk) OR Dictionary of DataFrames (if in-memory).</p> Source code in <code>syntho_hive/interface/synthesizer.py</code> <pre><code>def sample(self, num_rows: Dict[str, int], output_format: str = \"delta\", output_path: Optional[str] = None) -&gt; Union[Dict[str, str], Dict[str, pd.DataFrame]]:\n    \"\"\"Generate synthetic data for each table.\n\n    Args:\n        num_rows: Mapping of table name to number of rows to generate.\n        output_format: Storage format for generated datasets (default ``\"delta\"``).\n        output_path: Optional path to write files. If None, returns DataFrames in memory.\n\n    Raises:\n        TrainingError: If generation fails for any reason.\n\n    Returns:\n        Mapping of table name to the output path (if wrote to disk) OR Dictionary of DataFrames (if in-memory).\n    \"\"\"\n    try:\n        if not self.orchestrator:\n            raise ValueError(\"SparkSession required for sample()\")\n\n        print(f\"Generating data with {self.backend} backend...\")\n\n        # If output_path is explicitly None, we return DataFrames\n        if output_path is None:\n             return self.orchestrator.generate(num_rows, output_path_base=None)\n\n        output_base = output_path\n        self.orchestrator.generate(num_rows, output_base)\n\n        # Return paths mapping\n        return {t: f\"{output_base}/{t}\" for t in self.metadata.tables}\n    except SynthoHiveError:\n        raise\n    except Exception as exc:\n        log.error(\"sample_failed\", error=str(exc))\n        raise TrainingError(\n            f\"sample() failed. Original error: {exc}\"\n        ) from exc\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.synthesizer.Synthesizer.save","title":"save","text":"<pre><code>save(path: str) -&gt; None\n</code></pre> <p>Persist the synthesizer state to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Filesystem path to write the synthesizer checkpoint to.</p> required <p>Raises:</p> Type Description <code>SerializationError</code> <p>If saving fails for any reason.</p> Source code in <code>syntho_hive/interface/synthesizer.py</code> <pre><code>def save(self, path: str) -&gt; None:\n    \"\"\"Persist the synthesizer state to disk.\n\n    Args:\n        path: Filesystem path to write the synthesizer checkpoint to.\n\n    Raises:\n        SerializationError: If saving fails for any reason.\n    \"\"\"\n    try:\n        import joblib\n        joblib.dump(self, path)\n        log.info(\"synthesizer_saved\", path=path)\n    except SynthoHiveError:\n        raise\n    except Exception as exc:\n        log.error(\"save_failed\", path=path, error=str(exc))\n        raise SerializationError(\n            f\"save() failed writing synthesizer to '{path}'. Original error: {exc}\"\n        ) from exc\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.synthesizer.Synthesizer.save_to_hive","title":"save_to_hive","text":"<pre><code>save_to_hive(synthetic_data: Dict[str, str], target_db: str, overwrite: bool = True)\n</code></pre> <p>Register generated datasets as Hive tables.</p> <p>Parameters:</p> Name Type Description Default <code>synthetic_data</code> <code>Dict[str, str]</code> <p>Map of table name to generated dataset path.</p> required <code>target_db</code> <code>str</code> <p>Hive database where tables should be registered.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to drop and recreate existing tables.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If Spark is unavailable.</p> Source code in <code>syntho_hive/interface/synthesizer.py</code> <pre><code>def save_to_hive(self, synthetic_data: Dict[str, str], target_db: str, overwrite: bool = True):\n    \"\"\"Register generated datasets as Hive tables.\n\n    Args:\n        synthetic_data: Map of table name to generated dataset path.\n        target_db: Hive database where tables should be registered.\n        overwrite: Whether to drop and recreate existing tables.\n\n    Raises:\n        ValueError: If Spark is unavailable.\n    \"\"\"\n    if not self.spark:\n        raise ValueError(\"SparkSession required for Hive registration\")\n\n    # Validate database name against allowlist before any SQL interpolation.\n    # Raises SchemaError immediately \u2014 no Spark context touched for invalid names.\n    if not _SAFE_IDENTIFIER.match(target_db):\n        raise SchemaError(\n            f\"SchemaError: Database name '{target_db}' contains invalid characters. \"\n            f\"Only letters, digits, and underscores [a-zA-Z0-9_] are allowed. \"\n            f\"This validation prevents SQL injection via unsanitized user input.\"\n        )\n\n    # Validate table names from synthetic_data keys\n    for table_name in synthetic_data:\n        if not _SAFE_IDENTIFIER.match(str(table_name)):\n            raise SchemaError(\n                f\"SchemaError: Table name '{table_name}' contains invalid characters. \"\n                f\"Only letters, digits, and underscores [a-zA-Z0-9_] are allowed.\"\n            )\n\n    print(f\"Save to Hive database: {target_db}\")\n\n    # Ensure DB exists\n    self.spark.sql(f\"CREATE DATABASE IF NOT EXISTS {target_db}\")\n\n    for table, path in synthetic_data.items():\n        full_table_name = f\"{target_db}.{table}\"\n        print(f\"Registering table {full_table_name} at {path}\")\n\n        if overwrite:\n            self.spark.sql(f\"DROP TABLE IF EXISTS {full_table_name}\")\n\n        # Register External Table\n        self.spark.sql(f\"CREATE TABLE {full_table_name} USING DELTA LOCATION '{path}'\")\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.config.Metadata","title":"syntho_hive.interface.config.Metadata","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema definition for the entire dataset.</p> Source code in <code>syntho_hive/interface/config.py</code> <pre><code>class Metadata(BaseModel):\n    \"\"\"Schema definition for the entire dataset.\"\"\"\n    tables: Dict[str, TableConfig] = Field(default_factory=dict)\n\n    def add_table(self, name: str, pk: str, **kwargs: Union[List[str], Dict[str, str], Dict[str, Constraint]]):\n        \"\"\"Register a table configuration.\n\n        Args:\n            name: Table name.\n            pk: Primary key column name.\n            **kwargs: Additional fields to populate ``TableConfig``.\n\n        Raises:\n            ValueError: If a table with the same name already exists.\n        \"\"\"\n        if name in self.tables:\n             raise ValueError(f\"Table '{name}' already exists in metadata.\")\n        self.tables[name] = TableConfig(name=name, pk=pk, **kwargs)\n\n    def get_table(self, name: str) -&gt; Optional[TableConfig]:\n        \"\"\"Fetch a table configuration by name.\n\n        Args:\n            name: Table name to retrieve.\n\n        Returns:\n            Corresponding ``TableConfig`` or ``None`` if missing.\n        \"\"\"\n        return self.tables.get(name)\n\n    def validate_schema(self):\n        \"\"\"Validate schema integrity, focusing on foreign key references.\n\n        Raises:\n            ValueError: When an FK reference is malformed or targets a missing table.\n        \"\"\"\n        for table_name, table_config in self.tables.items():\n            for local_col, parent_ref in table_config.fk.items():\n                if \".\" not in parent_ref:\n                    raise ValueError(f\"Invalid FK reference '{parent_ref}' in table '{table_name}'. Format should be 'parent_table.parent_col'.\")\n\n                parent_table, parent_col = parent_ref.split(\".\", 1)\n\n                if parent_table not in self.tables:\n                    raise ValueError(f\"Table '{table_name}' references non-existent parent table '{parent_table}'.\")\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.config.Metadata.add_table","title":"add_table","text":"<pre><code>add_table(name: str, pk: str, **kwargs: Union[List[str], Dict[str, str], Dict[str, Constraint]])\n</code></pre> <p>Register a table configuration.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Table name.</p> required <code>pk</code> <code>str</code> <p>Primary key column name.</p> required <code>**kwargs</code> <code>Union[List[str], Dict[str, str], Dict[str, Constraint]]</code> <p>Additional fields to populate <code>TableConfig</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a table with the same name already exists.</p> Source code in <code>syntho_hive/interface/config.py</code> <pre><code>def add_table(self, name: str, pk: str, **kwargs: Union[List[str], Dict[str, str], Dict[str, Constraint]]):\n    \"\"\"Register a table configuration.\n\n    Args:\n        name: Table name.\n        pk: Primary key column name.\n        **kwargs: Additional fields to populate ``TableConfig``.\n\n    Raises:\n        ValueError: If a table with the same name already exists.\n    \"\"\"\n    if name in self.tables:\n         raise ValueError(f\"Table '{name}' already exists in metadata.\")\n    self.tables[name] = TableConfig(name=name, pk=pk, **kwargs)\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.config.Metadata.get_table","title":"get_table","text":"<pre><code>get_table(name: str) -&gt; Optional[TableConfig]\n</code></pre> <p>Fetch a table configuration by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Table name to retrieve.</p> required <p>Returns:</p> Type Description <code>Optional[TableConfig]</code> <p>Corresponding <code>TableConfig</code> or <code>None</code> if missing.</p> Source code in <code>syntho_hive/interface/config.py</code> <pre><code>def get_table(self, name: str) -&gt; Optional[TableConfig]:\n    \"\"\"Fetch a table configuration by name.\n\n    Args:\n        name: Table name to retrieve.\n\n    Returns:\n        Corresponding ``TableConfig`` or ``None`` if missing.\n    \"\"\"\n    return self.tables.get(name)\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.config.Metadata.validate_schema","title":"validate_schema","text":"<pre><code>validate_schema()\n</code></pre> <p>Validate schema integrity, focusing on foreign key references.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When an FK reference is malformed or targets a missing table.</p> Source code in <code>syntho_hive/interface/config.py</code> <pre><code>def validate_schema(self):\n    \"\"\"Validate schema integrity, focusing on foreign key references.\n\n    Raises:\n        ValueError: When an FK reference is malformed or targets a missing table.\n    \"\"\"\n    for table_name, table_config in self.tables.items():\n        for local_col, parent_ref in table_config.fk.items():\n            if \".\" not in parent_ref:\n                raise ValueError(f\"Invalid FK reference '{parent_ref}' in table '{table_name}'. Format should be 'parent_table.parent_col'.\")\n\n            parent_table, parent_col = parent_ref.split(\".\", 1)\n\n            if parent_table not in self.tables:\n                raise ValueError(f\"Table '{table_name}' references non-existent parent table '{parent_table}'.\")\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.config.TableConfig","title":"syntho_hive.interface.config.TableConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a single table, including keys and constraints.</p> Source code in <code>syntho_hive/interface/config.py</code> <pre><code>class TableConfig(BaseModel):\n    \"\"\"Configuration for a single table, including keys and constraints.\"\"\"\n    name: str\n    pk: str\n    pii_cols: List[str] = Field(default_factory=list)\n    high_cardinality_cols: List[str] = Field(default_factory=list)\n    fk: Dict[str, str] = Field(default_factory=dict, description=\"Map of local_col -&gt; parent_table.parent_col\")\n    parent_context_cols: List[str] = Field(default_factory=list, description=\"List of parent attributes to condition on (e.g., 'users.region')\")\n    constraints: Dict[str, Constraint] = Field(default_factory=dict, description=\"Map of col_name -&gt; Constraint\")\n\n    @property\n    def has_dependencies(self) -&gt; bool:\n        \"\"\"Whether the table declares any foreign key dependencies.\"\"\"\n        return bool(self.fk)\n</code></pre>"},{"location":"api/interface/#syntho_hive.interface.config.TableConfig.has_dependencies","title":"has_dependencies  <code>property</code>","text":"<pre><code>has_dependencies: bool\n</code></pre> <p>Whether the table declares any foreign key dependencies.</p>"},{"location":"api/interface/#syntho_hive.interface.config.PrivacyConfig","title":"syntho_hive.interface.config.PrivacyConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for privacy guardrails applied during synthesis.</p> Source code in <code>syntho_hive/interface/config.py</code> <pre><code>class PrivacyConfig(BaseModel):\n    \"\"\"Configuration for privacy guardrails applied during synthesis.\"\"\"\n    enable_differential_privacy: bool = False\n    epsilon: float = 1.0\n    pii_strategy: Literal[\"mask\", \"faker\", \"context_aware_faker\"] = \"context_aware_faker\"\n    k_anonymity_threshold: int = 5\n    pii_columns: List[str] = Field(default_factory=list)\n</code></pre>"},{"location":"api/interface/#syntho_hive.validation.report_generator.ValidationReport","title":"syntho_hive.validation.report_generator.ValidationReport","text":"<p>Generate summary reports of validation metrics.</p> Source code in <code>syntho_hive/validation/report_generator.py</code> <pre><code>class ValidationReport:\n    \"\"\"Generate summary reports of validation metrics.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize statistical validator and metric store.\"\"\"\n        self.validator = StatisticalValidator()\n        self.metrics = {}\n\n    def _calculate_detailed_stats(self, real_df: pd.DataFrame, synth_df: pd.DataFrame) -&gt; Dict[str, Any]:\n        \"\"\"Calculate descriptive statistics for side-by-side comparison.\n\n        Args:\n            real_df: Real dataframe.\n            synth_df: Synthetic dataframe aligned to the real columns.\n\n        Returns:\n            Nested dict of summary stats for each column.\n        \"\"\"\n        stats = {}\n        for col in real_df.columns:\n            if col not in synth_df.columns:\n                continue\n\n            col_stats = {\"real\": {}, \"synth\": {}}\n\n            for name, df, res in [(\"real\", real_df, col_stats[\"real\"]), (\"synth\", synth_df, col_stats[\"synth\"])]:\n                series = df[col]\n                if pd.api.types.is_numeric_dtype(series):\n                    res[\"mean\"] = series.mean()\n                    res[\"std\"] = series.std()\n                    res[\"min\"] = series.min()\n                    res[\"max\"] = series.max()\n                else:\n                    res[\"unique_count\"] = series.nunique()\n                    res[\"top_value\"] = series.mode().iloc[0] if not series.mode().empty else \"N/A\"\n                    res[\"top_freq\"] = series.value_counts().iloc[0] if not series.empty else 0\n\n            stats[col] = col_stats\n        return stats\n\n    def generate(self, real_data: Dict[str, pd.DataFrame], synth_data: Dict[str, pd.DataFrame], output_path: str):\n        \"\"\"Run validation and save a report.\n\n        Args:\n            real_data: Mapping of table name to real dataframe.\n            synth_data: Mapping of table name to synthetic dataframe.\n            output_path: Destination path for HTML or JSON report.\n        \"\"\"\n        report = {\n            \"tables\": {},\n            \"summary\": \"Validation Report\"\n        }\n\n        for table_name, real_df in real_data.items():\n            if table_name not in synth_data:\n                continue\n\n            synth_df = synth_data[table_name]\n\n            # 1. Column comparisons\n            col_metrics = self.validator.compare_columns(real_df, synth_df)\n\n            # 2. Correlation\n            corr_diff = self.validator.check_correlations(real_df, synth_df)\n\n            # 3. Detailed Stats\n            stats = self._calculate_detailed_stats(real_df, synth_df)\n\n            # 4. Data Preview\n            # Use Pandas to_html for easy formatting, strict constraints\n            preview = {\n                \"real_html\": real_df.head(10).to_html(index=False, classes='scroll-table', border=0),\n                \"synth_html\": synth_df.head(10).to_html(index=False, classes='scroll-table', border=0)\n            }\n\n            report[\"tables\"][table_name] = {\n                \"column_metrics\": col_metrics,\n                \"correlation_distance\": corr_diff,\n                \"detailed_stats\": stats,\n                \"preview\": preview\n            }\n\n        if output_path.endswith(\".html\"):\n            self._save_html(report, output_path)\n        else:\n            # Save to JSON for now (PDF requires more deps)\n            with open(output_path, \"w\") as f:\n                json.dump(report, f, indent=2, default=str)\n\n        import os\n        print(f\"Report saved to {os.path.abspath(output_path)}\")\n\n    def _save_html(self, report: Dict[str, Any], output_path: str):\n        \"\"\"Render a rich HTML report with metric explanations, stats, and previews.\n\n        Args:\n            report: Structured report dictionary produced by ``generate``.\n            output_path: Filesystem path to write the HTML file.\n        \"\"\"\n        html_content = [\n            \"\"\"&lt;html&gt;\n            &lt;head&gt;\n                &lt;style&gt;\n                    body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 20px; background-color: #f9f9f9; color: #333; }\n                    h1, h2, h3 { color: #2c3e50; }\n                    .container { max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }\n\n                    /* Tables */\n                    table { border-collapse: collapse; width: 100%; margin-bottom: 20px; font-size: 14px; }\n                    th, td { border: 1px solid #e1e4e8; padding: 10px; text-align: left; }\n                    th { background-color: #f1f8ff; color: #0366d6; font-weight: 600; }\n                    tr:nth-child(even) { background-color: #f8f9fa; }\n\n                    /* Status Colors */\n                    .pass { color: #28a745; font-weight: bold; }\n                    .fail { color: #dc3545; font-weight: bold; }\n\n                    /* Layout */\n                    .section { margin-top: 40px; border-top: 1px solid #eee; padding-top: 20px; }\n                    .metric-box { background: #f0f4f8; padding: 15px; border-radius: 5px; margin-bottom: 20px; border-left: 5px solid #0366d6; }\n                    .row { display: flex; gap: 20px; }\n                    .col { flex: 1; overflow-x: auto; }\n\n                    /* Tabs/Previews */\n                    .preview-header { font-weight: bold; margin-bottom: 10px; color: #555; }\n                    .scroll-table { max-height: 400px; overflow-y: auto; display: block; }\n                &lt;/style&gt;\n            &lt;/head&gt;\n            &lt;body&gt;\n            &lt;div class=\"container\"&gt;\n                &lt;h1&gt;Validation Report&lt;/h1&gt;\n\n                &lt;div class=\"metric-box\"&gt;\n                    &lt;h3&gt;Metric Explanations&lt;/h3&gt;\n                    &lt;ul&gt;\n                        &lt;li&gt;&lt;strong&gt;KS Test (Kolmogorov-Smirnov):&lt;/strong&gt; Used for continuous numerical columns. Compares the cumulative distribution functions of the real and synthetic data. &lt;br&gt;\n                            &lt;em&gt;Result:&lt;/em&gt; Returns a p-value. If p &gt; 0.05, we fail to reject the null hypothesis (i.e., distributions are likely the same).&lt;/li&gt;\n                        &lt;li&gt;&lt;strong&gt;TVD (Total Variation Distance):&lt;/strong&gt; Used for categorical or discrete columns. Measures the maximum difference between probabilities assigned to the same event by two distributions. &lt;br&gt;\n                            &lt;em&gt;Result:&lt;/em&gt; Value between 0 and 1. Lower is better (0 means identical). We consider &lt; 0.1 as passing.&lt;/li&gt;\n                        &lt;li&gt;&lt;strong&gt;Correlation Distance:&lt;/strong&gt; Measures how well the pairwise correlations between numerical columns are preserved. Calculated as the Frobenius norm of the difference between correlation matrices. &lt;br&gt;\n                            &lt;em&gt;Result:&lt;/em&gt; Lower is better (0 means identical correlation structure).&lt;/li&gt;\n                    &lt;/ul&gt;\n                &lt;/div&gt;\n            \"\"\"]\n\n        for table_name, data in report[\"tables\"].items():\n            html_content.append(f\"&lt;div class='section'&gt;&lt;h2&gt;Table: {table_name}&lt;/h2&gt;\")\n\n            # --- 1. Correlation &amp; Overall ---\n            corr_dist = data.get('correlation_distance', 0.0)\n            html_content.append(f\"&lt;p&gt;&lt;strong&gt;Correlation Distance:&lt;/strong&gt; {corr_dist:.4f}&lt;/p&gt;\")\n\n            # --- 2. Column Metrics ---\n            html_content.append(\"&lt;h3&gt;Column Validation Metrics&lt;/h3&gt;\")\n            html_content.append(\"&lt;table&gt;&lt;tr&gt;&lt;th&gt;Column&lt;/th&gt;&lt;th&gt;Test Type&lt;/th&gt;&lt;th&gt;Statistic&lt;/th&gt;&lt;th&gt;P-Value / Score&lt;/th&gt;&lt;th&gt;Status&lt;/th&gt;&lt;/tr&gt;\")\n\n            for col, metrics in data[\"column_metrics\"].items():\n                if \"error\" in metrics:\n                    html_content.append(f\"&lt;tr&gt;&lt;td&gt;{col}&lt;/td&gt;&lt;td colspan='4' class='fail'&gt;Error: {metrics['error']}&lt;/td&gt;&lt;/tr&gt;\")\n                    continue\n\n                status = \"PASS\" if metrics.get(\"passed\", False) else \"FAIL\"\n                cls = \"pass\" if status == \"PASS\" else \"fail\"\n\n                stat = f\"{metrics.get('statistic', 0):.4f}\"\n                # TVD doesn't have a p-value, KS does.\n                pval = f\"{metrics.get('p_value', 0):.4f}\" if metrics.get('p_value') is not None else \"N/A\"\n                test_name = metrics.get('test', 'N/A')\n\n                html_content.append(f\"&lt;tr&gt;&lt;td&gt;{col}&lt;/td&gt;&lt;td&gt;{test_name}&lt;/td&gt;&lt;td&gt;{stat}&lt;/td&gt;&lt;td&gt;{pval}&lt;/td&gt;&lt;td class='{cls}'&gt;{status}&lt;/td&gt;&lt;/tr&gt;\")\n\n            html_content.append(\"&lt;/table&gt;\")\n\n            # --- 3. Detailed Statistics ---\n            if \"detailed_stats\" in data:\n                html_content.append(\"&lt;h3&gt;Detailed Statistics (Real vs Synthetic)&lt;/h3&gt;\")\n                html_content.append(\"&lt;table&gt;&lt;tr&gt;&lt;th&gt;Column&lt;/th&gt;&lt;th&gt;Metric&lt;/th&gt;&lt;th&gt;Real&lt;/th&gt;&lt;th&gt;Synthetic&lt;/th&gt;&lt;/tr&gt;\")\n\n                for col, stats in data[\"detailed_stats\"].items():\n                    # stats has \"real\": {...}, \"synth\": {...}\n                    real_s = stats.get(\"real\", {})\n                    synth_s = stats.get(\"synth\", {})\n\n                    # Merge keys to show\n                    all_keys = sorted(list(set(real_s.keys()) | set(synth_s.keys())))\n                    # Usually we want mean, std, min, max or unique, top\n\n                    first = True\n                    for k in all_keys:\n                        r_val = real_s.get(k, \"-\")\n                        s_val = synth_s.get(k, \"-\")\n\n                        # Format floats\n                        if isinstance(r_val, (float, np.floating)): r_val = f\"{r_val:.4f}\"\n                        if isinstance(s_val, (float, np.floating)): s_val = f\"{s_val:.4f}\"\n\n                        row_start = f\"&lt;tr&gt;&lt;td rowspan='{len(all_keys)}'&gt;{col}&lt;/td&gt;\" if first else \"&lt;tr&gt;\"\n                        row_end = f\"&lt;td&gt;{k}&lt;/td&gt;&lt;td&gt;{r_val}&lt;/td&gt;&lt;td&gt;{s_val}&lt;/td&gt;&lt;/tr&gt;\"\n                        html_content.append(row_start + row_end)\n                        first = False\n                html_content.append(\"&lt;/table&gt;\")\n\n            # --- 4. Data Preview ---\n            if \"preview\" in data:\n                html_content.append(\"&lt;h3&gt;Data Preview (First 10 Rows)&lt;/h3&gt;\")\n                html_content.append(\"&lt;div class='row'&gt;\")\n\n                # Real\n                html_content.append(\"&lt;div class='col'&gt;\")\n                html_content.append(\"&lt;div class='preview-header'&gt;Original Data (Real)&lt;/div&gt;\")\n                html_content.append(data[\"preview\"][\"real_html\"])\n                html_content.append(\"&lt;/div&gt;\")\n\n                # Synth\n                html_content.append(\"&lt;div class='col'&gt;\")\n                html_content.append(\"&lt;div class='preview-header'&gt;Synthetic Data (Generated)&lt;/div&gt;\")\n                html_content.append(data[\"preview\"][\"synth_html\"])\n                html_content.append(\"&lt;/div&gt;\")\n\n                html_content.append(\"&lt;/div&gt;\") # End row\n\n            html_content.append(\"&lt;/div&gt;\") # End section\n\n        html_content.append(\"&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;\")\n\n        with open(output_path, \"w\") as f:\n            f.write(\"\\n\".join(html_content))\n</code></pre>"},{"location":"api/interface/#syntho_hive.validation.report_generator.ValidationReport.generate","title":"generate","text":"<pre><code>generate(real_data: Dict[str, DataFrame], synth_data: Dict[str, DataFrame], output_path: str)\n</code></pre> <p>Run validation and save a report.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>Dict[str, DataFrame]</code> <p>Mapping of table name to real dataframe.</p> required <code>synth_data</code> <code>Dict[str, DataFrame]</code> <p>Mapping of table name to synthetic dataframe.</p> required <code>output_path</code> <code>str</code> <p>Destination path for HTML or JSON report.</p> required Source code in <code>syntho_hive/validation/report_generator.py</code> <pre><code>def generate(self, real_data: Dict[str, pd.DataFrame], synth_data: Dict[str, pd.DataFrame], output_path: str):\n    \"\"\"Run validation and save a report.\n\n    Args:\n        real_data: Mapping of table name to real dataframe.\n        synth_data: Mapping of table name to synthetic dataframe.\n        output_path: Destination path for HTML or JSON report.\n    \"\"\"\n    report = {\n        \"tables\": {},\n        \"summary\": \"Validation Report\"\n    }\n\n    for table_name, real_df in real_data.items():\n        if table_name not in synth_data:\n            continue\n\n        synth_df = synth_data[table_name]\n\n        # 1. Column comparisons\n        col_metrics = self.validator.compare_columns(real_df, synth_df)\n\n        # 2. Correlation\n        corr_diff = self.validator.check_correlations(real_df, synth_df)\n\n        # 3. Detailed Stats\n        stats = self._calculate_detailed_stats(real_df, synth_df)\n\n        # 4. Data Preview\n        # Use Pandas to_html for easy formatting, strict constraints\n        preview = {\n            \"real_html\": real_df.head(10).to_html(index=False, classes='scroll-table', border=0),\n            \"synth_html\": synth_df.head(10).to_html(index=False, classes='scroll-table', border=0)\n        }\n\n        report[\"tables\"][table_name] = {\n            \"column_metrics\": col_metrics,\n            \"correlation_distance\": corr_diff,\n            \"detailed_stats\": stats,\n            \"preview\": preview\n        }\n\n    if output_path.endswith(\".html\"):\n        self._save_html(report, output_path)\n    else:\n        # Save to JSON for now (PDF requires more deps)\n        with open(output_path, \"w\") as f:\n            json.dump(report, f, indent=2, default=str)\n\n    import os\n    print(f\"Report saved to {os.path.abspath(output_path)}\")\n</code></pre>"},{"location":"api/privacy/","title":"Privacy &amp; Sanitization","text":"<p>The privacy module ensures that sensitive information is detected and handled correctly before any modeling takes place.</p>"},{"location":"api/privacy/#sanitizer","title":"Sanitizer","text":""},{"location":"api/privacy/#syntho_hive.privacy.sanitizer.PIISanitizer","title":"syntho_hive.privacy.sanitizer.PIISanitizer","text":"<p>Detect and sanitize PII columns based on configurable rules.</p> Source code in <code>syntho_hive/privacy/sanitizer.py</code> <pre><code>class PIISanitizer:\n    \"\"\"Detect and sanitize PII columns based on configurable rules.\"\"\"\n\n    def __init__(self, config: Optional[PrivacyConfig] = None):\n        \"\"\"Create a sanitizer with contextual faker support.\n\n        Args:\n            config: Optional privacy configuration; defaults to ``PrivacyConfig.default``.\n        \"\"\"\n        self.config = config or PrivacyConfig.default()\n        self.faker = ContextualFaker()\n\n    def analyze(self, df: pd.DataFrame) -&gt; Dict[str, str]:\n        \"\"\"Detect potential PII columns using configured rules.\n\n        Args:\n            df: DataFrame to inspect for PII.\n\n        Returns:\n            Mapping of column name to matched PII rule name.\n        \"\"\"\n        detected = {}\n\n        # 1. Check column names (heuristics)\n        for col in df.columns:\n            col_lower = col.lower()\n            for rule in self.config.rules:\n                if rule.name in col_lower:\n                    detected[col] = rule.name\n                    break\n\n        # 2. Check content for remaining columns\n        # Sample first 100 rows (or all if small) to speed up\n        sample = df.head(100)\n\n        for col in df.columns:\n            if col in detected:\n                continue\n\n            # Skip non-string columns for regex matching\n            if not pd.api.types.is_string_dtype(sample[col]):\n                continue\n\n            valid_rows = sample[col].dropna().astype(str)\n            if len(valid_rows) == 0:\n                continue\n\n            # Check each rule\n            best_rule = None\n            max_matches = 0\n\n            for rule in self.config.rules:\n                match_count = 0\n                for val in valid_rows:\n                    # Check any pattern for this rule\n                    for pat in rule.patterns:\n                        if re.search(pat, val):\n                            match_count += 1\n                            break # Match found for this value\n\n                # If &gt; 50% match, consider it a candidate\n                if match_count &gt; len(valid_rows) * 0.5:\n                    if match_count &gt; max_matches:\n                        max_matches = match_count\n                        best_rule = rule.name\n\n            if best_rule:\n                detected[col] = best_rule\n\n        return detected\n\n    def sanitize(self, df: pd.DataFrame, pii_map: Optional[Dict[str, str]] = None) -&gt; pd.DataFrame:\n        \"\"\"Apply sanitization rules to a dataframe.\n\n        Args:\n            df: Input dataframe containing potential PII.\n            pii_map: Optional precomputed map of column name to PII rule name.\n\n        Returns:\n            Sanitized dataframe with PII handled according to configured actions.\n        \"\"\"\n        if pii_map is None:\n            pii_map = self.analyze(df)\n\n        output_df = df.copy()\n\n        for col, rule_name in pii_map.items():\n            rule = next((r for r in self.config.rules if r.name == rule_name), None)\n            if not rule:\n                continue\n\n            if rule.action == \"drop\":\n                output_df.drop(columns=[col], inplace=True)\n\n            elif rule.action == \"mask\":\n                output_df[col] = output_df[col].apply(lambda x: self._mask_value(x))\n\n            elif rule.action == \"hash\":\n                output_df[col] = output_df[col].apply(lambda x: self._hash_value(x))\n\n            elif rule.action == \"fake\":\n                output_df[col] = self._fake_column(output_df, col, rule)\n\n            elif rule.action == \"custom\":\n                if rule.custom_generator:\n                    # Use custom generator, passing row context\n                    # Note: This checks frame line by line, slower but powerful\n                    output_df[col] = output_df.apply(lambda row: rule.custom_generator(row.to_dict()), axis=1)\n                else:\n                    # Fallback if no generator provided\n                    output_df[col] = self._mask_value(output_df[col])\n\n        return output_df\n\n    def _mask_value(self, val: Any) -&gt; str:\n        \"\"\"Mask a value, preserving only the last four characters.\"\"\"\n        s = str(val)\n        if len(s) &lt;= 4:\n            return \"*\" * len(s)\n        return \"*\" * (len(s) - 4) + s[-4:]\n\n    def _hash_value(self, val: Any) -&gt; str:\n        \"\"\"Return a SHA256 hash representation of a value.\"\"\"\n        return hashlib.sha256(str(val).encode()).hexdigest()\n\n    def _fake_column(self, df: pd.DataFrame, col: str, rule: PiiRule) -&gt; pd.Series:\n        \"\"\"Generate fake data for a column using contextual faker.\n\n        Args:\n            df: DataFrame containing the column to fake.\n            col: Column name.\n            rule: PII rule describing the type being faked.\n\n        Returns:\n            Series of fake values aligned to ``df``.\n        \"\"\"\n        # Context strategy: \n        # If the rule has a context_key (not yet fully implemented in config, but good for future), use it.\n        # Fallback to simple random generation.\n\n        # We can pass the dataframe to the faker to handle this column\n        # But our FakerContextual currently handles whole DF. \n        # Let's call generate_pii for the length of DF.\n\n        # Optimization: fast path if no context needed\n        return df.apply(lambda row: self.faker.generate_pii(rule.name, context=row.to_dict())[0], axis=1)\n</code></pre>"},{"location":"api/privacy/#syntho_hive.privacy.sanitizer.PIISanitizer.analyze","title":"analyze","text":"<pre><code>analyze(df: DataFrame) -&gt; Dict[str, str]\n</code></pre> <p>Detect potential PII columns using configured rules.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to inspect for PII.</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Mapping of column name to matched PII rule name.</p> Source code in <code>syntho_hive/privacy/sanitizer.py</code> <pre><code>def analyze(self, df: pd.DataFrame) -&gt; Dict[str, str]:\n    \"\"\"Detect potential PII columns using configured rules.\n\n    Args:\n        df: DataFrame to inspect for PII.\n\n    Returns:\n        Mapping of column name to matched PII rule name.\n    \"\"\"\n    detected = {}\n\n    # 1. Check column names (heuristics)\n    for col in df.columns:\n        col_lower = col.lower()\n        for rule in self.config.rules:\n            if rule.name in col_lower:\n                detected[col] = rule.name\n                break\n\n    # 2. Check content for remaining columns\n    # Sample first 100 rows (or all if small) to speed up\n    sample = df.head(100)\n\n    for col in df.columns:\n        if col in detected:\n            continue\n\n        # Skip non-string columns for regex matching\n        if not pd.api.types.is_string_dtype(sample[col]):\n            continue\n\n        valid_rows = sample[col].dropna().astype(str)\n        if len(valid_rows) == 0:\n            continue\n\n        # Check each rule\n        best_rule = None\n        max_matches = 0\n\n        for rule in self.config.rules:\n            match_count = 0\n            for val in valid_rows:\n                # Check any pattern for this rule\n                for pat in rule.patterns:\n                    if re.search(pat, val):\n                        match_count += 1\n                        break # Match found for this value\n\n            # If &gt; 50% match, consider it a candidate\n            if match_count &gt; len(valid_rows) * 0.5:\n                if match_count &gt; max_matches:\n                    max_matches = match_count\n                    best_rule = rule.name\n\n        if best_rule:\n            detected[col] = best_rule\n\n    return detected\n</code></pre>"},{"location":"api/privacy/#syntho_hive.privacy.sanitizer.PIISanitizer.sanitize","title":"sanitize","text":"<pre><code>sanitize(df: DataFrame, pii_map: Optional[Dict[str, str]] = None) -&gt; pd.DataFrame\n</code></pre> <p>Apply sanitization rules to a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input dataframe containing potential PII.</p> required <code>pii_map</code> <code>Optional[Dict[str, str]]</code> <p>Optional precomputed map of column name to PII rule name.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Sanitized dataframe with PII handled according to configured actions.</p> Source code in <code>syntho_hive/privacy/sanitizer.py</code> <pre><code>def sanitize(self, df: pd.DataFrame, pii_map: Optional[Dict[str, str]] = None) -&gt; pd.DataFrame:\n    \"\"\"Apply sanitization rules to a dataframe.\n\n    Args:\n        df: Input dataframe containing potential PII.\n        pii_map: Optional precomputed map of column name to PII rule name.\n\n    Returns:\n        Sanitized dataframe with PII handled according to configured actions.\n    \"\"\"\n    if pii_map is None:\n        pii_map = self.analyze(df)\n\n    output_df = df.copy()\n\n    for col, rule_name in pii_map.items():\n        rule = next((r for r in self.config.rules if r.name == rule_name), None)\n        if not rule:\n            continue\n\n        if rule.action == \"drop\":\n            output_df.drop(columns=[col], inplace=True)\n\n        elif rule.action == \"mask\":\n            output_df[col] = output_df[col].apply(lambda x: self._mask_value(x))\n\n        elif rule.action == \"hash\":\n            output_df[col] = output_df[col].apply(lambda x: self._hash_value(x))\n\n        elif rule.action == \"fake\":\n            output_df[col] = self._fake_column(output_df, col, rule)\n\n        elif rule.action == \"custom\":\n            if rule.custom_generator:\n                # Use custom generator, passing row context\n                # Note: This checks frame line by line, slower but powerful\n                output_df[col] = output_df.apply(lambda row: rule.custom_generator(row.to_dict()), axis=1)\n            else:\n                # Fallback if no generator provided\n                output_df[col] = self._mask_value(output_df[col])\n\n    return output_df\n</code></pre>"},{"location":"api/privacy/#configuration","title":"Configuration","text":""},{"location":"api/privacy/#syntho_hive.privacy.sanitizer.PiiRule","title":"syntho_hive.privacy.sanitizer.PiiRule  <code>dataclass</code>","text":"<p>Configuration for a single PII type and handling strategy.</p> Source code in <code>syntho_hive/privacy/sanitizer.py</code> <pre><code>@dataclass\nclass PiiRule:\n    \"\"\"Configuration for a single PII type and handling strategy.\"\"\"\n    name: str\n    patterns: List[str]  # List of regex patterns to match\n    action: str = \"drop\"  # Options: \"drop\", \"mask\", \"hash\", \"fake\", \"custom\", \"keep\"\n    context_key: Optional[str] = None  # Key to look for in context (e.g. 'country' for locale)\n    custom_generator: Optional[Callable[[Dict[str, Any]], Any]] = None  # Custom lambda for generation\n</code></pre>"},{"location":"api/privacy/#syntho_hive.privacy.sanitizer.PrivacyConfig","title":"syntho_hive.privacy.sanitizer.PrivacyConfig  <code>dataclass</code>","text":"<p>Collection of rules for PII detection and handling.</p> Source code in <code>syntho_hive/privacy/sanitizer.py</code> <pre><code>@dataclass\nclass PrivacyConfig:\n    \"\"\"Collection of rules for PII detection and handling.\"\"\"\n    rules: List[PiiRule] = field(default_factory=list)\n\n    @classmethod\n    def default(cls) -&gt; 'PrivacyConfig':\n        \"\"\"Create a default privacy configuration with common PII rules.\"\"\"\n        return cls(rules=[\n            PiiRule(name=\"email\", patterns=[r\"[^@]+@[^@]+\\.[^@]+\"], action=\"fake\"),\n            PiiRule(name=\"ssn\", patterns=[r\"\\d{3}-\\d{2}-\\d{4}\"], action=\"mask\"),\n            PiiRule(name=\"phone\", patterns=[r\"\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\"], action=\"fake\"),\n            PiiRule(name=\"credit_card\", patterns=[r\"\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}\"], action=\"mask\"),\n            PiiRule(name=\"ipv4\", patterns=[r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\"], action=\"fake\"),\n        ])\n</code></pre>"},{"location":"api/privacy/#syntho_hive.privacy.sanitizer.PrivacyConfig.default","title":"default  <code>classmethod</code>","text":"<pre><code>default() -&gt; PrivacyConfig\n</code></pre> <p>Create a default privacy configuration with common PII rules.</p> Source code in <code>syntho_hive/privacy/sanitizer.py</code> <pre><code>@classmethod\ndef default(cls) -&gt; 'PrivacyConfig':\n    \"\"\"Create a default privacy configuration with common PII rules.\"\"\"\n    return cls(rules=[\n        PiiRule(name=\"email\", patterns=[r\"[^@]+@[^@]+\\.[^@]+\"], action=\"fake\"),\n        PiiRule(name=\"ssn\", patterns=[r\"\\d{3}-\\d{2}-\\d{4}\"], action=\"mask\"),\n        PiiRule(name=\"phone\", patterns=[r\"\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\"], action=\"fake\"),\n        PiiRule(name=\"credit_card\", patterns=[r\"\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}\"], action=\"mask\"),\n        PiiRule(name=\"ipv4\", patterns=[r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\"], action=\"fake\"),\n    ])\n</code></pre>"},{"location":"api/privacy/#faking-strategy","title":"Faking Strategy","text":""},{"location":"api/privacy/#syntho_hive.privacy.faker_contextual.ContextualFaker","title":"syntho_hive.privacy.faker_contextual.ContextualFaker","text":"<p>Context-aware PII generator leveraging Faker locales.</p> Source code in <code>syntho_hive/privacy/faker_contextual.py</code> <pre><code>class ContextualFaker:\n    \"\"\"Context-aware PII generator leveraging Faker locales.\"\"\"\n\n    LOCALE_MAP = {\n        \"JP\": \"ja_JP\",\n        \"US\": \"en_US\",\n        \"UK\": \"en_GB\",\n        \"GB\": \"en_GB\",\n        \"DE\": \"de_DE\",\n        \"FR\": \"fr_FR\",\n        \"CN\": \"zh_CN\",\n        \"IN\": \"en_IN\",\n        # Add more as needed\n    }\n\n    def __init__(self):\n        \"\"\"Initialize faker cache and logger.\"\"\"\n        self._fakers: Dict[str, Faker] = {}\n        # Initialize default\n        self._fakers[\"default\"] = Faker()\n        self.logger = logging.getLogger(__name__)\n\n    def _get_faker(self, locale: Optional[str]) -&gt; Faker:\n        \"\"\"Get or create a Faker instance for a locale.\n\n        Args:\n            locale: Optional locale string (e.g., ``\"JP\"`` or ``\"en_US\"``).\n\n        Returns:\n            Faker instance configured for the requested locale.\n        \"\"\"\n        if not locale:\n            return self._fakers[\"default\"]\n\n        mapped_locale = self.LOCALE_MAP.get(locale.upper(), \"en_US\")\n\n        if mapped_locale not in self._fakers:\n            try:\n                self._fakers[mapped_locale] = Faker(mapped_locale)\n            except Exception as e:\n                self.logger.warning(f\"Could not load locale {mapped_locale}, falling back to default. Error: {e}\")\n                self._fakers[mapped_locale] = self._fakers[\"default\"]\n\n        return self._fakers[mapped_locale]\n\n    def generate_pii(self, pii_type: str, context: Optional[Dict[str, Any]] = None, count: int = 1) -&gt; List[str]:\n        \"\"\"Generate PII values with optional contextual locale.\n\n        Args:\n            pii_type: Faker provider name (e.g., ``\"email\"`` or ``\"phone\"``).\n            context: Optional row context used to infer locale (country/locale/region keys).\n            count: Number of values to generate.\n\n        Returns:\n            List of generated PII strings.\n        \"\"\"\n        if context is None:\n            context = {}\n\n        # Attempt to infer locale from context\n        # Heuristic: Look for 'country', 'region', 'locale' keys\n        locale = context.get('country') or context.get('locale') or context.get('region')\n\n        fake = self._get_faker(locale if isinstance(locale, str) else None)\n\n        results = []\n        for _ in range(count):\n            val = self._generate_single_value(fake, pii_type)\n            results.append(val)\n\n        return results\n\n    def _generate_single_value(self, fake: Faker, pii_type: str) -&gt; str:\n        \"\"\"Generate a single PII value using a Faker instance.\"\"\"\n        try:\n            if hasattr(fake, pii_type):\n                 # Dynamic method call on Faker instance\n                return str(getattr(fake, pii_type)())\n\n            # Custom mappings for common PII types if name mismatch or special logic\n            if pii_type == 'phone':\n                 return fake.phone_number()\n            elif pii_type == 'ip' or pii_type == 'ipv4':\n                return fake.ipv4()\n            elif pii_type == 'credit_card':\n                return fake.credit_card_number()\n\n            # Fallback\n            return str(fake.text(max_nb_chars=20))\n        except Exception as e:\n            self.logger.error(f\"Error generating {pii_type}: {e}\")\n            return \"REDACTED\"\n\n    def process_dataframe(self, df: pd.DataFrame, pii_cols: Dict[str, str]) -&gt; pd.DataFrame:\n        \"\"\"Replace placeholders with generated PII in a dataframe.\n\n        Args:\n            df: Input dataframe containing placeholder columns.\n            pii_cols: Mapping of column name to PII type (e.g., ``{\"user_email\": \"email\"}``).\n\n        Returns:\n            DataFrame with specified columns replaced by generated PII.\n        \"\"\"\n        output_df = df.copy()\n\n        # Check if we have context columns\n        has_country_context = 'country' in df.columns or 'locale' in df.columns\n\n        if not has_country_context:\n            # Fast path: Vectorized apply (Fake doesn't vectorize well but we avoid row iteration overhead if possible)\n            # Actually simpler: Just generate N fake values using default locale\n            for col, pii_type in pii_cols.items():\n                fake = self._get_faker(None)\n                # Generate list\n                values = [self._generate_single_value(fake, pii_type) for _ in range(len(df))]\n                output_df[col] = values\n        else:\n            # Slow path: Row-by-row for context awareness\n            for idx, row in output_df.iterrows():\n                context = row.to_dict()\n                for col, pii_type in pii_cols.items():\n                    val = self.generate_pii(pii_type, context=context, count=1)[0]\n                    output_df.at[idx, col] = val\n\n        return output_df\n</code></pre>"},{"location":"api/privacy/#syntho_hive.privacy.faker_contextual.ContextualFaker.generate_pii","title":"generate_pii","text":"<pre><code>generate_pii(pii_type: str, context: Optional[Dict[str, Any]] = None, count: int = 1) -&gt; List[str]\n</code></pre> <p>Generate PII values with optional contextual locale.</p> <p>Parameters:</p> Name Type Description Default <code>pii_type</code> <code>str</code> <p>Faker provider name (e.g., <code>\"email\"</code> or <code>\"phone\"</code>).</p> required <code>context</code> <code>Optional[Dict[str, Any]]</code> <p>Optional row context used to infer locale (country/locale/region keys).</p> <code>None</code> <code>count</code> <code>int</code> <p>Number of values to generate.</p> <code>1</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of generated PII strings.</p> Source code in <code>syntho_hive/privacy/faker_contextual.py</code> <pre><code>def generate_pii(self, pii_type: str, context: Optional[Dict[str, Any]] = None, count: int = 1) -&gt; List[str]:\n    \"\"\"Generate PII values with optional contextual locale.\n\n    Args:\n        pii_type: Faker provider name (e.g., ``\"email\"`` or ``\"phone\"``).\n        context: Optional row context used to infer locale (country/locale/region keys).\n        count: Number of values to generate.\n\n    Returns:\n        List of generated PII strings.\n    \"\"\"\n    if context is None:\n        context = {}\n\n    # Attempt to infer locale from context\n    # Heuristic: Look for 'country', 'region', 'locale' keys\n    locale = context.get('country') or context.get('locale') or context.get('region')\n\n    fake = self._get_faker(locale if isinstance(locale, str) else None)\n\n    results = []\n    for _ in range(count):\n        val = self._generate_single_value(fake, pii_type)\n        results.append(val)\n\n    return results\n</code></pre>"},{"location":"api/privacy/#syntho_hive.privacy.faker_contextual.ContextualFaker.process_dataframe","title":"process_dataframe","text":"<pre><code>process_dataframe(df: DataFrame, pii_cols: Dict[str, str]) -&gt; pd.DataFrame\n</code></pre> <p>Replace placeholders with generated PII in a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input dataframe containing placeholder columns.</p> required <code>pii_cols</code> <code>Dict[str, str]</code> <p>Mapping of column name to PII type (e.g., <code>{\"user_email\": \"email\"}</code>).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with specified columns replaced by generated PII.</p> Source code in <code>syntho_hive/privacy/faker_contextual.py</code> <pre><code>def process_dataframe(self, df: pd.DataFrame, pii_cols: Dict[str, str]) -&gt; pd.DataFrame:\n    \"\"\"Replace placeholders with generated PII in a dataframe.\n\n    Args:\n        df: Input dataframe containing placeholder columns.\n        pii_cols: Mapping of column name to PII type (e.g., ``{\"user_email\": \"email\"}``).\n\n    Returns:\n        DataFrame with specified columns replaced by generated PII.\n    \"\"\"\n    output_df = df.copy()\n\n    # Check if we have context columns\n    has_country_context = 'country' in df.columns or 'locale' in df.columns\n\n    if not has_country_context:\n        # Fast path: Vectorized apply (Fake doesn't vectorize well but we avoid row iteration overhead if possible)\n        # Actually simpler: Just generate N fake values using default locale\n        for col, pii_type in pii_cols.items():\n            fake = self._get_faker(None)\n            # Generate list\n            values = [self._generate_single_value(fake, pii_type) for _ in range(len(df))]\n            output_df[col] = values\n    else:\n        # Slow path: Row-by-row for context awareness\n        for idx, row in output_df.iterrows():\n            context = row.to_dict()\n            for col, pii_type in pii_cols.items():\n                val = self.generate_pii(pii_type, context=context, count=1)[0]\n                output_df.at[idx, col] = val\n\n    return output_df\n</code></pre>"},{"location":"api/relational/","title":"Relational Orchestration","text":"<p>This module handles the complexity of multi-table generation, ensuring referential integrity and statistical correlation between tables.</p>"},{"location":"api/relational/#orchestrator","title":"Orchestrator","text":""},{"location":"api/relational/#syntho_hive.relational.orchestrator.StagedOrchestrator","title":"syntho_hive.relational.orchestrator.StagedOrchestrator","text":"<p>Manage staged relational synthesis across parent/child tables.</p> Source code in <code>syntho_hive/relational/orchestrator.py</code> <pre><code>class StagedOrchestrator:\n    \"\"\"Manage staged relational synthesis across parent/child tables.\"\"\"\n\n    def __init__(self, metadata: Metadata, spark: SparkSession):\n        \"\"\"Initialize orchestrator dependencies.\n\n        Args:\n            metadata: Dataset metadata with relational details.\n            spark: SparkSession used for IO and potential UDFs.\n        \"\"\"\n        self.metadata = metadata\n        self.spark = spark\n        self.graph = SchemaGraph(metadata)\n        self.io = SparkIO(spark)\n        self.models: Dict[str, CTGAN] = {}\n        self.linkage_models: Dict[str, LinkageModel] = {}\n\n    def fit_all(self, real_data_paths: Dict[str, str], epochs: int = 300, batch_size: int = 500, **model_kwargs: Union[int, str, Tuple[int, int]]):\n        \"\"\"Fit CTGAN and linkage models for every table.\n\n        Args:\n            real_data_paths: Mapping ``{table_name: 'db.table' or '/path'}``.\n            epochs: Number of training epochs for CTGAN.\n            batch_size: Training batch size.\n            **model_kwargs: Extra parameters forwarded to CTGAN constructor.\n        \"\"\"\n        # Topo sort to train parents first? Or independent?\n        # Linkage model needs both parent and child data.\n        # CTGAN needs Child data + Parent attributes (joined).\n\n        # Training order doesn't strictly matter as long as we have data, \n        # but generation order matters.\n\n        for table_name in self.metadata.tables:\n            print(f\"Fitting model for table: {table_name}\")\n            data_path = real_data_paths.get(table_name)\n            if not data_path:\n                print(f\"Warning: No data path provided for {table_name}, skipping.\")\n                continue\n\n            # Read data\n            target_df = self.io.read_dataset(data_path)\n            # Convert to Pandas for CTGAN (prototype limitation)\n            target_pdf = target_df.toPandas()\n\n            config = self.metadata.get_table(table_name)\n            if not config.has_dependencies:\n                # Root Table\n                model = CTGAN(\n                    self.metadata, \n                    batch_size=batch_size, \n                    epochs=epochs,\n                    **model_kwargs\n                )\n                model.fit(target_pdf, table_name=table_name)\n                self.models[table_name] = model\n            else:\n                # Child Table\n                # 1. Identify \"Driver\" Parent (First FK)\n                pk_map = config.fk\n                # pk_map is {local_col: \"parent_table.parent_col\"}\n\n                # Sort keys to ensure deterministic driver selection\n                sorted_fks = sorted(pk_map.keys())\n                driver_fk = sorted_fks[0]\n                driver_ref = pk_map[driver_fk]\n\n                driver_parent_table, driver_parent_pk = driver_ref.split(\".\")\n\n                parent_path = real_data_paths.get(driver_parent_table)\n                parent_df = self.io.read_dataset(parent_path).toPandas()\n\n                # 2. Train Linkage Model on Driver Parent\n                print(f\"Training Linkage for {table_name} driven by {driver_parent_table}\")\n                linkage = LinkageModel()\n                linkage.fit(parent_df, target_pdf, fk_col=driver_fk, pk_col=driver_parent_pk)\n                self.linkage_models[table_name] = linkage\n\n                # 3. Train Conditional CTGAN (Conditioning on Driver Parent Context)\n                context_cols = config.parent_context_cols\n                if context_cols:\n                     # Prepare parent data for merge\n                     right_side = parent_df[[driver_parent_pk] + context_cols].copy()\n\n                     rename_map = {c: f\"__ctx__{c}\" for c in context_cols}\n                     right_side = right_side.rename(columns=rename_map)\n\n                     joined = target_pdf.merge(\n                         right_side,\n                         left_on=driver_fk,\n                         right_on=driver_parent_pk,\n                         how=\"left\"\n                     )\n\n                     context_df = joined[list(rename_map.values())].copy()\n                     context_df.columns = context_cols\n                else:\n                    context_df = None\n\n                model = CTGAN(\n                    self.metadata, \n                    batch_size=batch_size, \n                    epochs=epochs,\n                    **model_kwargs\n                )\n                # Note: We exclude ALL FK columns from CTGAN modeling to avoid them being treated as continuous/categorical features\n                # The DataTransformer handles excluding PK/FK if they are marked in metadata.\n                # But we must ensure metadata knows about ALL FKs. (It does via config.fk)\n                model.fit(target_pdf, context=context_df, table_name=table_name)\n                self.models[table_name] = model\n\n    def generate(self, num_rows_root: Dict[str, int], output_path_base: Optional[str] = None) -&gt; Dict[str, pd.DataFrame]:\n        \"\"\"Execute the multi-stage generation pipeline.\n\n        Args:\n            num_rows_root: Mapping of root table name to number of rows to generate.\n            output_path_base: Base path where generated tables will be stored. If None, returns DataFrames in memory.\n\n        Returns:\n            Dictionary of generated DataFrames {table_name: dataframe}.\n        \"\"\"\n        generation_order = self.graph.get_generation_order()\n\n        generated_tables = {}\n\n        for table_name in generation_order:\n            config = self.metadata.get_table(table_name)\n            is_root = not config.fk\n\n            model = self.models[table_name]\n\n            generated_pdf = None\n\n            if is_root:\n                print(f\"Generating root table: {table_name}\")\n                n_rows = num_rows_root.get(table_name, 1000)\n                generated_pdf = model.sample(n_rows)\n                # Assign PKs\n                generated_pdf[config.pk] = range(1, n_rows + 1)\n            else:\n                print(f\"Generating child table: {table_name}\")\n\n                # 1. Handle Driver Parent (Cardinality &amp; Context)\n                pk_map = config.fk\n                sorted_fks = sorted(pk_map.keys())\n                driver_fk = sorted_fks[0]\n                driver_ref = pk_map[driver_fk]\n                driver_parent_table, driver_parent_pk = driver_ref.split(\".\")\n\n                # Read Driver Parent Data (From Output or Memory)\n                if output_path_base:\n                     parent_path = f\"{output_path_base}/{driver_parent_table}\"\n                     parent_df = self.io.read_dataset(parent_path).toPandas()\n                else:\n                     parent_df = generated_tables[driver_parent_table]\n\n                linkage = self.linkage_models[table_name]\n\n                # Sample Counts\n                counts = linkage.sample_counts(parent_df)\n\n                # Construct Context from Driver\n                parent_ids_repeated = np.repeat(parent_df[driver_parent_pk].to_numpy(), counts)\n\n                context_cols = config.parent_context_cols\n                if context_cols:\n                    context_repeated_vals = {}\n                    for col in context_cols:\n                        context_repeated_vals[col] = np.repeat(parent_df[col].to_numpy(), counts)\n                    context_df = pd.DataFrame(context_repeated_vals)\n                else:\n                    context_df = None\n\n                total_child_rows = len(parent_ids_repeated)\n\n                # 2. Generate Data\n                if total_child_rows &gt; 0:\n                     generated_pdf = model.sample(total_child_rows, context=context_df)\n\n                     # Assign Driver FK\n                     generated_pdf[driver_fk] = parent_ids_repeated\n\n                     # Assign Secondary FKs (Random Sampling from respective Parents)\n                     for fk_col in sorted_fks[1:]:\n                         ref = pk_map[fk_col]\n                         p_table, p_pk = ref.split(\".\")\n\n                         # Read Secondary Parent\n                         if output_path_base:\n                             p_path = f\"{output_path_base}/{p_table}\"\n                             p_df = self.io.read_dataset(p_path).toPandas()\n                         else:\n                             p_df = generated_tables[p_table]\n\n                         valid_pks = p_df[p_pk].to_numpy()\n\n                         # Randomly sample valid PKs for this column\n                         generated_pdf[fk_col] = np.random.choice(valid_pks, size=total_child_rows)\n\n                     # Assign PKs\n                     generated_pdf[config.pk] = range(1, total_child_rows + 1)\n\n            if generated_pdf is not None:\n                if output_path_base:\n                    output_path = f\"{output_path_base}/{table_name}\"\n                    self.io.write_pandas(generated_pdf, output_path)\n\n                # Always store in memory for downstream children if needed (and return if requested)\n                # For massive datasets this might be risky, but consistent with user request: \"save to a df object\"\n                generated_tables[table_name] = generated_pdf\n\n        return generated_tables\n</code></pre>"},{"location":"api/relational/#syntho_hive.relational.orchestrator.StagedOrchestrator.fit_all","title":"fit_all","text":"<pre><code>fit_all(real_data_paths: Dict[str, str], epochs: int = 300, batch_size: int = 500, **model_kwargs: Union[int, str, Tuple[int, int]])\n</code></pre> <p>Fit CTGAN and linkage models for every table.</p> <p>Parameters:</p> Name Type Description Default <code>real_data_paths</code> <code>Dict[str, str]</code> <p>Mapping <code>{table_name: 'db.table' or '/path'}</code>.</p> required <code>epochs</code> <code>int</code> <p>Number of training epochs for CTGAN.</p> <code>300</code> <code>batch_size</code> <code>int</code> <p>Training batch size.</p> <code>500</code> <code>**model_kwargs</code> <code>Union[int, str, Tuple[int, int]]</code> <p>Extra parameters forwarded to CTGAN constructor.</p> <code>{}</code> Source code in <code>syntho_hive/relational/orchestrator.py</code> <pre><code>def fit_all(self, real_data_paths: Dict[str, str], epochs: int = 300, batch_size: int = 500, **model_kwargs: Union[int, str, Tuple[int, int]]):\n    \"\"\"Fit CTGAN and linkage models for every table.\n\n    Args:\n        real_data_paths: Mapping ``{table_name: 'db.table' or '/path'}``.\n        epochs: Number of training epochs for CTGAN.\n        batch_size: Training batch size.\n        **model_kwargs: Extra parameters forwarded to CTGAN constructor.\n    \"\"\"\n    # Topo sort to train parents first? Or independent?\n    # Linkage model needs both parent and child data.\n    # CTGAN needs Child data + Parent attributes (joined).\n\n    # Training order doesn't strictly matter as long as we have data, \n    # but generation order matters.\n\n    for table_name in self.metadata.tables:\n        print(f\"Fitting model for table: {table_name}\")\n        data_path = real_data_paths.get(table_name)\n        if not data_path:\n            print(f\"Warning: No data path provided for {table_name}, skipping.\")\n            continue\n\n        # Read data\n        target_df = self.io.read_dataset(data_path)\n        # Convert to Pandas for CTGAN (prototype limitation)\n        target_pdf = target_df.toPandas()\n\n        config = self.metadata.get_table(table_name)\n        if not config.has_dependencies:\n            # Root Table\n            model = CTGAN(\n                self.metadata, \n                batch_size=batch_size, \n                epochs=epochs,\n                **model_kwargs\n            )\n            model.fit(target_pdf, table_name=table_name)\n            self.models[table_name] = model\n        else:\n            # Child Table\n            # 1. Identify \"Driver\" Parent (First FK)\n            pk_map = config.fk\n            # pk_map is {local_col: \"parent_table.parent_col\"}\n\n            # Sort keys to ensure deterministic driver selection\n            sorted_fks = sorted(pk_map.keys())\n            driver_fk = sorted_fks[0]\n            driver_ref = pk_map[driver_fk]\n\n            driver_parent_table, driver_parent_pk = driver_ref.split(\".\")\n\n            parent_path = real_data_paths.get(driver_parent_table)\n            parent_df = self.io.read_dataset(parent_path).toPandas()\n\n            # 2. Train Linkage Model on Driver Parent\n            print(f\"Training Linkage for {table_name} driven by {driver_parent_table}\")\n            linkage = LinkageModel()\n            linkage.fit(parent_df, target_pdf, fk_col=driver_fk, pk_col=driver_parent_pk)\n            self.linkage_models[table_name] = linkage\n\n            # 3. Train Conditional CTGAN (Conditioning on Driver Parent Context)\n            context_cols = config.parent_context_cols\n            if context_cols:\n                 # Prepare parent data for merge\n                 right_side = parent_df[[driver_parent_pk] + context_cols].copy()\n\n                 rename_map = {c: f\"__ctx__{c}\" for c in context_cols}\n                 right_side = right_side.rename(columns=rename_map)\n\n                 joined = target_pdf.merge(\n                     right_side,\n                     left_on=driver_fk,\n                     right_on=driver_parent_pk,\n                     how=\"left\"\n                 )\n\n                 context_df = joined[list(rename_map.values())].copy()\n                 context_df.columns = context_cols\n            else:\n                context_df = None\n\n            model = CTGAN(\n                self.metadata, \n                batch_size=batch_size, \n                epochs=epochs,\n                **model_kwargs\n            )\n            # Note: We exclude ALL FK columns from CTGAN modeling to avoid them being treated as continuous/categorical features\n            # The DataTransformer handles excluding PK/FK if they are marked in metadata.\n            # But we must ensure metadata knows about ALL FKs. (It does via config.fk)\n            model.fit(target_pdf, context=context_df, table_name=table_name)\n            self.models[table_name] = model\n</code></pre>"},{"location":"api/relational/#syntho_hive.relational.orchestrator.StagedOrchestrator.generate","title":"generate","text":"<pre><code>generate(num_rows_root: Dict[str, int], output_path_base: Optional[str] = None) -&gt; Dict[str, pd.DataFrame]\n</code></pre> <p>Execute the multi-stage generation pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>num_rows_root</code> <code>Dict[str, int]</code> <p>Mapping of root table name to number of rows to generate.</p> required <code>output_path_base</code> <code>Optional[str]</code> <p>Base path where generated tables will be stored. If None, returns DataFrames in memory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, DataFrame]</code> <p>Dictionary of generated DataFrames {table_name: dataframe}.</p> Source code in <code>syntho_hive/relational/orchestrator.py</code> <pre><code>def generate(self, num_rows_root: Dict[str, int], output_path_base: Optional[str] = None) -&gt; Dict[str, pd.DataFrame]:\n    \"\"\"Execute the multi-stage generation pipeline.\n\n    Args:\n        num_rows_root: Mapping of root table name to number of rows to generate.\n        output_path_base: Base path where generated tables will be stored. If None, returns DataFrames in memory.\n\n    Returns:\n        Dictionary of generated DataFrames {table_name: dataframe}.\n    \"\"\"\n    generation_order = self.graph.get_generation_order()\n\n    generated_tables = {}\n\n    for table_name in generation_order:\n        config = self.metadata.get_table(table_name)\n        is_root = not config.fk\n\n        model = self.models[table_name]\n\n        generated_pdf = None\n\n        if is_root:\n            print(f\"Generating root table: {table_name}\")\n            n_rows = num_rows_root.get(table_name, 1000)\n            generated_pdf = model.sample(n_rows)\n            # Assign PKs\n            generated_pdf[config.pk] = range(1, n_rows + 1)\n        else:\n            print(f\"Generating child table: {table_name}\")\n\n            # 1. Handle Driver Parent (Cardinality &amp; Context)\n            pk_map = config.fk\n            sorted_fks = sorted(pk_map.keys())\n            driver_fk = sorted_fks[0]\n            driver_ref = pk_map[driver_fk]\n            driver_parent_table, driver_parent_pk = driver_ref.split(\".\")\n\n            # Read Driver Parent Data (From Output or Memory)\n            if output_path_base:\n                 parent_path = f\"{output_path_base}/{driver_parent_table}\"\n                 parent_df = self.io.read_dataset(parent_path).toPandas()\n            else:\n                 parent_df = generated_tables[driver_parent_table]\n\n            linkage = self.linkage_models[table_name]\n\n            # Sample Counts\n            counts = linkage.sample_counts(parent_df)\n\n            # Construct Context from Driver\n            parent_ids_repeated = np.repeat(parent_df[driver_parent_pk].to_numpy(), counts)\n\n            context_cols = config.parent_context_cols\n            if context_cols:\n                context_repeated_vals = {}\n                for col in context_cols:\n                    context_repeated_vals[col] = np.repeat(parent_df[col].to_numpy(), counts)\n                context_df = pd.DataFrame(context_repeated_vals)\n            else:\n                context_df = None\n\n            total_child_rows = len(parent_ids_repeated)\n\n            # 2. Generate Data\n            if total_child_rows &gt; 0:\n                 generated_pdf = model.sample(total_child_rows, context=context_df)\n\n                 # Assign Driver FK\n                 generated_pdf[driver_fk] = parent_ids_repeated\n\n                 # Assign Secondary FKs (Random Sampling from respective Parents)\n                 for fk_col in sorted_fks[1:]:\n                     ref = pk_map[fk_col]\n                     p_table, p_pk = ref.split(\".\")\n\n                     # Read Secondary Parent\n                     if output_path_base:\n                         p_path = f\"{output_path_base}/{p_table}\"\n                         p_df = self.io.read_dataset(p_path).toPandas()\n                     else:\n                         p_df = generated_tables[p_table]\n\n                     valid_pks = p_df[p_pk].to_numpy()\n\n                     # Randomly sample valid PKs for this column\n                     generated_pdf[fk_col] = np.random.choice(valid_pks, size=total_child_rows)\n\n                 # Assign PKs\n                 generated_pdf[config.pk] = range(1, total_child_rows + 1)\n\n        if generated_pdf is not None:\n            if output_path_base:\n                output_path = f\"{output_path_base}/{table_name}\"\n                self.io.write_pandas(generated_pdf, output_path)\n\n            # Always store in memory for downstream children if needed (and return if requested)\n            # For massive datasets this might be risky, but consistent with user request: \"save to a df object\"\n            generated_tables[table_name] = generated_pdf\n\n    return generated_tables\n</code></pre>"},{"location":"api/relational/#linkage","title":"Linkage","text":""},{"location":"api/relational/#syntho_hive.relational.linkage.LinkageModel","title":"syntho_hive.relational.linkage.LinkageModel","text":"<p>Model cardinality relationships between parent and child tables.</p> Source code in <code>syntho_hive/relational/linkage.py</code> <pre><code>class LinkageModel:\n    \"\"\"Model cardinality relationships between parent and child tables.\"\"\"\n\n    def __init__(self, method: str = \"gmm\"):\n        \"\"\"Create a linkage model.\n\n        Args:\n            method: Distribution family used to model child counts.\n        \"\"\"\n        self.method = method\n        self.model = None\n        self.max_children = 0\n\n    def fit(self, parent_df: pd.DataFrame, child_df: pd.DataFrame, fk_col: str, pk_col: str = \"id\"):\n        \"\"\"Fit the distribution of child counts per parent.\n\n        Args:\n            parent_df: Parent table with unique primary keys.\n            child_df: Child table containing foreign keys to parents.\n            fk_col: Name of the foreign key column in the child table.\n            pk_col: Name of the primary key column in the parent table.\n        \"\"\"\n        # 1. Aggregate child counts\n        # Assumes parent_df has unique PKs\n        counts = child_df[fk_col].value_counts()\n\n        # Merge with all parents to include 0-count parents\n        parent_ids = pd.DataFrame(parent_df[pk_col].unique(), columns=[pk_col])\n\n        # Ensure Types Match (Cast Child FK to Parent PK type)\n        try:\n            target_type = parent_ids[pk_col].dtype\n            if not np.issubdtype(counts.index.dtype, target_type):\n                counts.index = counts.index.astype(target_type)\n        except Exception as e:\n            # Fallback to string if direct cast fails\n            print(f\"Warning: Could not cast FK to match PK type ({e}). Falling back to string.\")\n            parent_ids[pk_col] = parent_ids[pk_col].astype(str)\n            counts.index = counts.index.astype(str)\n\n        # Use merge to get counts, fillna(0) for parents with no children\n        # Note: In real Spark env this aggregation happens differently\n        count_df = parent_ids.merge(\n            counts.rename(\"child_count\"), \n            left_on=pk_col, \n            right_index=True, \n            how=\"left\"\n        ).fillna(0)\n\n        X = count_df[\"child_count\"].to_numpy(dtype=float).reshape(-1, 1)\n        self.max_children = int(X.max())\n\n        if self.method == \"gmm\":\n            # Using GMM to learn continuous approximation of counts\n            # Could also use NegativeBinomial or KDE\n            self.model = GaussianMixture(n_components=min(5, len(np.unique(X))), random_state=42)\n            self.model.fit(X)\n\n    def sample_counts(self, parent_context: pd.DataFrame) -&gt; np.ndarray:\n        \"\"\"Sample child counts for a set of parents.\n\n        Args:\n            parent_context: Parent dataframe (only length is used here).\n\n        Returns:\n            Numpy array of integer child counts aligned with parents.\n\n        Raises:\n            ValueError: If called before fitting the model.\n        \"\"\"\n        n_samples = len(parent_context)\n        if self.model is None:\n            raise ValueError(\"LinkageModel not fitted\")\n\n        # Sample from GMM\n        counts, _ = self.model.sample(n_samples)\n\n        # Post-process: Round to nearest int, clip to [0, max]\n        counts = np.round(counts).flatten().astype(int)\n        counts = np.clip(counts, 0, None) # Ensure non-negative\n\n        return counts\n</code></pre>"},{"location":"api/relational/#syntho_hive.relational.linkage.LinkageModel.fit","title":"fit","text":"<pre><code>fit(parent_df: DataFrame, child_df: DataFrame, fk_col: str, pk_col: str = 'id')\n</code></pre> <p>Fit the distribution of child counts per parent.</p> <p>Parameters:</p> Name Type Description Default <code>parent_df</code> <code>DataFrame</code> <p>Parent table with unique primary keys.</p> required <code>child_df</code> <code>DataFrame</code> <p>Child table containing foreign keys to parents.</p> required <code>fk_col</code> <code>str</code> <p>Name of the foreign key column in the child table.</p> required <code>pk_col</code> <code>str</code> <p>Name of the primary key column in the parent table.</p> <code>'id'</code> Source code in <code>syntho_hive/relational/linkage.py</code> <pre><code>def fit(self, parent_df: pd.DataFrame, child_df: pd.DataFrame, fk_col: str, pk_col: str = \"id\"):\n    \"\"\"Fit the distribution of child counts per parent.\n\n    Args:\n        parent_df: Parent table with unique primary keys.\n        child_df: Child table containing foreign keys to parents.\n        fk_col: Name of the foreign key column in the child table.\n        pk_col: Name of the primary key column in the parent table.\n    \"\"\"\n    # 1. Aggregate child counts\n    # Assumes parent_df has unique PKs\n    counts = child_df[fk_col].value_counts()\n\n    # Merge with all parents to include 0-count parents\n    parent_ids = pd.DataFrame(parent_df[pk_col].unique(), columns=[pk_col])\n\n    # Ensure Types Match (Cast Child FK to Parent PK type)\n    try:\n        target_type = parent_ids[pk_col].dtype\n        if not np.issubdtype(counts.index.dtype, target_type):\n            counts.index = counts.index.astype(target_type)\n    except Exception as e:\n        # Fallback to string if direct cast fails\n        print(f\"Warning: Could not cast FK to match PK type ({e}). Falling back to string.\")\n        parent_ids[pk_col] = parent_ids[pk_col].astype(str)\n        counts.index = counts.index.astype(str)\n\n    # Use merge to get counts, fillna(0) for parents with no children\n    # Note: In real Spark env this aggregation happens differently\n    count_df = parent_ids.merge(\n        counts.rename(\"child_count\"), \n        left_on=pk_col, \n        right_index=True, \n        how=\"left\"\n    ).fillna(0)\n\n    X = count_df[\"child_count\"].to_numpy(dtype=float).reshape(-1, 1)\n    self.max_children = int(X.max())\n\n    if self.method == \"gmm\":\n        # Using GMM to learn continuous approximation of counts\n        # Could also use NegativeBinomial or KDE\n        self.model = GaussianMixture(n_components=min(5, len(np.unique(X))), random_state=42)\n        self.model.fit(X)\n</code></pre>"},{"location":"api/relational/#syntho_hive.relational.linkage.LinkageModel.sample_counts","title":"sample_counts","text":"<pre><code>sample_counts(parent_context: DataFrame) -&gt; np.ndarray\n</code></pre> <p>Sample child counts for a set of parents.</p> <p>Parameters:</p> Name Type Description Default <code>parent_context</code> <code>DataFrame</code> <p>Parent dataframe (only length is used here).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Numpy array of integer child counts aligned with parents.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If called before fitting the model.</p> Source code in <code>syntho_hive/relational/linkage.py</code> <pre><code>def sample_counts(self, parent_context: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Sample child counts for a set of parents.\n\n    Args:\n        parent_context: Parent dataframe (only length is used here).\n\n    Returns:\n        Numpy array of integer child counts aligned with parents.\n\n    Raises:\n        ValueError: If called before fitting the model.\n    \"\"\"\n    n_samples = len(parent_context)\n    if self.model is None:\n        raise ValueError(\"LinkageModel not fitted\")\n\n    # Sample from GMM\n    counts, _ = self.model.sample(n_samples)\n\n    # Post-process: Round to nearest int, clip to [0, max]\n    counts = np.round(counts).flatten().astype(int)\n    counts = np.clip(counts, 0, None) # Ensure non-negative\n\n    return counts\n</code></pre>"},{"location":"api/relational/#graph-schema","title":"Graph &amp; Schema","text":""},{"location":"api/relational/#syntho_hive.relational.graph.SchemaGraph","title":"syntho_hive.relational.graph.SchemaGraph","text":"<p>DAG representation of table dependencies derived from metadata.</p> Source code in <code>syntho_hive/relational/graph.py</code> <pre><code>class SchemaGraph:\n    \"\"\"DAG representation of table dependencies derived from metadata.\"\"\"\n    def __init__(self, metadata: Metadata):\n        \"\"\"Create a dependency graph from table metadata.\n\n        Args:\n            metadata: Dataset metadata containing FK relationships.\n        \"\"\"\n        self.metadata = metadata\n        self.adj_list: Dict[str, Set[str]] = {}\n        self._build_graph()\n\n    def _build_graph(self):\n        \"\"\"Build an adjacency list from FK relationships.\"\"\"\n        for table_name in self.metadata.tables:\n            self.adj_list[table_name] = set()\n\n        for table_name, config in self.metadata.tables.items():\n            for ref_col, ref_path in config.fk.items():\n                parent_table, _ = ref_path.split(\".\")\n                # Dependency: Parent -&gt; Child (we generate Parent first)\n                if parent_table in self.adj_list:\n                    self.adj_list[parent_table].add(table_name)\n\n    def get_generation_order(self) -&gt; List[str]:\n        \"\"\"Return a topologically sorted list of tables.\n\n        Returns:\n            List of table names ordered for parent-before-child generation.\n\n        Raises:\n            ValueError: If a cycle is detected in FK relationships.\n        \"\"\"\n        visited = set()\n        stack = []\n        path = set()\n\n        def visit(node):\n            if node in path:\n                raise ValueError(f\"Cycle detected involving {node}\")\n            if node in visited:\n                return\n\n            path.add(node)\n            visited.add(node)\n\n            # Note: For generation order (Parent -&gt; Child), we want to visit parents, then children.\n            # Standard topological sort gives reverse dependency order if edge is Dependency -&gt; Dependent\n            # Here Edge is Parent -&gt; Child. So generic topological sort:\n            # Visit Parent, allow it to finish, add to stack? No.\n            # If A -&gt; B (A is parent of B).\n            # We want [A, B].\n            # Normal DFS topo sort on A -&gt; B puts B on stack, then A. Stack: [A, B] (LIFO) -&gt; Pop A, Pop B. \n            # Yes, standard topological sort on (Parent -&gt; Child) edges returns [Parent, Child].\n\n            for neighbor in self.adj_list.get(node, []):\n                visit(neighbor)\n\n            path.remove(node)\n            stack.append(node)\n\n        # Iterate over all nodes, not just roots, to catch disconnected components\n        # Sort keys for deterministic order\n        for node in sorted(self.adj_list.keys()):\n            visit(node)\n\n        return stack[::-1] # Reverse stack to get topological order\n</code></pre>"},{"location":"api/relational/#syntho_hive.relational.graph.SchemaGraph.get_generation_order","title":"get_generation_order","text":"<pre><code>get_generation_order() -&gt; List[str]\n</code></pre> <p>Return a topologically sorted list of tables.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of table names ordered for parent-before-child generation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a cycle is detected in FK relationships.</p> Source code in <code>syntho_hive/relational/graph.py</code> <pre><code>def get_generation_order(self) -&gt; List[str]:\n    \"\"\"Return a topologically sorted list of tables.\n\n    Returns:\n        List of table names ordered for parent-before-child generation.\n\n    Raises:\n        ValueError: If a cycle is detected in FK relationships.\n    \"\"\"\n    visited = set()\n    stack = []\n    path = set()\n\n    def visit(node):\n        if node in path:\n            raise ValueError(f\"Cycle detected involving {node}\")\n        if node in visited:\n            return\n\n        path.add(node)\n        visited.add(node)\n\n        # Note: For generation order (Parent -&gt; Child), we want to visit parents, then children.\n        # Standard topological sort gives reverse dependency order if edge is Dependency -&gt; Dependent\n        # Here Edge is Parent -&gt; Child. So generic topological sort:\n        # Visit Parent, allow it to finish, add to stack? No.\n        # If A -&gt; B (A is parent of B).\n        # We want [A, B].\n        # Normal DFS topo sort on A -&gt; B puts B on stack, then A. Stack: [A, B] (LIFO) -&gt; Pop A, Pop B. \n        # Yes, standard topological sort on (Parent -&gt; Child) edges returns [Parent, Child].\n\n        for neighbor in self.adj_list.get(node, []):\n            visit(neighbor)\n\n        path.remove(node)\n        stack.append(node)\n\n    # Iterate over all nodes, not just roots, to catch disconnected components\n    # Sort keys for deterministic order\n    for node in sorted(self.adj_list.keys()):\n        visit(node)\n\n    return stack[::-1] # Reverse stack to get topological order\n</code></pre>"},{"location":"api/validation/","title":"Validation","text":""},{"location":"api/validation/#syntho_hive.validation.report_generator.ValidationReport","title":"syntho_hive.validation.report_generator.ValidationReport","text":"<p>Generate summary reports of validation metrics.</p> Source code in <code>syntho_hive/validation/report_generator.py</code> <pre><code>class ValidationReport:\n    \"\"\"Generate summary reports of validation metrics.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize statistical validator and metric store.\"\"\"\n        self.validator = StatisticalValidator()\n        self.metrics = {}\n\n    def _calculate_detailed_stats(self, real_df: pd.DataFrame, synth_df: pd.DataFrame) -&gt; Dict[str, Any]:\n        \"\"\"Calculate descriptive statistics for side-by-side comparison.\n\n        Args:\n            real_df: Real dataframe.\n            synth_df: Synthetic dataframe aligned to the real columns.\n\n        Returns:\n            Nested dict of summary stats for each column.\n        \"\"\"\n        stats = {}\n        for col in real_df.columns:\n            if col not in synth_df.columns:\n                continue\n\n            col_stats = {\"real\": {}, \"synth\": {}}\n\n            for name, df, res in [(\"real\", real_df, col_stats[\"real\"]), (\"synth\", synth_df, col_stats[\"synth\"])]:\n                series = df[col]\n                if pd.api.types.is_numeric_dtype(series):\n                    res[\"mean\"] = series.mean()\n                    res[\"std\"] = series.std()\n                    res[\"min\"] = series.min()\n                    res[\"max\"] = series.max()\n                else:\n                    res[\"unique_count\"] = series.nunique()\n                    res[\"top_value\"] = series.mode().iloc[0] if not series.mode().empty else \"N/A\"\n                    res[\"top_freq\"] = series.value_counts().iloc[0] if not series.empty else 0\n\n            stats[col] = col_stats\n        return stats\n\n    def generate(self, real_data: Dict[str, pd.DataFrame], synth_data: Dict[str, pd.DataFrame], output_path: str):\n        \"\"\"Run validation and save a report.\n\n        Args:\n            real_data: Mapping of table name to real dataframe.\n            synth_data: Mapping of table name to synthetic dataframe.\n            output_path: Destination path for HTML or JSON report.\n        \"\"\"\n        report = {\n            \"tables\": {},\n            \"summary\": \"Validation Report\"\n        }\n\n        for table_name, real_df in real_data.items():\n            if table_name not in synth_data:\n                continue\n\n            synth_df = synth_data[table_name]\n\n            # 1. Column comparisons\n            col_metrics = self.validator.compare_columns(real_df, synth_df)\n\n            # 2. Correlation\n            corr_diff = self.validator.check_correlations(real_df, synth_df)\n\n            # 3. Detailed Stats\n            stats = self._calculate_detailed_stats(real_df, synth_df)\n\n            # 4. Data Preview\n            # Use Pandas to_html for easy formatting, strict constraints\n            preview = {\n                \"real_html\": real_df.head(10).to_html(index=False, classes='scroll-table', border=0),\n                \"synth_html\": synth_df.head(10).to_html(index=False, classes='scroll-table', border=0)\n            }\n\n            report[\"tables\"][table_name] = {\n                \"column_metrics\": col_metrics,\n                \"correlation_distance\": corr_diff,\n                \"detailed_stats\": stats,\n                \"preview\": preview\n            }\n\n        if output_path.endswith(\".html\"):\n            self._save_html(report, output_path)\n        else:\n            # Save to JSON for now (PDF requires more deps)\n            with open(output_path, \"w\") as f:\n                json.dump(report, f, indent=2, default=str)\n\n        import os\n        print(f\"Report saved to {os.path.abspath(output_path)}\")\n\n    def _save_html(self, report: Dict[str, Any], output_path: str):\n        \"\"\"Render a rich HTML report with metric explanations, stats, and previews.\n\n        Args:\n            report: Structured report dictionary produced by ``generate``.\n            output_path: Filesystem path to write the HTML file.\n        \"\"\"\n        html_content = [\n            \"\"\"&lt;html&gt;\n            &lt;head&gt;\n                &lt;style&gt;\n                    body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 20px; background-color: #f9f9f9; color: #333; }\n                    h1, h2, h3 { color: #2c3e50; }\n                    .container { max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }\n\n                    /* Tables */\n                    table { border-collapse: collapse; width: 100%; margin-bottom: 20px; font-size: 14px; }\n                    th, td { border: 1px solid #e1e4e8; padding: 10px; text-align: left; }\n                    th { background-color: #f1f8ff; color: #0366d6; font-weight: 600; }\n                    tr:nth-child(even) { background-color: #f8f9fa; }\n\n                    /* Status Colors */\n                    .pass { color: #28a745; font-weight: bold; }\n                    .fail { color: #dc3545; font-weight: bold; }\n\n                    /* Layout */\n                    .section { margin-top: 40px; border-top: 1px solid #eee; padding-top: 20px; }\n                    .metric-box { background: #f0f4f8; padding: 15px; border-radius: 5px; margin-bottom: 20px; border-left: 5px solid #0366d6; }\n                    .row { display: flex; gap: 20px; }\n                    .col { flex: 1; overflow-x: auto; }\n\n                    /* Tabs/Previews */\n                    .preview-header { font-weight: bold; margin-bottom: 10px; color: #555; }\n                    .scroll-table { max-height: 400px; overflow-y: auto; display: block; }\n                &lt;/style&gt;\n            &lt;/head&gt;\n            &lt;body&gt;\n            &lt;div class=\"container\"&gt;\n                &lt;h1&gt;Validation Report&lt;/h1&gt;\n\n                &lt;div class=\"metric-box\"&gt;\n                    &lt;h3&gt;Metric Explanations&lt;/h3&gt;\n                    &lt;ul&gt;\n                        &lt;li&gt;&lt;strong&gt;KS Test (Kolmogorov-Smirnov):&lt;/strong&gt; Used for continuous numerical columns. Compares the cumulative distribution functions of the real and synthetic data. &lt;br&gt;\n                            &lt;em&gt;Result:&lt;/em&gt; Returns a p-value. If p &gt; 0.05, we fail to reject the null hypothesis (i.e., distributions are likely the same).&lt;/li&gt;\n                        &lt;li&gt;&lt;strong&gt;TVD (Total Variation Distance):&lt;/strong&gt; Used for categorical or discrete columns. Measures the maximum difference between probabilities assigned to the same event by two distributions. &lt;br&gt;\n                            &lt;em&gt;Result:&lt;/em&gt; Value between 0 and 1. Lower is better (0 means identical). We consider &lt; 0.1 as passing.&lt;/li&gt;\n                        &lt;li&gt;&lt;strong&gt;Correlation Distance:&lt;/strong&gt; Measures how well the pairwise correlations between numerical columns are preserved. Calculated as the Frobenius norm of the difference between correlation matrices. &lt;br&gt;\n                            &lt;em&gt;Result:&lt;/em&gt; Lower is better (0 means identical correlation structure).&lt;/li&gt;\n                    &lt;/ul&gt;\n                &lt;/div&gt;\n            \"\"\"]\n\n        for table_name, data in report[\"tables\"].items():\n            html_content.append(f\"&lt;div class='section'&gt;&lt;h2&gt;Table: {table_name}&lt;/h2&gt;\")\n\n            # --- 1. Correlation &amp; Overall ---\n            corr_dist = data.get('correlation_distance', 0.0)\n            html_content.append(f\"&lt;p&gt;&lt;strong&gt;Correlation Distance:&lt;/strong&gt; {corr_dist:.4f}&lt;/p&gt;\")\n\n            # --- 2. Column Metrics ---\n            html_content.append(\"&lt;h3&gt;Column Validation Metrics&lt;/h3&gt;\")\n            html_content.append(\"&lt;table&gt;&lt;tr&gt;&lt;th&gt;Column&lt;/th&gt;&lt;th&gt;Test Type&lt;/th&gt;&lt;th&gt;Statistic&lt;/th&gt;&lt;th&gt;P-Value / Score&lt;/th&gt;&lt;th&gt;Status&lt;/th&gt;&lt;/tr&gt;\")\n\n            for col, metrics in data[\"column_metrics\"].items():\n                if \"error\" in metrics:\n                    html_content.append(f\"&lt;tr&gt;&lt;td&gt;{col}&lt;/td&gt;&lt;td colspan='4' class='fail'&gt;Error: {metrics['error']}&lt;/td&gt;&lt;/tr&gt;\")\n                    continue\n\n                status = \"PASS\" if metrics.get(\"passed\", False) else \"FAIL\"\n                cls = \"pass\" if status == \"PASS\" else \"fail\"\n\n                stat = f\"{metrics.get('statistic', 0):.4f}\"\n                # TVD doesn't have a p-value, KS does.\n                pval = f\"{metrics.get('p_value', 0):.4f}\" if metrics.get('p_value') is not None else \"N/A\"\n                test_name = metrics.get('test', 'N/A')\n\n                html_content.append(f\"&lt;tr&gt;&lt;td&gt;{col}&lt;/td&gt;&lt;td&gt;{test_name}&lt;/td&gt;&lt;td&gt;{stat}&lt;/td&gt;&lt;td&gt;{pval}&lt;/td&gt;&lt;td class='{cls}'&gt;{status}&lt;/td&gt;&lt;/tr&gt;\")\n\n            html_content.append(\"&lt;/table&gt;\")\n\n            # --- 3. Detailed Statistics ---\n            if \"detailed_stats\" in data:\n                html_content.append(\"&lt;h3&gt;Detailed Statistics (Real vs Synthetic)&lt;/h3&gt;\")\n                html_content.append(\"&lt;table&gt;&lt;tr&gt;&lt;th&gt;Column&lt;/th&gt;&lt;th&gt;Metric&lt;/th&gt;&lt;th&gt;Real&lt;/th&gt;&lt;th&gt;Synthetic&lt;/th&gt;&lt;/tr&gt;\")\n\n                for col, stats in data[\"detailed_stats\"].items():\n                    # stats has \"real\": {...}, \"synth\": {...}\n                    real_s = stats.get(\"real\", {})\n                    synth_s = stats.get(\"synth\", {})\n\n                    # Merge keys to show\n                    all_keys = sorted(list(set(real_s.keys()) | set(synth_s.keys())))\n                    # Usually we want mean, std, min, max or unique, top\n\n                    first = True\n                    for k in all_keys:\n                        r_val = real_s.get(k, \"-\")\n                        s_val = synth_s.get(k, \"-\")\n\n                        # Format floats\n                        if isinstance(r_val, (float, np.floating)): r_val = f\"{r_val:.4f}\"\n                        if isinstance(s_val, (float, np.floating)): s_val = f\"{s_val:.4f}\"\n\n                        row_start = f\"&lt;tr&gt;&lt;td rowspan='{len(all_keys)}'&gt;{col}&lt;/td&gt;\" if first else \"&lt;tr&gt;\"\n                        row_end = f\"&lt;td&gt;{k}&lt;/td&gt;&lt;td&gt;{r_val}&lt;/td&gt;&lt;td&gt;{s_val}&lt;/td&gt;&lt;/tr&gt;\"\n                        html_content.append(row_start + row_end)\n                        first = False\n                html_content.append(\"&lt;/table&gt;\")\n\n            # --- 4. Data Preview ---\n            if \"preview\" in data:\n                html_content.append(\"&lt;h3&gt;Data Preview (First 10 Rows)&lt;/h3&gt;\")\n                html_content.append(\"&lt;div class='row'&gt;\")\n\n                # Real\n                html_content.append(\"&lt;div class='col'&gt;\")\n                html_content.append(\"&lt;div class='preview-header'&gt;Original Data (Real)&lt;/div&gt;\")\n                html_content.append(data[\"preview\"][\"real_html\"])\n                html_content.append(\"&lt;/div&gt;\")\n\n                # Synth\n                html_content.append(\"&lt;div class='col'&gt;\")\n                html_content.append(\"&lt;div class='preview-header'&gt;Synthetic Data (Generated)&lt;/div&gt;\")\n                html_content.append(data[\"preview\"][\"synth_html\"])\n                html_content.append(\"&lt;/div&gt;\")\n\n                html_content.append(\"&lt;/div&gt;\") # End row\n\n            html_content.append(\"&lt;/div&gt;\") # End section\n\n        html_content.append(\"&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;\")\n\n        with open(output_path, \"w\") as f:\n            f.write(\"\\n\".join(html_content))\n</code></pre>"},{"location":"api/validation/#syntho_hive.validation.report_generator.ValidationReport.generate","title":"generate","text":"<pre><code>generate(real_data: Dict[str, DataFrame], synth_data: Dict[str, DataFrame], output_path: str)\n</code></pre> <p>Run validation and save a report.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>Dict[str, DataFrame]</code> <p>Mapping of table name to real dataframe.</p> required <code>synth_data</code> <code>Dict[str, DataFrame]</code> <p>Mapping of table name to synthetic dataframe.</p> required <code>output_path</code> <code>str</code> <p>Destination path for HTML or JSON report.</p> required Source code in <code>syntho_hive/validation/report_generator.py</code> <pre><code>def generate(self, real_data: Dict[str, pd.DataFrame], synth_data: Dict[str, pd.DataFrame], output_path: str):\n    \"\"\"Run validation and save a report.\n\n    Args:\n        real_data: Mapping of table name to real dataframe.\n        synth_data: Mapping of table name to synthetic dataframe.\n        output_path: Destination path for HTML or JSON report.\n    \"\"\"\n    report = {\n        \"tables\": {},\n        \"summary\": \"Validation Report\"\n    }\n\n    for table_name, real_df in real_data.items():\n        if table_name not in synth_data:\n            continue\n\n        synth_df = synth_data[table_name]\n\n        # 1. Column comparisons\n        col_metrics = self.validator.compare_columns(real_df, synth_df)\n\n        # 2. Correlation\n        corr_diff = self.validator.check_correlations(real_df, synth_df)\n\n        # 3. Detailed Stats\n        stats = self._calculate_detailed_stats(real_df, synth_df)\n\n        # 4. Data Preview\n        # Use Pandas to_html for easy formatting, strict constraints\n        preview = {\n            \"real_html\": real_df.head(10).to_html(index=False, classes='scroll-table', border=0),\n            \"synth_html\": synth_df.head(10).to_html(index=False, classes='scroll-table', border=0)\n        }\n\n        report[\"tables\"][table_name] = {\n            \"column_metrics\": col_metrics,\n            \"correlation_distance\": corr_diff,\n            \"detailed_stats\": stats,\n            \"preview\": preview\n        }\n\n    if output_path.endswith(\".html\"):\n        self._save_html(report, output_path)\n    else:\n        # Save to JSON for now (PDF requires more deps)\n        with open(output_path, \"w\") as f:\n            json.dump(report, f, indent=2, default=str)\n\n    import os\n    print(f\"Report saved to {os.path.abspath(output_path)}\")\n</code></pre>"},{"location":"api/validation/#syntho_hive.validation.statistical.StatisticalValidator","title":"syntho_hive.validation.statistical.StatisticalValidator","text":"<p>Perform statistical checks between real and synthetic data.</p> Source code in <code>syntho_hive/validation/statistical.py</code> <pre><code>class StatisticalValidator:\n    \"\"\"Perform statistical checks between real and synthetic data.\"\"\"\n\n    def compare_columns(self, real_df: pd.DataFrame, synth_df: pd.DataFrame) -&gt; Dict[str, Any]:\n        \"\"\"Compare column-wise distributions between real and synthetic data.\n\n        Args:\n            real_df: Real dataframe.\n            synth_df: Synthetic dataframe aligned to the same schema.\n\n        Returns:\n            Mapping of column name to test results or error descriptions.\n        \"\"\"\n        results = {}\n\n        if real_df.empty or synth_df.empty:\n            return {\"error\": \"One or both DataFrames are empty.\"}\n\n        for col in real_df.columns:\n            if col not in synth_df.columns:\n                results[col] = {\"error\": \"Column missing in synthetic data\"}\n                continue\n\n            real_data = real_df[col].dropna()\n            synth_data = synth_df[col].dropna()\n\n            if real_data.empty or synth_data.empty:\n                results[col] = {\"error\": \"Column data is empty after dropping NaNs\"}\n                continue\n\n            # Check for type mismatch\n            if real_data.dtype != synth_data.dtype:\n                # Try to cast if compatible (e.g. float vs int)\n                if pd.api.types.is_numeric_dtype(real_data) and pd.api.types.is_numeric_dtype(synth_data):\n                    pass # Compatible enough for stats\n                else:\n                    results[col] = {\"error\": f\"Type mismatch: Real {real_data.dtype} vs Synth {synth_data.dtype}\"}\n                    continue\n\n            if pd.api.types.is_numeric_dtype(real_data):\n                # KS Test\n                try:\n                    stat, p_value = ks_2samp(real_data, synth_data)\n                    results[col] = {\n                        \"test\": \"ks_test\",\n                        \"statistic\": stat,\n                        \"p_value\": p_value,\n                        \"passed\": p_value &gt; 0.05 # Null hypothesis: Same distribution\n                    }\n                except Exception as e:\n                    results[col] = {\"error\": f\"KS Test failed: {str(e)}\"}\n            else:\n                # TVD (Total Variation Distance)\n                try:\n                    real_counts = real_data.value_counts(normalize=True)\n                    synth_counts = synth_data.value_counts(normalize=True)\n\n                    # Align categories\n                    all_cats = set(real_counts.index).union(set(synth_counts.index))\n\n                    tvd = 0.5 * sum(abs(real_counts.get(c, 0) - synth_counts.get(c, 0)) for c in all_cats)\n\n                    results[col] = {\n                        \"test\": \"tvd\",\n                        \"statistic\": tvd,\n                        \"passed\": tvd &lt; 0.1 # Threshold arbitrary\n                    }\n                except Exception as e:\n                    results[col] = {\"error\": f\"TVD Checks failed: {str(e)}\"}\n\n        return results\n\n    def check_correlations(self, real_df: pd.DataFrame, synth_df: pd.DataFrame) -&gt; float:\n        \"\"\"Compare correlation matrices using Frobenius norm.\n\n        Args:\n            real_df: Real dataframe.\n            synth_df: Synthetic dataframe.\n\n        Returns:\n            Frobenius norm distance between correlation matrices (0 when identical).\n        \"\"\"\n        # Numeric only\n        real_corr = real_df.select_dtypes(include=[np.number]).corr().fillna(0)\n        synth_corr = synth_df.select_dtypes(include=[np.number]).corr().fillna(0)\n\n        if real_corr.empty or synth_corr.empty:\n            return 0.0\n\n        diff = real_corr - synth_corr\n        frobenius_norm = np.linalg.norm(diff.values)\n\n        return float(frobenius_norm)\n</code></pre>"},{"location":"api/validation/#syntho_hive.validation.statistical.StatisticalValidator.check_correlations","title":"check_correlations","text":"<pre><code>check_correlations(real_df: DataFrame, synth_df: DataFrame) -&gt; float\n</code></pre> <p>Compare correlation matrices using Frobenius norm.</p> <p>Parameters:</p> Name Type Description Default <code>real_df</code> <code>DataFrame</code> <p>Real dataframe.</p> required <code>synth_df</code> <code>DataFrame</code> <p>Synthetic dataframe.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Frobenius norm distance between correlation matrices (0 when identical).</p> Source code in <code>syntho_hive/validation/statistical.py</code> <pre><code>def check_correlations(self, real_df: pd.DataFrame, synth_df: pd.DataFrame) -&gt; float:\n    \"\"\"Compare correlation matrices using Frobenius norm.\n\n    Args:\n        real_df: Real dataframe.\n        synth_df: Synthetic dataframe.\n\n    Returns:\n        Frobenius norm distance between correlation matrices (0 when identical).\n    \"\"\"\n    # Numeric only\n    real_corr = real_df.select_dtypes(include=[np.number]).corr().fillna(0)\n    synth_corr = synth_df.select_dtypes(include=[np.number]).corr().fillna(0)\n\n    if real_corr.empty or synth_corr.empty:\n        return 0.0\n\n    diff = real_corr - synth_corr\n    frobenius_norm = np.linalg.norm(diff.values)\n\n    return float(frobenius_norm)\n</code></pre>"},{"location":"api/validation/#syntho_hive.validation.statistical.StatisticalValidator.compare_columns","title":"compare_columns","text":"<pre><code>compare_columns(real_df: DataFrame, synth_df: DataFrame) -&gt; Dict[str, Any]\n</code></pre> <p>Compare column-wise distributions between real and synthetic data.</p> <p>Parameters:</p> Name Type Description Default <code>real_df</code> <code>DataFrame</code> <p>Real dataframe.</p> required <code>synth_df</code> <code>DataFrame</code> <p>Synthetic dataframe aligned to the same schema.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Mapping of column name to test results or error descriptions.</p> Source code in <code>syntho_hive/validation/statistical.py</code> <pre><code>def compare_columns(self, real_df: pd.DataFrame, synth_df: pd.DataFrame) -&gt; Dict[str, Any]:\n    \"\"\"Compare column-wise distributions between real and synthetic data.\n\n    Args:\n        real_df: Real dataframe.\n        synth_df: Synthetic dataframe aligned to the same schema.\n\n    Returns:\n        Mapping of column name to test results or error descriptions.\n    \"\"\"\n    results = {}\n\n    if real_df.empty or synth_df.empty:\n        return {\"error\": \"One or both DataFrames are empty.\"}\n\n    for col in real_df.columns:\n        if col not in synth_df.columns:\n            results[col] = {\"error\": \"Column missing in synthetic data\"}\n            continue\n\n        real_data = real_df[col].dropna()\n        synth_data = synth_df[col].dropna()\n\n        if real_data.empty or synth_data.empty:\n            results[col] = {\"error\": \"Column data is empty after dropping NaNs\"}\n            continue\n\n        # Check for type mismatch\n        if real_data.dtype != synth_data.dtype:\n            # Try to cast if compatible (e.g. float vs int)\n            if pd.api.types.is_numeric_dtype(real_data) and pd.api.types.is_numeric_dtype(synth_data):\n                pass # Compatible enough for stats\n            else:\n                results[col] = {\"error\": f\"Type mismatch: Real {real_data.dtype} vs Synth {synth_data.dtype}\"}\n                continue\n\n        if pd.api.types.is_numeric_dtype(real_data):\n            # KS Test\n            try:\n                stat, p_value = ks_2samp(real_data, synth_data)\n                results[col] = {\n                    \"test\": \"ks_test\",\n                    \"statistic\": stat,\n                    \"p_value\": p_value,\n                    \"passed\": p_value &gt; 0.05 # Null hypothesis: Same distribution\n                }\n            except Exception as e:\n                results[col] = {\"error\": f\"KS Test failed: {str(e)}\"}\n        else:\n            # TVD (Total Variation Distance)\n            try:\n                real_counts = real_data.value_counts(normalize=True)\n                synth_counts = synth_data.value_counts(normalize=True)\n\n                # Align categories\n                all_cats = set(real_counts.index).union(set(synth_counts.index))\n\n                tvd = 0.5 * sum(abs(real_counts.get(c, 0) - synth_counts.get(c, 0)) for c in all_cats)\n\n                results[col] = {\n                    \"test\": \"tvd\",\n                    \"statistic\": tvd,\n                    \"passed\": tvd &lt; 0.1 # Threshold arbitrary\n                }\n            except Exception as e:\n                results[col] = {\"error\": f\"TVD Checks failed: {str(e)}\"}\n\n    return results\n</code></pre>"},{"location":"demos/01_single_table_ctgan/","title":"Single Table CTGAN Demo","text":"<p>Path: <code>examples/demos/01_single_table_ctgan</code></p>"},{"location":"demos/01_single_table_ctgan/#goal","title":"Goal","text":"<p>Train a CTGAN on a single table with mixed types (numerical, categorical) and generate synthetic data.</p>"},{"location":"demos/01_single_table_ctgan/#run","title":"Run","text":"<pre><code>python examples/demos/01_single_table_ctgan/run.py\n</code></pre>"},{"location":"demos/01_single_table_ctgan/#source-code","title":"Source Code","text":"<pre><code>from pathlib import Path\nimport argparse\nimport numpy as np\nimport pandas as pd\n\nfrom syntho_hive.interface.config import Metadata\nfrom syntho_hive.core.models.ctgan import CTGAN\n\n\ndef build_training_data(num_rows: int = 600) -&gt; pd.DataFrame:\n    \"\"\"Create a small, mixed-type table to keep the demo self contained.\"\"\"\n    rng = np.random.default_rng(42)\n    cities = [\"NYC\", \"SF\", \"SEA\", \"CHI\", \"DAL\", \"MIA\"]\n    channels = [\"web\", \"retail\", \"partner\"]\n\n    df = pd.DataFrame(\n        {\n            \"customer_id\": np.arange(1, num_rows + 1),\n            \"age\": rng.integers(18, 75, size=num_rows),\n            \"annual_spend\": rng.normal(55000, 15000, size=num_rows).clip(5000, 150000).round(2),\n            \"city\": rng.choice(cities, p=[0.28, 0.22, 0.18, 0.14, 0.1, 0.08], size=num_rows),\n            \"signup_channel\": rng.choice(channels, p=[0.6, 0.3, 0.1], size=num_rows),\n            \"loyalty_score\": rng.uniform(0, 1, size=num_rows).round(4),\n        }\n    )\n    return df\n\n\ndef configure_metadata() -&gt; Metadata:\n    \"\"\"Tell SynthoHive which columns are structural (PK) vs. modeled.\"\"\"\n    meta = Metadata()\n    meta.add_table(\n        name=\"customers\",\n        pk=\"customer_id\",\n        pii_cols=[\"customer_id\"],\n        high_cardinality_cols=[\"city\"],\n    )\n    return meta\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Train a CTGAN on a single table.\")\n    parser.add_argument(\"--epochs\", type=int, default=5, help=\"Training epochs for the GAN.\")\n    parser.add_argument(\"--rows\", type=int, default=200, help=\"Number of synthetic rows to generate.\")\n    parser.add_argument(\n        \"--output-dir\",\n        default=\"examples/demos/01_single_table_ctgan/outputs\",\n        help=\"Directory where outputs will be written.\",\n    )\n    args = parser.parse_args()\n\n    output_dir = Path(args.output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    print(\"Building training data...\")\n    train_df = build_training_data()\n    meta = configure_metadata()\n\n    print(\"Fitting CTGAN (small config for demo speed)...\")\n    model = CTGAN(\n        meta,\n        batch_size=128,\n        epochs=args.epochs,\n        embedding_dim=64,\n        generator_dim=(128, 128),\n        discriminator_dim=(128, 128),\n        device=\"cpu\",\n    )\n    model.fit(train_df, table_name=\"customers\")\n\n    print(f\"Sampling {args.rows} synthetic rows...\")\n    synthetic_df = model.sample(args.rows)\n    synthetic_df.insert(0, \"customer_id\", range(1, len(synthetic_df) + 1))\n\n    output_path = output_dir / \"synthetic_customers.csv\"\n    synthetic_df.to_csv(output_path, index=False)\n    print(f\"Wrote synthetic data to {output_path}\")\n    print(synthetic_df.head())\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"demos/01_single_table_ctgan/#outputs","title":"Outputs","text":"<ul> <li><code>outputs/synthetic_customers.csv</code></li> </ul>"},{"location":"demos/01_single_table_ctgan/#notes","title":"Notes","text":"<ul> <li>Demonstrates basic usage of <code>CTGAN</code> and <code>Metadata</code>.</li> </ul>"},{"location":"demos/02_privacy_sanitization/","title":"Privacy Sanitization Demo","text":"<p>Path: <code>examples/demos/02_privacy_sanitization</code></p>"},{"location":"demos/02_privacy_sanitization/#goal","title":"Goal","text":"<p>Detect PII, apply sanitization (mask/hash/fake), and compare raw vs sanitized CSV outputs.</p>"},{"location":"demos/02_privacy_sanitization/#run","title":"Run","text":"<pre><code>python examples/demos/02_privacy_sanitization/run.py\n</code></pre>"},{"location":"demos/02_privacy_sanitization/#outputs","title":"Outputs","text":"<ul> <li><code>outputs/raw_users.csv</code></li> <li><code>outputs/sanitized_users.csv</code></li> </ul>"},{"location":"demos/02_privacy_sanitization/#notes","title":"Notes","text":"<ul> <li>Uses <code>PIISanitizer</code> + <code>ContextualFaker</code>.</li> <li>Adjust rules or actions in the demo script to experiment with masking vs hashing vs faking.</li> </ul>"},{"location":"demos/02_privacy_sanitization/#source-code","title":"Source Code","text":"<pre><code>from pathlib import Path\nimport argparse\nimport numpy as np\nimport pandas as pd\n\nfrom syntho_hive.privacy.sanitizer import PIISanitizer, PrivacyConfig, PiiRule\n\n\ndef make_raw_users(num_rows: int) -&gt; pd.DataFrame:\n    rng = np.random.default_rng(7)\n    first_names = [\"Alex\", \"Jordan\", \"Sam\", \"Taylor\", \"Jamie\", \"Riley\", \"Casey\", \"Drew\"]\n    last_names = [\"Lee\", \"Patel\", \"Garcia\", \"Chen\", \"Olsen\", \"Diaz\", \"Nguyen\", \"Brown\"]\n    cities = [\"NY\", \"SF\", \"SEA\", \"DAL\", \"BOS\"]\n\n    rows = []\n    for i in range(num_rows):\n        first = rng.choice(first_names)\n        last = rng.choice(last_names)\n        city = rng.choice(cities)\n        email = f\"{first.lower()}.{last.lower()}{i}@example.com\"\n        phone = f\"({rng.integers(200, 999)})-{rng.integers(200, 999)}-{rng.integers(1000, 9999)}\"\n        ssn = f\"{rng.integers(100, 999):03d}-{rng.integers(10, 99):02d}-{rng.integers(1000, 9999):04d}\"\n        credit_card = f\"{rng.integers(1000, 9999):04d}-{rng.integers(1000, 9999):04d}-{rng.integers(1000, 9999):04d}-{rng.integers(1000, 9999):04d}\"\n        loyalty_id = f\"L-{rng.integers(10_000, 99_999)}\"\n\n        rows.append(\n            {\n                \"user_id\": i + 1,\n                \"first_name\": first,\n                \"last_name\": last,\n                \"city\": city,\n                \"email\": email,\n                \"phone\": phone,\n                \"ssn\": ssn,\n                \"credit_card\": credit_card,\n                \"loyalty_id\": loyalty_id,\n                \"notes\": f\"Called support on ticket {rng.integers(1000, 9999)}\",\n            }\n        )\n\n    return pd.DataFrame(rows)\n\n\ndef build_config() -&gt; PrivacyConfig:\n    \"\"\"\n    Extend the defaults with a custom rule to hash loyalty IDs\n    instead of masking or faking them.\n    \"\"\"\n    config = PrivacyConfig.default()\n    config.rules.append(\n        PiiRule(\n            name=\"loyalty_id\",\n            patterns=[r\"L-\\d{5}\"],\n            action=\"hash\",\n        )\n    )\n    return config\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run the PII sanitization demo.\")\n    parser.add_argument(\"--rows\", type=int, default=50, help=\"How many raw rows to generate.\")\n    parser.add_argument(\n        \"--output-dir\",\n        default=\"examples/demos/02_privacy_sanitization/outputs\",\n        help=\"Directory to place raw and sanitized CSVs.\",\n    )\n    args = parser.parse_args()\n\n    output_dir = Path(args.output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    raw_df = make_raw_users(args.rows)\n    raw_path = output_dir / \"raw_users.csv\"\n    raw_df.to_csv(raw_path, index=False)\n    print(f\"Wrote raw data to {raw_path}\")\n\n    config = build_config()\n    sanitizer = PIISanitizer(config=config)\n    detected = sanitizer.analyze(raw_df)\n    print(\"Detected PII columns:\", detected)\n\n    sanitized_df = sanitizer.sanitize(raw_df, pii_map=detected)\n    sanitized_path = output_dir / \"sanitized_users.csv\"\n    sanitized_df.to_csv(sanitized_path, index=False)\n    print(f\"Wrote sanitized data to {sanitized_path}\")\n    print(sanitized_df.head())\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"demos/03_validation_report/","title":"Validation Report Demo","text":"<p>Path: <code>examples/demos/03_validation_report</code></p>"},{"location":"demos/03_validation_report/#goal","title":"Goal","text":"<p>Compare real vs synthetic tables, compute KS/TVD metrics, correlation distance, and render an HTML report.</p>"},{"location":"demos/03_validation_report/#run","title":"Run","text":"<pre><code>python examples/demos/03_validation_report/run.py\n</code></pre>"},{"location":"demos/03_validation_report/#outputs","title":"Outputs","text":"<ul> <li><code>outputs/validation_metrics.json</code></li> <li><code>outputs/validation_report.html</code></li> <li>Sample inputs: <code>outputs/real_users.csv</code>, <code>outputs/synthetic_users.csv</code></li> </ul>"},{"location":"demos/03_validation_report/#notes","title":"Notes","text":"<ul> <li>Uses <code>ValidationReport.generate</code> for both JSON and HTML formats.</li> <li>Open the HTML in a browser to inspect column-level results and previews.</li> </ul>"},{"location":"demos/03_validation_report/#source-code","title":"Source Code","text":"<pre><code>from pathlib import Path\nimport argparse\nimport numpy as np\nimport pandas as pd\n\nfrom syntho_hive.validation.report_generator import ValidationReport\n\n\ndef make_real_data(num_rows: int) -&gt; pd.DataFrame:\n    rng = np.random.default_rng(21)\n    df = pd.DataFrame(\n        {\n            \"user_id\": np.arange(1, num_rows + 1),\n            \"age\": rng.integers(18, 70, size=num_rows),\n            \"region\": rng.choice([\"NE\", \"SE\", \"MW\", \"W\"], size=num_rows, p=[0.25, 0.25, 0.25, 0.25]),\n            \"monthly_spend\": rng.normal(120, 35, size=num_rows).round(2),\n            \"active\": rng.choice([0, 1], size=num_rows, p=[0.35, 0.65]),\n        }\n    )\n    return df\n\n\ndef make_synthetic_variant(real_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Create a synthetic dataset with slight distribution shifts to highlight\n    how the validator surfaces differences.\n    \"\"\"\n    rng = np.random.default_rng(22)\n    synth = real_df.copy()\n    synth[\"monthly_spend\"] = (synth[\"monthly_spend\"] * rng.normal(1.05, 0.05, size=len(real_df))).round(2)\n    synth[\"active\"] = rng.choice([0, 1], size=len(real_df), p=[0.4, 0.6])\n    synth[\"region\"] = rng.choice([\"NE\", \"SE\", \"MW\", \"W\"], size=len(real_df), p=[0.35, 0.2, 0.25, 0.2])\n    return synth\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a validation report for real vs synthetic data.\")\n    parser.add_argument(\"--rows\", type=int, default=300, help=\"Rows per dataset to generate.\")\n    parser.add_argument(\n        \"--output-dir\",\n        default=\"examples/demos/03_validation_report/outputs\",\n        help=\"Directory where the report files will be written.\",\n    )\n    args = parser.parse_args()\n\n    output_dir = Path(args.output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    real_df = make_real_data(args.rows)\n    synthetic_df = make_synthetic_variant(real_df)\n\n    real_df.to_csv(output_dir / \"real_users.csv\", index=False)\n    synthetic_df.to_csv(output_dir / \"synthetic_users.csv\", index=False)\n\n    report = ValidationReport()\n    html_path = output_dir / \"validation_report.html\"\n    json_path = output_dir / \"validation_metrics.json\"\n\n    report.generate(real_data={\"users\": real_df}, synth_data={\"users\": synthetic_df}, output_path=str(html_path))\n    report.generate(real_data={\"users\": real_df}, synth_data={\"users\": synthetic_df}, output_path=str(json_path))\n\n    print(f\"Wrote HTML report to {html_path}\")\n    print(f\"Wrote JSON metrics to {json_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"demos/04_relational_linkage_ctgan/","title":"Relational Linkage CTGAN Demo","text":"<p>Path: <code>examples/demos/04_relational_linkage_ctgan</code></p>"},{"location":"demos/04_relational_linkage_ctgan/#goal","title":"Goal","text":"<p>Train relational CTGAN on users/orders with parent-child linkage and generate synthetic tables preserving FKs.</p>"},{"location":"demos/04_relational_linkage_ctgan/#run","title":"Run","text":"<pre><code>python examples/demos/04_relational_linkage_ctgan/run.py\n</code></pre>"},{"location":"demos/04_relational_linkage_ctgan/#outputs","title":"Outputs","text":"<ul> <li><code>outputs/users.csv</code></li> <li><code>outputs/orders.csv</code></li> </ul>"},{"location":"demos/04_relational_linkage_ctgan/#notes","title":"Notes","text":"<ul> <li>Demonstrates <code>StagedOrchestrator</code>, <code>LinkageModel</code>, and conditional CTGAN.</li> <li>Requires Spark/Delta for IO; ensure SparkSession is available.</li> </ul>"},{"location":"demos/04_relational_linkage_ctgan/#source-code","title":"Source Code","text":"<pre><code>from pathlib import Path\nimport argparse\nimport numpy as np\nimport pandas as pd\n\nfrom syntho_hive.interface.config import Metadata\nfrom syntho_hive.core.models.ctgan import CTGAN\nfrom syntho_hive.relational.linkage import LinkageModel\n\n\ndef make_parent_child_seed_data(num_parents: int = 300, max_children: int = 5):\n    \"\"\"Create a small parent/child dataset to train linkage + CTGAN.\"\"\"\n    rng = np.random.default_rng(10)\n    regions = [\"NE\", \"SE\", \"MW\", \"W\"]\n\n    parents = pd.DataFrame(\n        {\n            \"user_id\": np.arange(1, num_parents + 1),\n            \"region\": rng.choice(regions, size=num_parents, p=[0.3, 0.2, 0.25, 0.25]),\n            \"age\": rng.integers(20, 70, size=num_parents),\n        }\n    )\n\n    child_rows = []\n    order_id = 1\n    for _, row in parents.iterrows():\n        n_orders = rng.integers(0, max_children + 1)\n        for _ in range(n_orders):\n            child_rows.append(\n                {\n                    \"order_id\": order_id,\n                    \"user_id\": row[\"user_id\"],\n                    \"basket_value\": max(5, rng.normal(80, 25)),\n                    \"channel\": rng.choice([\"web\", \"store\", \"mobile\"], p=[0.5, 0.3, 0.2]),\n                }\n            )\n            order_id += 1\n\n    children = pd.DataFrame(child_rows)\n    return parents, children\n\n\ndef build_metadata() -&gt; Metadata:\n    meta = Metadata()\n    meta.add_table(name=\"users\", pk=\"user_id\", pii_cols=[], high_cardinality_cols=[\"region\"])\n    meta.add_table(\n        name=\"orders\",\n        pk=\"order_id\",\n        fk={\"user_id\": \"users.user_id\"},\n        parent_context_cols=[\"region\"],\n        constraints={\"basket_value\": {\"dtype\": \"float\", \"min\": 1.0}},\n    )\n    return meta\n\n\ndef train_models(meta: Metadata, parents: pd.DataFrame, children: pd.DataFrame, epochs: int) -&gt; tuple[CTGAN, CTGAN, LinkageModel]:\n    \"\"\"Train CTGAN for parents, linkage + conditional CTGAN for children.\"\"\"\n    users_model = CTGAN(meta, batch_size=128, epochs=epochs, generator_dim=(128, 128), discriminator_dim=(128, 128), embedding_dim=64)\n    print(\"Training users CTGAN...\")\n    users_model.fit(parents, table_name=\"users\")\n\n    linkage = LinkageModel()\n    print(\"Training linkage model...\")\n    linkage.fit(parents, children, fk_col=\"user_id\", pk_col=\"user_id\")\n\n    # Build context dataframe for child training\n    joined = children.merge(parents[[\"user_id\", \"region\"]], on=\"user_id\", how=\"left\")\n    context_df = joined[[\"region\"]].copy()\n\n    orders_model = CTGAN(meta, batch_size=128, epochs=epochs, generator_dim=(128, 128), discriminator_dim=(128, 128), embedding_dim=64)\n    print(\"Training orders CTGAN with parent context...\")\n    orders_model.fit(children, context=context_df, table_name=\"orders\")\n\n    return users_model, orders_model, linkage\n\n\ndef generate(meta: Metadata, users_model: CTGAN, orders_model: CTGAN, linkage: LinkageModel, num_parents: int, output_dir: Path):\n    print(f\"Generating {num_parents} synthetic parents...\")\n    users = users_model.sample(num_parents)\n    users.insert(0, \"user_id\", range(1, len(users) + 1))\n\n    counts = linkage.sample_counts(users)\n    total_children = int(counts.sum())\n    print(f\"Generating {total_children} synthetic children conditioned on parents...\")\n\n    # Build repeated context rows for each parent\n    context_rows = []\n    fk_values = []\n    for idx, parent in users.iterrows():\n        repeat = counts[idx]\n        if repeat &lt;= 0:\n            continue\n        fk_values.extend([parent[\"user_id\"]] * repeat)\n        context_rows.extend([{\"region\": parent[\"region\"]}] * repeat)\n\n    if total_children &gt; 0:\n        context_df = pd.DataFrame(context_rows)\n        orders = orders_model.sample(total_children, context=context_df)\n        orders.insert(0, \"order_id\", range(1, len(orders) + 1))\n        orders[\"user_id\"] = fk_values\n    else:\n        orders = pd.DataFrame(columns=[\"order_id\", \"user_id\", \"basket_value\", \"channel\", \"region\"])\n\n    users_path = output_dir / \"users.csv\"\n    orders_path = output_dir / \"orders.csv\"\n\n    users.to_csv(users_path, index=False)\n    orders.to_csv(orders_path, index=False)\n\n    print(f\"Wrote {len(users)} users to {users_path}\")\n    print(f\"Wrote {len(orders)} orders to {orders_path}\")\n    print(orders.head())\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Relational generation demo without Spark.\")\n    parser.add_argument(\"--parents\", type=int, default=150, help=\"Number of synthetic parents to generate.\")\n    parser.add_argument(\"--epochs\", type=int, default=3, help=\"Training epochs for both GANs.\")\n    parser.add_argument(\n        \"--output-dir\",\n        default=\"examples/demos/04_relational_linkage_ctgan/outputs\",\n        help=\"Directory to place generated CSVs.\",\n    )\n    args = parser.parse_args()\n\n    output_dir = Path(args.output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    meta = build_metadata()\n    parent_df, child_df = make_parent_child_seed_data()\n    users_model, orders_model, linkage = train_models(meta, parent_df, child_df, epochs=args.epochs)\n\n    generate(meta, users_model, orders_model, linkage, num_parents=args.parents, output_dir=output_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"demos/05_transformer_embeddings/","title":"Transformer Embeddings Demo","text":"<p>Path: <code>examples/demos/05_transformer_embeddings</code></p>"},{"location":"demos/05_transformer_embeddings/#goal","title":"Goal","text":"<p>Show transformer-based embeddings and recovery of transformed features.</p>"},{"location":"demos/05_transformer_embeddings/#run","title":"Run","text":"<pre><code>python examples/demos/05_transformer_embeddings/run.py\n</code></pre>"},{"location":"demos/05_transformer_embeddings/#outputs","title":"Outputs","text":"<ul> <li><code>outputs/transformed.npy</code></li> <li><code>outputs/recovered.csv</code></li> </ul>"},{"location":"demos/05_transformer_embeddings/#notes","title":"Notes","text":"<ul> <li>Highlights <code>DataTransformer</code> behavior with embedding thresholds.</li> <li>Useful for inspecting how categorical embeddings are produced and inverted.</li> </ul>"},{"location":"demos/05_transformer_embeddings/#source-code","title":"Source Code","text":"<pre><code>from pathlib import Path\nimport argparse\nimport numpy as np\nimport pandas as pd\n\nfrom syntho_hive.core.data.transformer import DataTransformer\nfrom syntho_hive.interface.config import Metadata\n\n\ndef make_data(num_rows: int, high_card: int) -&gt; pd.DataFrame:\n    rng = np.random.default_rng(100)\n    df = pd.DataFrame(\n        {\n            \"product_id\": np.arange(1, num_rows + 1),\n            \"category\": rng.choice([\"electronics\", \"apparel\", \"home\", \"toys\"], size=num_rows, p=[0.35, 0.25, 0.25, 0.15]),\n            \"brand\": rng.choice([f\"brand_{i}\" for i in range(high_card)], size=num_rows),\n            \"price\": rng.normal(80, 25, size=num_rows).clip(5, 300).round(2),\n            \"inventory\": rng.integers(0, 500, size=num_rows),\n        }\n    )\n    return df\n\n\ndef build_metadata(embedding_threshold: int) -&gt; Metadata:\n    meta = Metadata()\n    meta.add_table(\n        name=\"products\",\n        pk=\"product_id\",\n        high_cardinality_cols=[\"brand\"],\n    )\n    # DataTransformer reads embedding_threshold from the class initialization\n    # rather than the metadata field directly, but we keep metadata accurate\n    # for PK/FK stripping.\n    return meta\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Inspect DataTransformer encoding behavior.\")\n    parser.add_argument(\"--rows\", type=int, default=120, help=\"Number of rows to fabricate.\")\n    parser.add_argument(\n        \"--embedding-threshold\",\n        type=int,\n        default=20,\n        help=\"Switch to embeddings when categories exceed this threshold.\",\n    )\n    parser.add_argument(\n        \"--output-dir\",\n        default=\"examples/demos/05_transformer_embeddings/outputs\",\n        help=\"Directory to write transformed artifacts.\",\n    )\n    args = parser.parse_args()\n\n    output_dir = Path(args.output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    df = make_data(args.rows, high_card=args.embedding_threshold + 5)\n    meta = build_metadata(args.embedding_threshold)\n\n    transformer = DataTransformer(metadata=meta, embedding_threshold=args.embedding_threshold)\n    print(\"Fitting transformer (drops PK/FK automatically)...\")\n    transformer.fit(df, table_name=\"products\")\n    transformed = transformer.transform(df)\n\n    print(f\"Original shape: {df.shape}\")\n    print(f\"Transformed matrix shape: {transformed.shape}\")\n    print(f\"First 5 rows (dense matrix):\\n{transformed[:5]}\")\n\n    recovered = transformer.inverse_transform(transformed)\n    recovered.insert(0, \"product_id\", range(1, len(recovered) + 1))\n\n    np.save(output_dir / \"transformed.npy\", transformed)\n    recovered.to_csv(output_dir / \"recovered.csv\", index=False)\n\n    print(f\"Wrote dense matrix to {output_dir/'transformed.npy'}\")\n    print(f\"Wrote recovered table to {output_dir/'recovered.csv'}\")\n    print(recovered.head())\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"demos/overview/","title":"Demo Overview","text":"<p>The <code>examples/demos</code> folder contains runnable scenarios. Ensure dependencies (Spark/Delta for relational pieces) are installed, then run from repo root:</p> <pre><code>pip install -e .[docs]\n</code></pre> <p>Each demo has a <code>run.py</code> script and an <code>outputs/</code> folder with sample artifacts.</p>"},{"location":"demos/overview/#available-demos","title":"Available demos","text":"<ul> <li>01_single_table_ctgan: Train CTGAN on a single table with mixed data types.</li> <li>02_privacy_sanitization: Detect and sanitize PII, compare raw vs sanitized outputs.</li> <li>03_validation_report: Generate validation metrics and HTML report comparing real vs synthetic.</li> <li>04_relational_linkage_ctgan: Train relational CTGAN with linkage, synthesize users/orders.</li> <li>05_transformer_embeddings: Demonstrate transformer-based embeddings and recovery. </li> </ul>"},{"location":"guides/embeddings/","title":"Feature Embeddings","text":"<p>SynthoHive provides specialized handling for High Cardinality categorical columns (e.g., Use ID, Zip Code, Product ID) using Entity Embeddings. This avoids the computational explosion of One-Hot Encoding and allows the model to learn semantic relationships between categories.</p>"},{"location":"guides/embeddings/#when-to-use-embeddings-vs-one-hot","title":"When to use Embeddings vs One-Hot?","text":"Feature One-Hot Encoding Entity Embeddings Technique Creates a binary column for every unique category. Maps each category to a dense vector of floating point numbers. Cardinality Low (&lt; 50 unique values). High (&gt; 50 unique values). Memory Usage High (Sparse but wide). Low (Compact dense vectors). Relationships Independent. No relationship between 'A' and 'B'. Learned. Similar categories end up close in vector space. Example <code>Gender</code>, <code>MaritalStatus</code>. <code>ZipCode</code>, <code>UserID</code>, <code>ICD9_Code</code>."},{"location":"guides/embeddings/#how-it-works","title":"How it Works","text":"<p>The transformation pipeline automatically detects high-cardinality columns based on a threshold.</p>"},{"location":"guides/embeddings/#1-detection","title":"1. Detection","text":"<p>During <code>DataTransformer.fit()</code>, the system checks the number of unique values in each categorical column. If <code>num_unique &gt; embedding_threshold</code> (default: 50), the column is flagged for embedding.</p>"},{"location":"guides/embeddings/#2-transformation","title":"2. Transformation","text":"<ul> <li>One-Hot: Converts string <code>\"A\"</code> -&gt; <code>[1, 0, 0]</code>.</li> <li>Embedding: Converts string <code>\"A\"</code> -&gt; <code>Integer Index (42)</code>.</li> </ul>"},{"location":"guides/embeddings/#3-model-training-ctgan","title":"3. Model Training (CTGAN)","text":"<p>Inside the neural network model: *   Generator: Outputs a probability distribution (logits) over all possible categories. *   Discriminator: Feeds the index (for real data) or probability-weighted vector (for fake data) into a learnable Embedding Layer. *   Learning: The model learns to place similar entities near each other. For example, if Zip Codes <code>10001</code> and <code>10002</code> have similar correlations with <code>Income</code>, their embedding vectors will become similar during training.</p>"},{"location":"guides/embeddings/#configuration","title":"Configuration","text":"<p>You can control the threshold for switching to embeddings globally or per-model.</p>"},{"location":"guides/embeddings/#global-configuration","title":"Global Configuration","text":"<p>Set the <code>embedding_threshold</code> when initializing the synthesizer or calling <code>fit</code>.</p> <pre><code>synth.fit(\n    data=df,\n    embedding_threshold=100  # Only use embeddings if &gt; 100 unique values\n)\n</code></pre> <p>Lowering this value forces more columns to use embeddings, which saves memory but might reduce precision for small categorical sets. increasing it uses One-Hot for more columns, which is more precise but memory-intensive.</p>"},{"location":"guides/embeddings/#use-cases","title":"Use Cases","text":""},{"location":"guides/embeddings/#1-geographical-data","title":"1. Geographical Data","text":"<p>Zip codes, Cities, or State abbreviations often have hundreds of values. Embeddings allow the model to learn valid geography (e.g., that \"NY\" and \"NJ\" are related) rather than treating them as unrelated tokens.</p>"},{"location":"guides/embeddings/#2-id-columns","title":"2. ID Columns","text":"<p>While primary keys like <code>UserID</code> are usually excluded, you might have Foreign Keys or distinct identifiers like <code>ProductCode</code> that you want to synthesize while preserving their statistical properties.</p>"},{"location":"guides/embeddings/#3-medical-codes","title":"3. Medical Codes","text":"<p>ICD-9 or CPT codes have thousands of distinct values. Embeddings are essential for synthesizing electronic health records (EHR) effectively.</p>"},{"location":"guides/fitting/","title":"Fitting Models","text":"<p>The <code>fit</code> process is the core of SynthoHive's generative engine. It takes your real relational database and trains a set of machine learning models that learn both the structure (relationships between tables) and the content (statistical distribution of data within tables).</p>"},{"location":"guides/fitting/#concepts","title":"Concepts","text":"<p>SynthoHive uses a Hybrid Relational Approach. Instead of trying to train one massive model for the entire database (which is often computationally infeasible), it breaks the problem down:</p> <ol> <li>Linkage Models: Learn how many child records typically exist for a given parent record. For example, \"How many <code>Orders</code> does a typical <code>User</code> have?\"</li> <li>Generative Models (CTGAN): Learn the content of each table individually. To preserve relational integrity dev-to-dev (e.g., ensuring an Order's <code>city</code> matches the User's <code>region</code>), child models are conditioned on context from their parent tables.</li> </ol>"},{"location":"guides/fitting/#the-role-of-metadata","title":"The Role of Metadata","text":"<p>Your <code>Metadata</code> object is the blueprint for this process. It tells the synthesizer: *   Hierarchy: Which tables are parents and which are children (defined by Foreign Keys). *   Context: Which columns from a parent should influence the generation of a child (e.g., <code>region</code> affecting <code>shipping_speed</code>). *   Constraints: Data types and logical rules that must be preserved.</p>"},{"location":"guides/fitting/#the-fitting-workflow","title":"The Fitting Workflow","text":"<p>When you call <code>synthesizer.fit()</code>, the following steps occur for each table in your metadata:</p>"},{"location":"guides/fitting/#1-data-ingestion","title":"1. Data Ingestion","text":"<p>The system reads your real data using Spark. *   Note: Currently, the system converts Spark DataFrames to Pandas for checking into the CTGAN backend. This means the working set for a single table must fit in memory.</p>"},{"location":"guides/fitting/#2-preprocessing-transformation","title":"2. Preprocessing &amp; Transformation","text":"<p>Each column is analyzed and transformed: *   Continuous Columns: Modeled using Variational Gaussian Mixture Models (VGM) to handle multi-modal distributions (e.g., a salary distribution with peaks at $40k and $120k). *   Categorical Columns:     *   Low Cardinality: Converted using One-Hot Encoding.     *   High Cardinality: Converted using Entity Embeddings (see Embeddings Guide). *   Primary/Foreign Keys: Excluded from the content model (CTGAN) because they are structural identifiers, not semantic content.</p>"},{"location":"guides/fitting/#3-linkage-learning","title":"3. Linkage Learning","text":"<p>For child tables, a <code>LinkageModel</code> describes the relationship with its \"Driver\" parent (the primary foreign key table). *   It calculates the probability distribution of child counts given parent attributes. *   Example: A <code>User</code> in the \"Enterprise\" segment might have 50-100 <code>logs</code>, while a \"Free\" user has 0-5.</p>"},{"location":"guides/fitting/#4-conditional-training-the-magic","title":"4. Conditional Training (The \"Magic\")","text":"<p>To ensure relational consistency, child tables are trained with Context. 1.  The system performs a left join of the Child table with selected columns from the Parent table. 2.  The CTGAN model is trained on this joined dataset. 3.  Result: When generating a new <code>Order</code>, the model receives the specific <code>User</code>'s context (e.g., <code>Region=US</code>) and generates an invoice meant for that region.</p>"},{"location":"guides/fitting/#configuration","title":"Configuration","text":"<p>You can tune the fitting process via the <code>fit()</code> arguments and global config.</p> <pre><code>synth.fit(\n    data=\"metrics_db\",\n    epochs=300,\n    batch_size=500,\n    embedding_threshold=50\n)\n</code></pre>"},{"location":"guides/fitting/#key-parameters","title":"Key Parameters","text":"Parameter Type Default Description <code>epochs</code> <code>int</code> <code>300</code> Number of training iterations. Higher means better quality but longer training. \u2022 Testing/Dev: Use <code>10-50</code> to verify the pipeline runs.\u2022 Production: Use <code>300+</code> for high-fidelity data. <code>batch_size</code> <code>int</code> <code>500</code> Number of samples processed at once. \u2022 Increase for speed (if GPU memory allows).\u2022 Decrease if you hit Out-Of-Memory (OOM) errors. <code>embedding_threshold</code> <code>int</code> <code>50</code> Columns with unique values &gt; this number will use Embeddings instead of One-Hot. <code>validate</code> <code>bool</code> <code>False</code> If <code>True</code>, automatically runs a validation report after training (requires <code>validate</code> extra dependencies). <p>Unused Parameters</p> <p>The parameters <code>sample_size</code> and <code>sampling_strategy</code> currently appear in the <code>fit()</code> signature but are not yet implemented. The system currently uses the full dataset provided in the <code>data</code> argument. Future versions will support downsampling large datasets before training.</p>"},{"location":"guides/fitting/#hardware-performance","title":"Hardware &amp; Performance","text":"<ul> <li>Memory: The most critical resource. Since data is converted to Pandas/Numpy for training, you generally need 2-3x the size of your largest single table in RAM.</li> <li>CPU vs GPU:<ul> <li>CPU: Default. Works fine for smaller datasets (&lt; 100k rows/table).</li> <li>GPU: Highly recommended for production training. CTGAN is a neural network; training on a GPU is 10-50x faster.</li> <li>To use GPU, ensure you have a CUDA-compatible PyTorch version installed. The synthesizer automatically detects and uses CUDA if available.</li> </ul> </li> </ul>"},{"location":"guides/fitting/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/fitting/#out-of-memory-oom-errors","title":"Out of Memory (OOM) Errors","text":"<ul> <li>Symptom: Process crashes with <code>MemoryError</code> or <code>CUDA out of memory</code>.</li> <li>Fix 1: Reduce <code>batch_size</code> (e.g., to 100 or 50).</li> <li>Fix 2: Reduce the number of context columns in <code>Metadata</code>. Joining parent context increases the width of the training data.</li> <li>Fix 3: Increase <code>embedding_threshold</code> carefully (embeddings use less memory than massive One-Hot vectors, but the transformation step itself needs memory).</li> </ul>"},{"location":"guides/fitting/#mode-collapse-output-is-constant-or-repetitive","title":"Mode Collapse (Output is constant or repetitive)","text":"<ul> <li>Symptom: The generated data looks very repetitive (e.g., every User is named \"John\").</li> <li>Cause: The Generator found a \"safe\" output that fools the Discriminator, or the Discriminator is too weak.</li> <li>Fix:<ul> <li>Increase <code>epochs</code>. In early epochs, GANs often produce noise or mode-collapsed data before converging.</li> <li>Check for data skew. If 99% of your real data is \"John\", the model is actually correct!</li> </ul> </li> </ul>"},{"location":"guides/fitting/#training-is-too-slow","title":"Training is too slow","text":"<ul> <li>Fix: Enable GPU acceleration.</li> <li>Fix: Explicitly exclude irrelevant text columns (e.g., \"Description\" fields that are unique per row) from <code>Metadata</code> or mark them as PII to be faked rather than modeled. Learning free-text fields with a tabular GAN is inefficient.</li> </ul>"},{"location":"guides/privacy/","title":"Privacy Guardrails","text":"<p>Synthetic data is only useful if it effectively masks sensitive information while retaining utility. SynthoHive applies privacy controls before any finding or training occurs, ensuring that no raw PII ever enters the generative models.</p>"},{"location":"guides/privacy/#the-sanitize-workflow","title":"The Sanitize workflow","text":"<pre><code>flowchart LR\n    A[Raw Data] --&gt; B{PII Analyzer}\n    B -- Regex/NER --&gt; C[Detected Map]\n    C --&gt; D[Sanitizer Engine]\n    D -- Rules --&gt; E(Clean Data)\n    E --&gt; F[Training]</code></pre>"},{"location":"guides/privacy/#faker-vs-embeddings","title":"Faker vs. Embeddings","text":"<p>It is important to distinguish when to use Faker versus Embeddings:</p> <ul> <li>Faker (Sanitization): Used when you need to hide the original values and replace them with plausible dummies. This creates new values (names, emails) that never existed in your real dataset. Use this for PII.</li> <li>Embeddings (Modeling): Used when you want the model to learn associations between high-cardinality categories (e.g., \"ZipCode\" correllated with \"Income\"). The generative model uses embeddings to capture these patterns but typically reproduces values from the existing vocabulary of the training data.</li> </ul> <p>[!TIP] Use Faker for privacy (hiding identity). Use Embeddings for utility (preserving statistical relationships in categories like Brands, Cities, or Diagnosis Codes).</p>"},{"location":"guides/privacy/#detection-strategies","title":"Detection Strategies","text":"<p>We use a combination of heuristics to detect Personal Identifiable Information (PII):</p> <ol> <li>Column Naming: Checks for keywords like \"email\", \"ssn\", \"phone\", \"address\".</li> <li>Pattern Matching: Scans a sample of data against a library of regular expressions (Email, IPv4, Credit Cards, Social Security Numbers).</li> <li>Thresholding: A column is flagged only if &gt; 50% of non-null values match a pattern.</li> </ol>"},{"location":"guides/privacy/#sanitization-actions","title":"Sanitization Actions","text":"<p>Once PII is detected (or manually configured), you can apply one of several actions:</p> Action Description Use Case <code>drop</code> Removes the column entirely. High-risk fields with zero utility (e.g., internal system IDs). <code>mask</code> Replaces all but the last 4 characters with <code>*</code>. Credit cards, phone numbers where visual format matters. <code>hash</code> Replaces value with SHA-256 hash. Maintaining distinctness for joining without revealing value (e.g., User IDs). <code>fake</code> Replaces with realistic fake data. Names, Addresses, Emails that need to look \"real\" for the model. <code>keep</code> Retains the original value. Low-sensitivity fields or when data is already sanitized source. <code>custom</code> Uses a user-provided function. complex logic requiring row-level context."},{"location":"guides/privacy/#context-aware-faking","title":"Context-Aware Faking","text":"<p>Standard fakers generate random data. Contextual Faking ensures consistency across columns.</p>"},{"location":"guides/privacy/#how-it-works","title":"How it works","text":"<p>The <code>ContextualFaker</code> looks for specific keys in the row data to determine the locale for generation: *   <code>country</code> *   <code>locale</code> *   <code>region</code></p> <p>Example: If a record has <code>Country=\"US\"</code>, the sanitizer will generate a US-formatted phone number (e.g., <code>+1-555...</code>). If <code>Country=\"JP\"</code>, it generates a Japanese number.</p> <p>Supported locales include: <code>US</code>, <code>UK</code>/<code>GB</code>, <code>JP</code>, <code>DE</code>, <code>FR</code>, <code>CN</code>, <code>IN</code>.</p>"},{"location":"guides/privacy/#internal-mechanism","title":"Internal Mechanism","text":"<p>Internally, <code>ContextualFaker</code> leverages the <code>faker</code> python library. It optimizes performance by maintaining a cache of Faker instances initiated for each required locale (e.g., <code>ja_JP</code> for Japan, <code>de_DE</code> for Germany).</p> <ol> <li>Locale Resolution: It scans the row for the context keys.</li> <li>Instance Selection: It maps the value (e.g., \"JP\") to a specific <code>Faker</code> locale identifier (e.g., <code>ja_JP</code>) using an internal <code>LOCALE_MAP</code>.</li> <li>Generation: It delegates the generation to that specific locale's provider. For example, <code>fake.phone_number()</code> for <code>ja_JP</code> produces a valid Japanese format compliant with local regex rules.</li> </ol> <p>If no context is found, it falls back to a default <code>en_US</code> generator.</p>"},{"location":"guides/privacy/#configuration","title":"Configuration","text":"<p>You can customize rules via the <code>PrivacyConfig</code> object:</p> <pre><code>from syntho_hive.interface.config import PrivacyConfig\nfrom syntho_hive.privacy.sanitizer import PiiRule\n\nconfig = PrivacyConfig(rules=[\n    PiiRule(name=\"custom_code\", patterns=[r\"^[A-Z]{2}-\\d{4}$\"], action=\"mask\"),\n    PiiRule(name=\"audit_id\", patterns=[r\"^AUD-\\d+\"], action=\"keep\")\n])\n</code></pre>"},{"location":"guides/privacy/#custom-logic","title":"Custom Logic","text":"<p>For complex scenarios, you can define a custom generator function that receives the entire row context. This is useful for conditional sanitization or format-preserving encryption.</p> <pre><code>def redact_sensitive_id(row: dict) -&gt; str:\n    # Example: Redact differently based on user role from another column\n    if row.get('role') == 'admin':\n        return row['user_id'] # Keep visible for admins (in this hypothetical flow)\n    return f\"REDACTED-{hash(row['user_id']) % 1000}\"\n\nconfig = PrivacyConfig(rules=[\n    PiiRule(\n        name=\"custom_logic\", \n        patterns=[r\"user_\\d+\"], \n        action=\"custom\", \n        custom_generator=redact_sensitive_id\n    )\n])\n</code></pre>"},{"location":"guides/relational/","title":"Relational Data Generation","text":"<p>SynthoHive specializes in maintaining referential integrity and statistical correlations across multiple tables. This guide explains the core concepts behind our orchestration engine.</p>"},{"location":"guides/relational/#the-driver-parent-concept","title":"The \"Driver Parent\" Concept","text":"<p>In a complex schema, a child table might refer to multiple parent tables. For example, an <code>Orders</code> table might refer to both <code>Users</code> and <code>Products</code>.</p> <p>When generating a synthetic <code>Order</code>: 1.  Which parent dictates existence? We treat one foreign key relationship as the Driver. Usually, this is the entity that \"owns\" the record (e.g., <code>User</code>). 2.  How many records? We use a <code>LinkageModel</code> to learn the distribution of child records per driver parent (e.g., \"Users typically have 0-5 orders\").</p>"},{"location":"guides/relational/#secondary-parents","title":"Secondary Parents","text":"<p>Other foreign keys (e.g., <code>Product</code>) are treated as Secondary. These are assigned to ensure referential integrity, but they do not drive the count of generated records.</p>"},{"location":"guides/relational/#contextual-conditioning","title":"Contextual Conditioning","text":"<p>To preserve correlations across tables (e.g., \"Users in NY order Winter Coats\"), we use Conditional Generation.</p> <ol> <li>Fit Phase: We join relevant columns from the Driver Parent (e.g., <code>User.City</code>) to the Child Table.</li> <li>Training: The CTGAN model learns not just <code>P(Order)</code>, but <code>P(Order | User.City)</code>.</li> <li>Generation Phase:<ul> <li>We generate a synthetic User: <code>{ID: 1, City: \"NY\"}</code>.</li> <li>The <code>LinkageModel</code> says \"Generate 3 orders for User 1\".</li> <li>We pass <code>City=\"NY\"</code> as context to the Order Generator.</li> <li>The generator produces orders statistically likely for a NY user.</li> </ul> </li> </ol>"},{"location":"guides/relational/#the-orchestration-flow","title":"The Orchestration Flow","text":"<ol> <li>Schema Analysis: Construct a Directed Acyclic Graph (DAG) of the schema.</li> <li>Topological Sort: Determine generation order (Parents -&gt; Children).</li> <li>Root Generation: Generate independent root tables using standard CTGAN.</li> <li>Child Loop:<ul> <li>Load synthetic parent data.</li> <li>Sample child counts for each parent row.</li> <li>Repeat parent IDs and Context attributes.</li> <li>Generate child rows conditioned on repeated context.</li> <li>Sample valid FKs for secondary parents.</li> <li>Assign Primary Keys.</li> </ul> </li> </ol>"},{"location":"guides/sampling/","title":"Sampling &amp; Relational Generation","text":"<p>Once your models are trained, you can generate synthetic data. SynthoHive uses a Top-Down Logic for generation, ensuring that parent records are created before their children, preserving referential integrity.</p>"},{"location":"guides/sampling/#the-generation-logic","title":"The Generation Logic","text":""},{"location":"guides/sampling/#1-root-tables","title":"1. Root Tables","text":"<p>Generation starts with tables that have no foreign keys (Roots). *   Method: The CTGAN model samples <code>n</code> rows directly from the learned latent space. *   Identity: Primary Keys (e.g., <code>user_id</code>) are assigned sequentially (1, 2, 3...) to these new rows.</p>"},{"location":"guides/sampling/#2-child-tables-the-cascade","title":"2. Child Tables (The Cascade)","text":"<p>Once a parent table exists, its children are generated using the Linkage Model. *   Step A: Sample Counts: For every row in the Parent table, the Linkage Model predicts how many children it should have.     *   Real World Analogy: For User #1, the model predicts 2 orders. For User #2, it predicts 0. *   Step B: Expand Context: The Parent's \"Context Columns\" (e.g., <code>Region</code>) are repeated <code>k</code> times, where <code>k</code> is the predicted count. *   Step C: Conditional Sampling: The Child CTGAN model generates the specific details for these rows, conditioned on the repeated parent context. *   Step D: Assign FKs: The Parent's ID is assigned to the foreign key column (<code>user_id</code>).</p>"},{"location":"guides/sampling/#3-multi-parent-tables-secondary-fks","title":"3. Multi-Parent Tables (Secondary FKs)","text":"<p>If a table has multiple parents (e.g., <code>OrderItems</code> has <code>order_id</code> AND <code>product_id</code>): 1.  One parent is chosen as the Driver (usually the first FK alphabetically or by configuration). This driver controls the volume (how many items per order). 2.  The other parent (<code>product_id</code>) is assigned via Random Sampling.     *   The system creates the rows based on the Order driver.     *   For the <code>product_id</code> column, it randomly picks valid IDs from the already-generated <code>Products</code> table.     *   Note: This random assignment preserves the validity of the FK (it points to a real Product), but does not currently enforce strict joint distributions between two parents.</p>"},{"location":"guides/sampling/#scaling-volume","title":"Scaling &amp; Volume","text":"<p>You control the volume of the generated database by setting the row counts for Root Tables.</p> <pre><code>num_rows = {\n    \"users\": 100_000,    # Root table: Exact count\n    \"products\": 500      # Root table: Exact count\n}\n\n# Child tables (e.g., \"orders\") are NOT specified here. \n# Their volume is determined by the Linkage Model ratios (e.g., avg 5 orders/user).\n</code></pre> <p>Scaling Factor</p> <p>If you want a 2x larger database, simply double the root counts. The child tables will naturally scale up by ~2x because the linkage ratios (children per parent) remain constant.</p>"},{"location":"guides/sampling/#api-usage","title":"API Usage","text":""},{"location":"guides/sampling/#synthesizersample","title":"<code>synthesizer.sample()</code>","text":"<pre><code>output = synth.sample(\n    num_rows={\"users\": 1000},\n    output_format=\"delta\",\n    output_path=\"/tmp/synthetic_db\"\n)\n</code></pre> <p>Arguments: -   <code>num_rows</code> (Dict[str, int]): Map of root table names to desired row counts. -   <code>output_format</code> (str): Format for writing files. Default is <code>\"delta\"</code>. -   <code>output_path</code> (Optional[str]):     -   If provided (str): Writes files to disk at this path. Returns a dictionary of table paths.     -   If <code>None</code>: Returns a dictionary of Pandas DataFrames in memory.</p>"},{"location":"guides/sampling/#example-in-memory-generation","title":"Example: In-Memory Generation","text":"<p>Useful for smaller datasets or unit tests.</p> <pre><code>dfs = synth.sample(\n    num_rows={\"users\": 100}, \n    output_path=None\n)\n\nusers_df = dfs[\"users\"]\norders_df = dfs[\"orders\"]\nprint(users_df.head())\n</code></pre>"},{"location":"guides/sampling/#performance-limitations","title":"Performance &amp; Limitations","text":""},{"location":"guides/sampling/#memory-usage","title":"Memory Usage","text":"<p>The current implementation generates data in batches per table. -   Limitation: The entire generated table must fit in memory before writing to disk. -   Workaround: If you need 100M rows, do not generate them in one generic call. Write a loop to generate 1M rows 100 times, saving each batch to disk.</p>"},{"location":"guides/sampling/#referential-integrity","title":"Referential Integrity","text":"<ul> <li>Primary Keys: Guaranteed unique for the generated batch.</li> <li>Foreign Keys: Guaranteed valid (always point to an existing parent in the current batch).</li> </ul> <p>Partitioning</p> <p>Generation is not currently distributed across Spark workers. It runs on the driver logic. Future versions will support distributed generation for massive scale.</p>"},{"location":"guides/validation/","title":"Quality Validation","text":"<p>How do you know your synthetic data is good? SynthoHive provides a comprehensive <code>ValidationReport</code> to quantify fidelity.</p>"},{"location":"guides/validation/#metrics","title":"Metrics","text":""},{"location":"guides/validation/#1-kolmogorov-smirnov-ks-test","title":"1. Kolmogorov-Smirnov (KS) Test","text":"<ul> <li>Target: Continuous Columns (Float/Int)</li> <li>Measure: The maximum distance between the cumulative distribution functions (CDFs) of real and synthetic data.</li> <li>Interpretation:<ul> <li><code>0.0</code> = Perfect fit (Distributions are identical).</li> <li><code>1.0</code> = Totally different.</li> <li>Typically, &lt; 0.1 is considered excellent quality.</li> </ul> </li> </ul>"},{"location":"guides/validation/#2-total-variation-distance-tvd","title":"2. Total Variation Distance (TVD)","text":"<ul> <li>Target: Categorical Columns</li> <li>Measure: Half the sum of absolute differences between category probabilities.</li> <li>Interpretation:<ul> <li><code>0.0</code> = Perfect fit (Category frequencies match exactly).</li> <li><code>1.0</code> = Totally different.</li> </ul> </li> </ul>"},{"location":"guides/validation/#3-correlation-distance","title":"3. Correlation Distance","text":"<ul> <li>Target: Column Pairs</li> <li>Measure: We compute correlation matrices (Pearson for continuous, Theil's U for categorical) for both Real and Synthetic datasets. The score is the L2 norm of the difference matrix.</li> <li>Goal: Measures how well the model captured relationships between columns (e.g., Age vs. Income).</li> </ul>"},{"location":"guides/validation/#the-html-report","title":"The HTML Report","text":"<p>The <code>ValidationReport.generate()</code> method produces a self-contained HTML file containing:</p> <ol> <li>Summary Score: Aggregate utility score (0-100%).</li> <li>Column Distributions: Overlay plots (Histogram/KDE) for every column.</li> <li>Correlation Heatmaps: Side-by-side comparison of Real vs. Synthetic associations.</li> <li>Row Previews: Snippets of raw data to verify formatting.</li> </ol>"},{"location":"guides/validation/#usage","title":"Usage","text":"<pre><code>from syntho_hive.validation.report_generator import ValidationReport\n\nreport = ValidationReport()\nreport.generate(\n    real_data=real_dfs,      # Dict[str, pd.DataFrame]\n    synth_data=synth_dfs,    # Dict[str, pd.DataFrame]\n    output_path=\"report.html\"\n)\n</code></pre>"},{"location":"reference/config-examples/","title":"Config Examples","text":""},{"location":"reference/config-examples/#simple-single-table-metadata","title":"Simple single-table metadata","text":"<pre><code>from syntho_hive.interface.config import Metadata\n\nmetadata = Metadata()\nmetadata.add_table(\n    name=\"customers\",\n    pk=\"customer_id\",\n    fk={},\n    parent_context_cols=[],\n    constraints={}\n)\n</code></pre>"},{"location":"reference/config-examples/#relational-metadata-with-constraints","title":"Relational metadata with constraints","text":"<pre><code>metadata.add_table(\n    name=\"users\",\n    pk=\"user_id\",\n    fk={},\n    constraints={\n        \"age\": {\"dtype\": \"int\", \"min\": 18, \"max\": 99},\n    },\n    parent_context_cols=[]\n)\n\nmetadata.add_table(\n    name=\"orders\",\n    pk=\"order_id\",\n    fk={\"user_id\": \"users.user_id\"},\n    parent_context_cols=[\"region\"],\n    constraints={\"amount\": {\"dtype\": \"float\", \"min\": 0.0}}\n)\n</code></pre>"},{"location":"reference/config-examples/#privacy-config","title":"Privacy config","text":"<pre><code>from syntho_hive.interface.config import PrivacyConfig\n\nprivacy = PrivacyConfig(\n    enable_differential_privacy=False,\n    epsilon=1.0,\n    pii_strategy=\"context_aware_faker\",\n    k_anonymity_threshold=5,\n    pii_columns=[\"email\", \"phone\"]\n)\n</code></pre>"},{"location":"reference/troubleshooting/","title":"Troubleshooting","text":"<ul> <li>Spark not found: Ensure <code>pyspark</code> is installed and <code>SPARK_HOME</code> is set. For local-only runs, some demos may be limited.</li> <li>Delta support: Install <code>delta-spark</code> and use Spark 3.2+.</li> <li>GPU vs CPU: CTGAN runs on CPU by default; set <code>device=\"cuda\"</code> when available.</li> <li>High-cardinality categoricals: Increase <code>embedding_threshold</code> to use embeddings instead of OHE.</li> <li>Validation failures: Inspect KS/TVD results; large TVD often means categorical imbalance\u2014check sampling strategy or increase training epochs.</li> <li>Doc build errors: Run <code>pip install -e .[docs]</code>; ensure <code>mkdocs</code> and <code>mkdocstrings</code> are installed. </li> </ul>"}]}