---
phase: 01-core-reliability
plan: "02"
type: execute
wave: 2
depends_on:
  - "01-01"
files_modified:
  - syntho_hive/core/models/ctgan.py
  - syntho_hive/__init__.py
autonomous: true
requirements:
  - CORE-02
  - CORE-03

must_haves:
  truths:
    - "CTGAN.save(path) writes a directory containing generator.pt, discriminator.pt, transformer.joblib, context_transformer.joblib, embedding_layers.joblib, data_column_info.joblib, metadata.json"
    - "A fresh CTGAN instance (no prior fit) can call load(path) and then sample(100) without error and without access to original training data"
    - "CTGAN.save(path) raises SerializationError when the path already exists and overwrite=False (default)"
    - "CTGAN.save(path, overwrite=True) succeeds when path already exists"
    - "CTGAN.load() logs a structlog WARNING when the saved SynthoHive version differs from the current version, but does not fail"
  artifacts:
    - path: "syntho_hive/core/models/ctgan.py"
      provides: "Directory-based save/load replacing single-file .pt approach"
      contains: "joblib.dump"
      min_lines: 50
  key_links:
    - from: "syntho_hive/core/models/ctgan.py CTGAN.save()"
      to: "joblib.dump"
      via: "joblib.dump(self.transformer, p / 'transformer.joblib')"
      pattern: "joblib\\.dump.*transformer"
    - from: "syntho_hive/core/models/ctgan.py CTGAN.load()"
      to: "torch.load"
      via: "torch.load(p / 'generator.pt', weights_only=False)"
      pattern: "weights_only=False"
    - from: "syntho_hive/core/models/ctgan.py CTGAN.load()"
      to: "_build_model()"
      via: "rebuild generator/discriminator architecture from loaded transformer.output_dim and context_transformer dims before loading state dicts"
      pattern: "_build_model"
---

<objective>
Replace CTGAN's broken single-file .pt serialization with a complete directory-based checkpoint that includes all DataTransformer state, embedding layers, and network weights — enabling cold load and sample without retraining.

Purpose: CORE-02 and CORE-03 are the most critical Phase 1 fixes. The current save() discards all DataTransformer, context_transformer, data_column_info, and embedding_layers state, making load() followed by sample() impossible without retraining. This plan makes model persistence actually work.
Output: Modified `syntho_hive/core/models/ctgan.py` with a directory-based save/load that passes the serialization round-trip test.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-reliability/01-CONTEXT.md
@.planning/phases/01-core-reliability/01-RESEARCH.md
@.planning/phases/01-core-reliability/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement directory-based CTGAN.save() with full checkpoint</name>
  <files>syntho_hive/core/models/ctgan.py</files>
  <action>
Read `syntho_hive/core/models/ctgan.py` completely first to understand the current save() and the object's attribute structure.

**Replace the existing `save()` method** with the following logic (adapt attribute names to match the actual code — confirm the exact attribute names for generator, discriminator, transformer, context_transformer, embedding_layers, data_column_info by reading the class):

```python
def save(self, path: str, *, overwrite: bool = False) -> None:
    """
    Save full model state to a directory.

    The directory contains separate files per component so the model
    can be loaded and sampled without the original training data.

    Args:
        path: Directory path to save into. Default: ./syntho_models/{table_name}/
        overwrite: If False (default), raises SerializationError if path exists.

    Raises:
        SerializationError: If path exists and overwrite=False, or if any
            component fails to serialize.
    """
    import joblib
    import json
    import torch
    from pathlib import Path
    from datetime import datetime, timezone
    from syntho_hive.exceptions import SerializationError

    p = Path(path)
    if p.exists() and not overwrite:
        raise SerializationError(
            f"SerializationError: Save path '{path}' already exists. "
            f"Pass overwrite=True to replace it."
        )

    try:
        p.mkdir(parents=True, exist_ok=True)

        # Network weights — torch native format
        torch.save(self.generator.state_dict(), p / "generator.pt")
        torch.save(self.discriminator.state_dict(), p / "discriminator.pt")

        # sklearn and numpy-heavy objects — joblib for efficient NumPy serialization
        joblib.dump(self.transformer, p / "transformer.joblib")
        joblib.dump(self.context_transformer, p / "context_transformer.joblib")

        # Embedding layers (nn.ModuleDict) — joblib serializes via pickle
        joblib.dump(self.embedding_layers, p / "embedding_layers.joblib")

        # Column layout list (Python list of dicts or similar)
        joblib.dump(self.data_column_info, p / "data_column_info.joblib")

        # Metadata — human-readable, enables version mismatch detection
        try:
            from syntho_hive import __version__
            current_version = __version__
        except Exception:
            current_version = "unknown"

        meta = {
            "synthohive_version": current_version,
            "embedding_dim": self.embedding_dim,
            "generator_dim": list(self.generator_dim),
            "discriminator_dim": list(self.discriminator_dim),
            "saved_at": datetime.now(timezone.utc).isoformat(),
        }
        # Include any other constructor hyperparams that _build_model() needs
        # (check what parameters _build_model() accepts and store them here)
        with open(p / "metadata.json", "w") as f:
            json.dump(meta, f, indent=2)

        log.info("model_saved", path=str(p))

    except SerializationError:
        raise
    except Exception as exc:
        raise SerializationError(
            f"SerializationError: Failed to save model to '{path}'. "
            f"Original error: {exc}"
        ) from exc
```

**Important:** `overwrite` must be keyword-only (use `*` in the signature) per Claude's discretion. Confirm attribute names by reading the CTGAN class: look for `self.generator`, `self.discriminator`, `self.transformer` (or `self._transformer`), `self.context_transformer`, `self.embedding_layers`, `self.data_column_info` (or `self._data_column_info`). Use the actual attribute names.

Add `import structlog` and `log = structlog.get_logger()` at module level if not already present. Add `from syntho_hive.exceptions import SerializationError, TrainingError` at top of file (Plan 01 added this — confirm it's there).
  </action>
  <verify>
Run:
```bash
python -c "
from syntho_hive.core.models.ctgan import CTGAN
import inspect
src = inspect.getsource(CTGAN.save)
assert 'joblib.dump' in src, 'save() must use joblib.dump'
assert 'metadata.json' in src, 'save() must write metadata.json'
assert 'overwrite' in src, 'save() must have overwrite param'
print('save() implementation OK')
"
```
  </verify>
  <done>
CTGAN.save() writes a directory with 7 files: generator.pt, discriminator.pt, transformer.joblib, context_transformer.joblib, embedding_layers.joblib, data_column_info.joblib, metadata.json. Raises SerializationError on path-exists without overwrite=True.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement CTGAN.load() to reconstruct full model from directory checkpoint</name>
  <files>syntho_hive/core/models/ctgan.py</files>
  <action>
Read the current `load()` method and `_build_model()` method carefully. Understand what `_build_model()` needs to reconstruct the generator and discriminator (what dimension parameters it requires). This is critical for the load sequence.

**Replace the existing `load()` method** with the following logic:

```python
def load(self, path: str) -> None:
    """
    Load full model state from a directory checkpoint.

    Reconstructs the complete model — DataTransformer, context_transformer,
    embedding_layers, and network weights — without requiring the original
    training data.

    Args:
        path: Directory path produced by save().

    Raises:
        SerializationError: If path does not exist, is missing required files,
            or if any component fails to deserialize.
    """
    import joblib
    import json
    import torch
    from pathlib import Path
    from syntho_hive.exceptions import SerializationError

    p = Path(path)
    if not p.exists():
        raise SerializationError(
            f"SerializationError: Checkpoint path '{path}' does not exist."
        )

    required_files = [
        "generator.pt", "discriminator.pt",
        "transformer.joblib", "context_transformer.joblib",
        "embedding_layers.joblib", "data_column_info.joblib",
    ]
    missing = [f for f in required_files if not (p / f).exists()]
    if missing:
        raise SerializationError(
            f"SerializationError: Checkpoint at '{path}' is incomplete. "
            f"Missing files: {', '.join(missing)}. "
            f"The checkpoint may have been saved by an older version or is corrupt."
        )

    try:
        # Version check — warn but do not fail (per CONTEXT.md decision)
        meta_path = p / "metadata.json"
        if meta_path.exists():
            with open(meta_path) as f:
                meta = json.load(f)
            try:
                from syntho_hive import __version__
                current_version = __version__
            except Exception:
                current_version = "unknown"
            saved_version = meta.get("synthohive_version", "unknown")
            if saved_version != current_version:
                log.warning(
                    "checkpoint_version_mismatch",
                    saved_version=saved_version,
                    current_version=current_version,
                    path=str(p),
                    note="Attempting load — schema changes between versions may cause failures",
                )
            # Restore hyperparams from metadata so _build_model() has correct dims
            if "embedding_dim" in meta:
                self.embedding_dim = meta["embedding_dim"]
            if "generator_dim" in meta:
                self.generator_dim = tuple(meta["generator_dim"])
            if "discriminator_dim" in meta:
                self.discriminator_dim = tuple(meta["discriminator_dim"])

        # Load sklearn objects first — needed to derive dims for _build_model()
        self.transformer = joblib.load(p / "transformer.joblib")
        self.context_transformer = joblib.load(p / "context_transformer.joblib")
        self.data_column_info = joblib.load(p / "data_column_info.joblib")
        self.embedding_layers = joblib.load(p / "embedding_layers.joblib")

        # Verify transformer output_dim survived the joblib round-trip
        if not hasattr(self.transformer, 'output_dim') or self.transformer.output_dim <= 0:
            raise SerializationError(
                f"SerializationError: Loaded transformer has invalid output_dim "
                f"({getattr(self.transformer, 'output_dim', 'missing')}). "
                f"The checkpoint may be corrupt."
            )

        # Reconstruct generator/discriminator architecture from loaded state
        # Derive context_dim from context_transformer (check attribute name — may be
        # output_dim, n_components, or a similar attribute; inspect the object)
        data_dim = self.transformer.output_dim
        # Derive context_dim: try common attribute names
        context_dim = getattr(self.context_transformer, 'output_dim',
                     getattr(self.context_transformer, 'n_components', 0))

        self._build_model(data_dim, context_dim)

        # Load network weights — weights_only=False REQUIRED for PyTorch 2.6+
        # (PyTorch 2.6 changed default to weights_only=True; custom objects fail without False)
        self.generator.load_state_dict(
            torch.load(p / "generator.pt", weights_only=False)
        )
        self.discriminator.load_state_dict(
            torch.load(p / "discriminator.pt", weights_only=False)
        )

        # Set model to eval mode for inference
        self.generator.eval()
        self.discriminator.eval()

        log.info("model_loaded", path=str(p), version=saved_version if meta_path.exists() else "unknown")

    except SerializationError:
        raise
    except Exception as exc:
        raise SerializationError(
            f"SerializationError: Failed to load model from '{path}'. "
            f"Original error: {exc}"
        ) from exc
```

**Critical:** Read `_build_model()` to understand its exact signature and what `data_dim` and `context_dim` parameters it expects. The attribute names for `transformer`, `context_transformer`, `data_column_info`, `embedding_layers` must match exactly what save() used. If the attribute names differ from the example above, use the correct ones.

Also read `context_transformer` to understand how to derive `context_dim` from it. If `context_transformer` is a custom object, inspect it to find the correct attribute that gives the output dimensionality.
  </action>
  <verify>
Run a functional round-trip test:
```bash
python -c "
import pandas as pd
import numpy as np
import tempfile, os
from syntho_hive.core.models.ctgan import CTGAN
from syntho_hive.interface.config import Metadata

# Build a small dataset
np.random.seed(0)
df = pd.DataFrame({
    'id': range(200),
    'age': np.random.randint(18, 80, 200),
    'income': np.random.exponential(50000, 200),
    'city': np.random.choice(['NY', 'SF', 'LA'], 200),
})
m = Metadata()
m.add_table('customers', pk='id')

# Train
model = CTGAN(m, batch_size=32, epochs=2, embedding_dim=16,
              generator_dim=(32,32), discriminator_dim=(32,32))
model.fit(df, table_name='customers', seed=42)

with tempfile.TemporaryDirectory() as tmp:
    save_path = os.path.join(tmp, 'customers')
    model.save(save_path)

    # Verify directory structure
    files = os.listdir(save_path)
    required = ['generator.pt','discriminator.pt','transformer.joblib',
                'context_transformer.joblib','embedding_layers.joblib',
                'data_column_info.joblib','metadata.json']
    for f in required:
        assert f in files, f'Missing: {f}'

    # Cold load — fresh instance, no fit
    new_model = CTGAN(m, embedding_dim=16, generator_dim=(32,32), discriminator_dim=(32,32))
    new_model.load(save_path)
    result = new_model.sample(50)
    assert len(result) == 50, f'Expected 50 rows, got {len(result)}'
    print('Round-trip test PASSED')

# Test overwrite protection
import tempfile
with tempfile.TemporaryDirectory() as tmp:
    save_path = os.path.join(tmp, 'customers')
    model.save(save_path)
    try:
        model.save(save_path)  # Should raise
        print('ERROR: Should have raised SerializationError')
    except Exception as e:
        if 'SerializationError' in type(e).__name__ or 'already exists' in str(e):
            print('Overwrite protection PASSED')
        else:
            print(f'Wrong error: {type(e).__name__}: {e}')
"
```
  </verify>
  <done>
- save() writes 7 files to directory
- Fresh CTGAN().load(path).sample(50) returns a DataFrame with 50 rows
- save() raises SerializationError when path exists and overwrite=False
- save(overwrite=True) succeeds on existing path
- load() logs structlog WARNING on version mismatch, does not raise
</done>
</task>

</tasks>

<verification>
1. Directory structure: `ls syntho_models/customers/` (after running a save) shows: generator.pt, discriminator.pt, transformer.joblib, context_transformer.joblib, embedding_layers.joblib, data_column_info.joblib, metadata.json
2. Cold load: `CTGAN(meta).load(path).sample(50)` — no AttributeError, returns 50 rows
3. Overwrite guard: second `save(path)` raises SerializationError
4. `torch.load(..., weights_only=False)` present in load() — confirmed via `grep -n "weights_only=False" syntho_hive/core/models/ctgan.py`
5. `grep -n "joblib.dump" syntho_hive/core/models/ctgan.py` shows all 4 sklearn components being saved
</verification>

<success_criteria>
- CTGAN.save() produces a 7-file directory (not a .pt file)
- A fresh CTGAN instance can load() and sample() without any training data
- SerializationError raised on path-exists without overwrite=True
- weights_only=False used in all torch.load() calls
- Version mismatch is warned, not raised
- Pre-existing tests in tests/ still pass
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-reliability/01-02-SUMMARY.md`
</output>
