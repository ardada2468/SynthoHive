---
phase: 02-relational-correctness
plan: "02"
type: execute
wave: 1
depends_on: []
files_modified:
  - syntho_hive/core/models/ctgan.py
  - syntho_hive/relational/orchestrator.py
  - pyproject.toml
autonomous: true
requirements:
  - REL-01
  - REL-04
  - CONN-02

must_haves:
  truths:
    - "CTGAN generator training step independently resamples parent context instead of reusing the stale discriminator batch"
    - "CTGAN accepts legacy_context_conditioning=True constructor flag to restore old behavior for backwards compatibility"
    - "StagedOrchestrator releases DataFrames from memory after disk write when output_path_base is set"
    - "StagedOrchestrator accepts on_write_failure parameter ('raise', 'cleanup', 'retry') defaulting to 'raise'"
    - "pyproject.toml pins pyspark>=4.0.0,<5.0.0 and delta-spark>=4.0.0,<5.0.0"
  artifacts:
    - path: "syntho_hive/core/models/ctgan.py"
      provides: "Fixed generator context resample + legacy_context_conditioning flag"
      contains: "legacy_context_conditioning"
    - path: "syntho_hive/relational/orchestrator.py"
      provides: "Memory-safe generation with on_write_failure policy"
      contains: "on_write_failure"
    - path: "pyproject.toml"
      provides: "Updated Spark version pins"
      contains: "pyspark>=4.0.0"
  key_links:
    - from: "syntho_hive/core/models/ctgan.py"
      to: "generator training block"
      via: "np.random.randint for independent context resample"
      pattern: "legacy_context_conditioning"
    - from: "syntho_hive/relational/orchestrator.py"
      to: "generated_tables dict"
      via: "conditional del after output_path_base write"
      pattern: "on_write_failure"
---

<objective>
Fix the stale context conditioning bug in CTGAN's generator training loop, add memory-safe DataFrame release in StagedOrchestrator, and update Spark version pins in pyproject.toml.

Purpose: These three changes are independent of the schema validation chain (Plan 01) and can be developed in parallel. REL-01 is a two-line fix that directly causes FK cardinality drift. REL-04 prevents OOM crashes on large schemas. CONN-02 eliminates version mismatch errors at import time.

Output: ctgan.py with correct generator context sampling + legacy opt-out flag; orchestrator.py with on_write_failure policy and DataFrame release; pyproject.toml with correct PySpark 4.x pins.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-relational-correctness/02-CONTEXT.md
@.planning/phases/02-relational-correctness/02-RESEARCH.md
@syntho_hive/core/models/ctgan.py
@syntho_hive/relational/orchestrator.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix stale context conditioning in CTGAN generator training (REL-01)</name>
  <files>syntho_hive/core/models/ctgan.py</files>
  <action>
Read syntho_hive/core/models/ctgan.py fully. Locate:
1. The CTGAN __init__ method — to add legacy_context_conditioning parameter
2. The generator training block around lines 399-408 — the stale context reuse bug (look for comment "Ideally yes, but reusing batch is fine for conditional stability")
3. The save() and load() methods — to persist the new flag

### Change 1: Add legacy_context_conditioning to __init__

Add `legacy_context_conditioning: bool = False` as a constructor parameter. Store as `self.legacy_context_conditioning = legacy_context_conditioning`.

Place it after the existing constructor parameters. Include in the docstring/comments as: "If True, reuses discriminator batch context in generator step (legacy behavior, default False applies correct independent resample)."

### Change 2: Fix the generator training block

Find the generator training section. The current code reuses `real_context_batch` from the discriminator step. Replace it with independent resampling:

```python
# --- Train Generator ---
noise = torch.randn(self.batch_size, self.embedding_dim, device=self.device)
if context_data is not None:
    if self.legacy_context_conditioning:
        # Backwards-compatible: reuse last discriminator batch context
        gen_context_batch = real_context_batch
    else:
        # Correct: independently sample fresh context for generator step
        gen_ctx_idx = np.random.randint(0, len(context_data), self.batch_size)
        gen_context_batch = context_data[gen_ctx_idx]
    gen_input = torch.cat([noise, gen_context_batch], dim=1)
else:
    gen_input = noise
```

Remove the existing comment that calls the reuse "fine for conditional stability" — it was the bug.

IMPORTANT: The discriminator block uses `np.random.randint(0, len(context_data), self.batch_size)` for `idx` and then `real_context_batch = context_data[idx]`. Copy that exact pattern for the generator resample. PyTorch tensors support NumPy array indexing directly.

### Change 3: Persist legacy_context_conditioning in save()/load()

In save(): add `legacy_context_conditioning` to the metadata.json dict that is serialized.

In load(): read `legacy_context_conditioning` from metadata.json and set `self.legacy_context_conditioning` (default False if key missing for forward compatibility with old checkpoints).
  </action>
  <verify>
Run the following verification:

```bash
cd /Users/arnavdadarya/FedEx/SynthoHive && python -c "
import numpy as np
import pandas as pd
from syntho_hive.core.models.ctgan import CTGAN
from syntho_hive.interface.config import Metadata

# Test: legacy_context_conditioning parameter is accepted
meta = Metadata()
meta.add_table('t', 'id')
model_default = CTGAN(metadata=meta, batch_size=10, epochs=1)
assert model_default.legacy_context_conditioning == False, 'default must be False'
model_legacy = CTGAN(metadata=meta, batch_size=10, epochs=1, legacy_context_conditioning=True)
assert model_legacy.legacy_context_conditioning == True
print('PASS: legacy_context_conditioning parameter accepted with correct defaults')
"
```

Then run the full test suite:
```bash
cd /Users/arnavdadarya/FedEx/SynthoHive && python -m pytest tests/ -x --tb=short -q 2>&1 | tail -20
```

All existing tests must pass.
  </verify>
  <done>
- CTGAN constructor accepts legacy_context_conditioning: bool = False
- Generator training block uses np.random.randint for fresh context resample when legacy_context_conditioning is False
- legacy_context_conditioning is serialized in save() and restored in load()
- All existing tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Memory-safe generation in StagedOrchestrator + pyproject.toml Spark pins (REL-04, CONN-02)</name>
  <files>syntho_hive/relational/orchestrator.py, pyproject.toml</files>
  <action>
### Part A: Memory-safe generation in orchestrator.py

Read syntho_hive/relational/orchestrator.py fully. Locate:
1. StagedOrchestrator.__init__ — to add on_write_failure parameter
2. The generation loop around lines 219-226 — where generated_tables[table_name] is set unconditionally

**Add on_write_failure to __init__:**

```python
from typing import Literal

def __init__(self, ..., on_write_failure: Literal['raise', 'cleanup', 'retry'] = 'raise'):
    ...
    self.on_write_failure = on_write_failure
```

**Add the _write_with_failure_policy helper function** at module level (above the class):

```python
def _write_with_failure_policy(io, pdf, path, policy, written_paths):
    """Write pdf to path; handle failures per policy ('raise', 'cleanup', 'retry')."""
    def _attempt_write():
        io.write_pandas(pdf, path)

    if policy == 'retry':
        try:
            _attempt_write()
            written_paths.append(path)
        except Exception:
            # One retry, no delay — transient lock release
            _attempt_write()
            written_paths.append(path)
    elif policy == 'cleanup':
        try:
            _attempt_write()
            written_paths.append(path)
        except Exception as exc:
            import shutil
            for p in written_paths:
                try:
                    shutil.rmtree(p, ignore_errors=True)
                except Exception:
                    pass
            raise exc
    else:  # 'raise' (default)
        _attempt_write()
        written_paths.append(path)
```

**Replace the unconditional storage block** in the generation loop. Find where `generated_tables[table_name] = generated_pdf` is set (after generation) and replace with:

```python
if generated_pdf is not None:
    if output_path_base:
        output_path = f"{output_path_base}/{table_name}"
        written_paths = getattr(self, '_written_paths', [])
        _write_with_failure_policy(
            io=self.io,
            pdf=generated_pdf,
            path=output_path,
            policy=self.on_write_failure,
            written_paths=written_paths,
        )
        log.debug("table_released_from_memory", table=table_name, path=output_path)
        # Do NOT store in generated_tables — child tables read from disk via output_path_base
    else:
        generated_tables[table_name] = generated_pdf
```

Initialize `written_paths` at the top of the generate() method: `written_paths = []` and store as `self._written_paths = written_paths` so the helper can track paths for cleanup mode.

CRITICAL: The existing orchestrator already has a code path where child tables read parent data from disk when output_path_base is set (check lines ~167-171 for the `parent_df = io.read_dataset(...)` branch). The memory release must only happen AFTER topological processing — parents are always generated before children, so deleting after write is safe. Verify this pattern exists before making the change. Do NOT change the in-memory path (when output_path_base is None, keep generated_tables[table_name] = generated_pdf as before).

Use structlog for the DEBUG log: `log = structlog.get_logger()` should already be at the module level.

### Part B: Update pyproject.toml

Read pyproject.toml. Find the pyspark and delta-spark version pins. Update:
- FROM: `"pyspark>=3.2.0"` TO: `"pyspark>=4.0.0,<5.0.0"`
- FROM: `"delta-spark>=2.0.0"` TO: `"delta-spark>=4.0.0,<5.0.0"`

The upper bound <5.0.0 prevents accidental upgrade to a future major version where delta-spark compatibility may break (delta-spark has strict Spark major version coupling per the compatibility matrix).

Do NOT change any other version pins.
  </action>
  <verify>
```bash
# Verify orchestrator changes compile correctly
cd /Users/arnavdadarya/FedEx/SynthoHive && python -c "
from syntho_hive.relational.orchestrator import StagedOrchestrator
from syntho_hive.interface.config import Metadata
from unittest.mock import MagicMock

meta = Metadata()
meta.add_table('users', 'id')
io = MagicMock()
orch = StagedOrchestrator(metadata=meta, io=io, on_write_failure='raise')
assert orch.on_write_failure == 'raise', 'default should be raise'
orch2 = StagedOrchestrator(metadata=meta, io=io, on_write_failure='cleanup')
assert orch2.on_write_failure == 'cleanup'
print('PASS: on_write_failure parameter accepted')
"

# Verify pyproject.toml pins
cd /Users/arnavdadarya/FedEx/SynthoHive && grep -E 'pyspark|delta-spark' pyproject.toml
# Expected output lines containing: pyspark>=4.0.0,<5.0.0 and delta-spark>=4.0.0,<5.0.0

# Full test suite
cd /Users/arnavdadarya/FedEx/SynthoHive && python -m pytest tests/ -x --tb=short -q 2>&1 | tail -20
```
  </verify>
  <done>
- StagedOrchestrator accepts on_write_failure parameter with default 'raise'
- Generation loop gates DataFrame storage on output_path_base: stores in generated_tables when None, writes-and-releases when set
- _write_with_failure_policy handles 'raise', 'cleanup', 'retry' policies
- DataFrame release uses structlog DEBUG log, not INFO
- pyproject.toml has pyspark>=4.0.0,<5.0.0 and delta-spark>=4.0.0,<5.0.0
- All existing tests pass
  </done>
</task>

</tasks>

<verification>
```bash
cd /Users/arnavdadarya/FedEx/SynthoHive && python -m pytest tests/ -x --tb=short -q 2>&1 | tail -20
```

Also verify the version pins are correct:
```bash
grep -E 'pyspark|delta-spark' /Users/arnavdadarya/FedEx/SynthoHive/pyproject.toml
```

Expected: `pyspark>=4.0.0,<5.0.0` and `delta-spark>=4.0.0,<5.0.0`
</verification>

<success_criteria>
- ctgan.py generator training block independently resamples context when legacy_context_conditioning=False (the default)
- CTGAN constructor accepts legacy_context_conditioning: bool = False; flag persisted in save/load
- orchestrator.py generation loop: when output_path_base is set, DataFrames are written and released (not accumulated in generated_tables)
- on_write_failure parameter on StagedOrchestrator with 'raise' default and 'cleanup'/'retry' alternatives
- pyproject.toml pins updated to pyspark>=4.0.0,<5.0.0 and delta-spark>=4.0.0,<5.0.0
- All existing tests pass with no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/02-relational-correctness/02-02-SUMMARY.md`
</output>
